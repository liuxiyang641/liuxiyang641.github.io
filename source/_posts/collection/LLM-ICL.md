---
title: LLM-ICL
published: true
date: 2023-06-01 20:07:01
categories:
- Paper
- LLM
- ICL
tags:
- LLM
- ICL
---

# LLM的in-context learning和prompt相关论文

<!--more-->

## APE

Large Language Models are Human-Level Prompt Engineers

ICLR 2023，University of Toronto，[代码](https://github.com/keirp/automatic_prompt_engineer)。

> By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, **we propose Automatic Prompt Engineer  (APE) for automatic instruction generation and selection.** In our method, we treat the instruction as the “program,” optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. **Extensive experiments show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 24/24 Instruction Induction tasks and 17/21 curated BIG-Bench tasks.** We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that APE-engineered prompts are able to improve few-shot learning performance (by simply prepending them to standard in-context learning prompts), find better zero-shot chain-of-thought prompts, as well as steer models toward truthfulness and/or informativeness.

作者提出一种从几个样例中自动创建task instruction的training free的方法APE（Automatic Prompt Engineer）。

由于LLM对于prompt的理解和人类不一样，因此简单的语言prompt不一定能够得到理想的结果。因此往往需要大量的人工去设计prompt，去搜索寻找最好的prompt实践。作者将自动寻找最能够激发LLM执行具体task能力的prompt的过程看做是一个black-box optimization problem，称之为natural language program synthesis。

作者提出的APE方案如下：

![image-20230601201707682](https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230601201707682.png)

简单来说就是3步，

1. 让LLM从几个样例中产生一系列的task instructions
2. 让LLM使用不同的task instructions，评估不同task instructions的效果。先选一个子集评估所有的instruction，然后对于score比较高的instructions，再选新的子集评估筛选，直至选出一定数量的候选instructions
3. [可选] 为了防止在某些任务下，LLM一开始没有找到合适的instructions，重新采样。让LLM在当前score最高的instructions附近采样

第1步采样初始的task instructions，作者使用了三种方式：

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230601202801649.png"   style="zoom:40%;" />

第一种forward mode prompt是最直接的，让LLM续写query，但是instructions只会出现在句子最后。作者提出第2种reverse mode prompt让LLM填空，让instructions可以出现在句子的任意地方。最后是对于一些已经发现了效果比较好的模板，可以加进来（具体在实验中可能作者只是在TruthfulQA数据集下使用了？）。

第2步怎么样评价不同instructions的效果，当然最直接的是根据任务指标来验证。在一般情况下，作者建议可以直接使用0-1 loss来给不同生成的instructions打分。还可以使用log probability。同时，让每个instructions都在所有的训练样例下进行评估是不实际的，因此作者就提出使用训练子集来筛选合适的instructions。如果一个子集筛选过后的instructions还是太多，就继续采样新的不重叠的训练子集，继续筛选instructions。

第3步是针对一些情况下，LLM生成的所有instructions没有找到合适的instructions，让LLM继续在当前效果最好的instructions周围进行寻找。we consider exploring the search space locally around the current best candidates. 这一步是可选的，因为作者发现不断增加更多的迭代步骤没有带来更多的提升，同时这种提升也不是在所有task中都会出现。（在实验中APE默认无迭代，APE-IT是加入了迭代）

zero-shot instruction induction实验结果来看，APE方法在很多任务上已经达到了人工prompting的效果：

![image-20230601204311989](https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230601204311989.png)

生成更多的候选instructions带来了更好的效果，同时iterative search对于那些一开始没有办法找到合适的instructions的task有提升作用，而在另外task上仅仅是生成一次就效果比较好了：

![image-20230601204823591](https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230601204823591.png)

## SG-ICL

Self-Generated In-Context Learning: Leveraging Auto-regressive Language Models as a Demonstration Generator

NAACL 2022 Workshop，首尔大学

> Large-scale pre-trained language models (PLMs) are well-known for being capable of solving a task simply by conditioning a few input-label pairs dubbed demonstrations on a prompt without being explicitly tuned for the desired downstream task. Such a process (i.e., in-context learning), however, naturally leads to high reliance on the demonstrations which are usually selected from external datasets. In this paper, **we propose self-generated in-context learning (SG-ICL), which generates demonstrations for in-context learning from PLM itself to minimize the reliance on the external demonstration.** We conduct experiments on four different text classification tasks and show SG-ICL significantly outperforms zero-shot learning and is generally worth approximately 0.6 gold training samples. Moreover, our generated demonstrations show more consistent performance with low variance compared to randomly selected demonstrations from the training dataset.

利用LLM自动生成demonstrations来增强zero-shot ICL的能力（作者声称是首个这么做的工作）。

![image-20230601223224356](https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230601223224356.png)

需要注意的一点是，在使用LLM生成demonstrations的时候，是输入了对应的test instance和期望生成的class一起生成的。

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230601223450763.png"   style="zoom:40%;" />

作者在实验部分使用LLM位每个test instance生成8个构造的demonstrations，发现效果和在进行5-shot的LLM ICL效果差不多，因此作者认为1个LLM生成的demonstration价值相当于0.6个gold training sample：

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230601223612766.png"  style="zoom:40%;" />

## Self-Instruct

ACL 2023，华盛顿大学，[代码](https://github.com/ yizhongw/self-instruct)。

{% post_link llm/Self-Instruct [个人详细博客] %}

> Large “instruction-tuned” language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute improvement over the original model on SUPER-NATURALINSTRUCTIONS, on par with the performance of InstructGPT 001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with SELF-INSTRUCT outperforms using existing public instruction datasets by a large margin, leaving only a 5% absolute gap behind InstructGPT 001 . SELF-INSTRUCT provides an almost annotation-free method for aligning pretrained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning.

人工生成instructions一方面代价很大，另一方面人工生成的instructions难以保证quantity, diversity, and creativity。

作者提出使用LLM从已有的task instruction出发，自动生成新的task instruction和对应的input-output，然后过滤掉不符合规则的新task instructions，再加入到已有的task instructions集合中。作者在这个自动构造的instruction data上fine-tuning GPT3，发现效果提升了33%，非常接近InstructGPT001的效果。

作者提出的方法：

![image-20230603150047353](https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230603150047353.png)

首先，作者拥有一个task pool，包括175 tasks (1 instruction and 1 instance for each task)。这175个初始的task instructions都是由本文作者自己创建的。

然后，作者从task pool中随机抽取8个task instructions（6 are from the human-written tasks, and 2 are from the model-generated tasks）。下面是产生新task instruction的prompt：

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230603150335100.png" style="zoom: 25%;" />

之后，作者使用LLM判断新产生的instruction是否是一个classification task（using 12 classification instructions and 19 non-classification instructions）：

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230603150505630.png"   style="zoom:25%;" />

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230603150518579.png" alt="image-20230603150518579" style="zoom:25%;" />

随后，对于新产生的task instruction，用LLM生成新的对应的instance。对于生成任务，作者先生成input，再生成output，作者称为Input-first Approach：

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230603150903068.png"   style="zoom:25%;" />

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230603150941339.png"  style="zoom:25%;" />

对于分类任务，作者发现如果是先生成input，LLM总是会倾向于生成某一个label的输入。因此作者使用LLM先生成output label，再让LLM生成input，作者称为Output-first Approach：

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230603151018452.png"   style="zoom:25%;" />

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230603151030519.png"   style="zoom:25%;" />

对于LLM生成的task instruction、input和output，需要通过一些规则过滤，比如：

- 只有当和已有的task instruction相似度全部比较低（$\mbox{ROUGE-L}< 0.7$）的时候，一个新task instruction会被添加到task pool里
- We also exclude instructions that contain some specific keywords (e.g., image, picture, graph) that usually can not be processed by LMs.
- When generating new instances for each instruction, we filter out instances that are exactly the same or those with the same input but different outputs.
- Invalid generations are identified and filtered out based on heuristics (e.g., instruction is too long or too short, instance output is a repetition of the input).

作者从原始的175个task出发，最后构造了5万多的task，并且差异性也比较大：

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230603151912820.png"   style="zoom:30%;" />

在SuperNI数据集上的实验结果：

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230603152723289.png"   style="zoom:30%;" />

SuperNI数据集大多是已有的NLP任务，为了进一步评估模型在实际使用场景下的价值，作者人工创建了一个包括252 task的新数据集。

## Discrete prompt for different SLM

Can discrete information extraction prompts generalize across language models?

ICLR 2023，[代码](https://github.com/ncarraz/prompt_ generalization)。

作者探究了不同小参数量语言模型对于discrete prompt的泛化性的情况，并且提出了mixed-training autoprompt。也就是在AutoPrompt的方法基础上，用一个LM进行候选prompt生成，另一个LM进行评估。

> We study whether automatically-induced prompts that effectively extract information from a language model can also be used, out-of-the-box, to probe other language models for the same information. After confirming that discrete prompts induced with the AutoPrompt algorithm outperform manual and semi-manual prompts on the slot-filling task, we demonstrate a drop in performance for AutoPrompt prompts learned on a model and tested on another. We introduce a way to induce prompts by mixing language models at training time that results in prompts that generalize well across models. We conduct an extensive analysis of the induced prompts, finding that the more general prompts include a larger proportion of existing English words and have a less order-dependent and more uniform distribution of information across their component tokens. Our work provides preliminary evidence that it’s possible to generate discrete prompts that can be induced once and used with a number of different models, and gives insights on the properties characterizing such prompts.

作者对比了三种方法产生的prompt：

- LPAQA：使用预先定义好的几个prompt，通过mining和paraphrasing发现更多的prompt
- AutoPrompt：让LM自动生成prompt
- OptiPrompt：使用了soft prompt

作者在下面一系列的小LM上进行了实验：

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230614215935769.png"   style="zoom:50%;" />

在LAMA数据集上的对比效果如下，表格中LAMA是指该数据集本身提供的人工写的prompt：

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230614215904682.png"   style="zoom:50%;" />

作者发现使用了soft prompt的OptiPrompt方法效果最好，而AutoPrompt这种自动生成discrete prompt的方法效果次之，但也是好于人工写的prompt。

soft prompt虽然在自动生成prompt和使用prompt的LM是一致的情况下效果最好，但是泛化性很差：

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230614220633652.png"   style="zoom:50%;" />

同时soft prompt还要求能够访问LM的内部结构，能够使用它的embeddings。

作者进而测试了在使用AutoPrompt的情况下，这些生成的prompt在source LM和target LM不一致的情况下的效果：

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230614220800480.png"   style="zoom:50%;" />

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230614220826142.png"   style="zoom:50%;" />

可以看出来，当两个LM是一样的情况下，效果最好。如果target LM和source LM不一致的情况下，基本上效果都会下降。中间GPT-2的一系列变种变化不大，作者解释原因是GPT-2本身效果已经比较差了，再差点也没有什么太大变化了。

为了解决这一问题，作者提出了AutoPrompt的一个简单改动：

> Recall that the AutoPrompt algorithm involves two phases: one in which candidate prompts are generated, and one in which the prompts are evaluated. Rather than relying on the same model for the two phases, we now use two different LMs. **The first model, which we call the generator, proposes a set of candidates. Then, the second model, that we call the evaluator, evaluates the candidates and chooses the best one.**

下面是作者的实验：

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230614221621275.png"   style="zoom:50%;" />

可以看到，不同架构的LM混合在一起，是可能生成具有更好泛化性的prompt的。但是作者在论文中也指出，这种更好的泛化性的生成和哪两个LM组合到一起有关，不是说只要组合在一起，就能够生成泛化性更好的prompt。

在最后，作者尝试从4个方面对找到的泛化性更好的prompt进行定量分析：

1. Semantic overlap with English: We thus hypothesize that prompts that generalize better will have a larger semantic overlap with manually crafted English prompts.
   - 作者的观察：对于作者提出的mixed-training AutoPrompt方法来说，泛化性更强的prompt和人类实际语言之间的相似度有很强的相关性。但是这种相似度，和泛化性更弱的单个模型比如BERT-base产生的prompt对比的话，相似度耕地（说明这个假设还有待验证）。
2. Real-word ratio: We thus conjecture that prompts that generalize better will contain a larger proportion of real English words.
   - 作者的观察：进行了混合策略的方法总是自动产生了更多real world的单词；但是不一定能够带来更好的泛化性。
3. Shuffling: We thus conjecture that a “bag-of-token” prompt sequence that does not require the tokens to be in any special order will be more general than one where order matters and, consequently, generalizing prompts will be more robust to token shuffling.
   - 作者的观察：更加泛化的prompt确实对于token的顺序更加不敏感
4. Token deletion: We thus conjecture that generalizing prompts will distribute information more evenly across tokens and thus they will be more robust to single-token deletion.
   - 作者的观察：随机去掉prompt上不同位置token，测试prompt不同位置上，LM的关注程度是不是不同的。发现总是对于prompt最后一个位置的token有最大的关注。

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230614221822454.png"  style="zoom:50%;" />

## Wang et al.

Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning

arXiv 2023, [代码](https://github.com/WANGXinyiLinda/concept-based-demonstration-selection)。

> In recent years, pre-trained large language models have demonstrated remarkable efficiency in achieving an inference-time few-shot learning capability known as incontext learning. However, existing literature has highlighted the sensitivity of this capability to the selection of few-shot demonstrations. The underlying mechanisms by which this capability arises from regular language model pretraining objectives remain poorly understood. In this study, we aim to examine the in-context learning phenomenon through a Bayesian lens, viewing large language models as topic models that implicitly infer task-related information from demonstrations. On this premise, we propose an algorithm for selecting optimal demonstrations from a set of annotated data and demonstrate a significant 12.5% improvement relative to the random selection baseline, averaged over eight GPT2 and GPT3 models on eight different real-world text classification datasets. Our empirical findings support our hypothesis that large language models implicitly infer a latent concept variable.

作者提出了一种看待ICL中的demonstration的作用的角度，认为LLM可以从demonstrations中学习到隐式的任务相关的信息。并且给出了一些理论上的分析。

基于此假设，作者提出一种新的找demonstrations的方法，整体流程如下：

![image-20230615203143639](https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230615203143639.png)

首先是利用prompt-tuning的思想，固定LM，学习和task相关的soft prompt：

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230615203205166.png"   style="zoom:50%;" />

然后，作者认为最佳的demonstrations就是能够在给定样例的情况下，使得输出上一步学习到的soft prompt的概率最大的样例。直观上的理解就是说最能够“体现”任务的demonstrations就是最佳的找到的demonstrations。demonstrations的候选集应该是各种数量、各种排序的组合，但是为了减小搜索空间，作者简化到了假设最佳的单个demonstration的采样是相互独立的，然后再排序：

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230615203224510.png"   style="zoom:50%;" />

最后，找到的这种最佳的demonstrations，可以用于其它的LM。

实验结果：

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230615203308965.png"  style="zoom:50%;" />
