---
title: MMML-survey-list
published: false
date: 2022-06-19 16:07:15
categories:
- Reading-list
- MMKG
tags:
- Collection
- MMKG
- Reading-list
---

# Multimodal Machine Learning

对目前的多模态机器学习相关文献进行的调研list。

<!--more-->

## Tutorial and Course

|                                    地址                                     |                                  标题                                  | 内容                                                                                                          | 评价                                                                                                             |
|:---------------------------------------------------------------------------:|:----------------------------------------------------------------------:| ------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------- |
|   [课程地址](https://cmu-multicomp-lab.github.io/mmml-tutorial/schedule/)   |      **T**utorial on **M**ulti**M**odal **M**achine **L**earning       | CMU 2022年最新的MMML tutorial，包括了slides和videos，主要介绍多模态机器学习相关概念和关键挑战；值得新人阅读。 | 内容较为全面，并且从专业的角度看到MMML的发展现状和挑战。作者说了很快会有一个600+引用文献的survey出现，值得期待。 |
| [课程地址](https://cmu-multicomp-lab.github.io/adv-mmml-course/spring2022/) | **A**dvanced **T**opics in **M**ulti**M**odal **M**achine **L**earning | CMU 2022春季的MMML各个领域最新的研究挑战；没有视频，但是提供了讲义。                                          | 还未阅读，目前看到讲义内容不多。                                                                                 |
|                                                                             |                                                                        |                                                                                                               |                                                                                                                  |

## Paper

### ACL 22

1. Cross-Modal Discrete Representation Learning
2. Finding Structural Knowledge in Multimodal-BERT
3. Guided Attention Multimodal Multitask Financial Forecasting with Inter-Company Relationships and Global and Local News
4. Leveraging Unimodal Self-Supervised Learning for Multimodal Audio-Visual Speech Recognition
5. Leveraging Visual Knowledge in Language Tasks: An Empirical Study on Intermediate Pre-training for Cross-modal Knowledge Transfer
6. M3ED: Multi-modal Multi-scene Multi-label Emotional Dialogue Database
7. Modeling Temporal-Modal Entity Graph for Procedural Multimodal Machine Comprehension
8. MSCTD: A Multimodal Sentiment Chat Translation Dataset
9. Multi-Modal Sarcasm Detection via Cross-Modal Graph Convolutional Network
10. Multimodal Dialogue Response Generation
11. Multimodal fusion via cortical network inspired losses
12. Multimodal Sarcasm Target Identification in Tweets
13. On Vision Features in Multimodal Machine Translation
14. Phone-ing it in: Towards Flexible, Multi-Modal Language Model Training using Phonetic Representations of Data
15. Premise-based Multimodal Reasoning: Conditional Inference on Joint Textual and Visual Clues
16. RoCBert: Robust Chinese Bert with Multimodal Contrastive Pretraining
17. SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing
18. Understanding Multimodal Procedural Knowledge by Sequencing Multimodal Instructional Manuals
19. UniXcoder: Unified Cross-Modal Pre-training for Code Representation
20. When did you become so smart, oh wise one?! Sarcasm Explanation in Multi-modal Multi-party Dialogues
21. XDBERT: Distilling Visual Information to BERT from Cross-Modal Systems to Improve Language Understanding
22. M-SENA: An Integrated Platform for Multimodal Sentiment Analysis
23. MMCoQA: Conversational Question Answering over Text, Tables, and Images

Findings of ACL 22

24. Reinforced Cross-modal Alignment for Radiology Report Generation
25. Cross-Modal Cloze Task: A New Task to Brain-to-Word Decoding
26. Sentiment Word Aware Multimodal Refinement for Multimodal Sentiment Analysis with ASR Errors
27. Modality-specific Learning Rates for Effective Multimodal Additive Late-fusion
28. Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation
29. Assessing Multilingual Fairness in Pre-trained Multimodal Representations
30. Modular and Parameter-Efficient Multimodal Fusion with Prompting
31. Comprehensive Multi-Modal Interactions for Referring Image Segmentation
32. Attention as Grounding: Exploring Textual and Cross-Modal Attention on Entities and Relations in Language-and-Vision Transformer
33. Improving Candidate Retrieval with Entity Profile Generation for Wikidata Entity Linking（把mention和Wikidata中的实体联系在一起，没有使用多模态信息）

### ACL 21

1. Multimodal Sentiment Detection Based on Multi-channel Graph Neural Networks
2. Self-Supervised Multimodal Opinion Summarization
3. Learning Relation Alignment for Calibrated Cross-modal Retrieval
4. KM-BART: Knowledge Enhanced Multimodal BART for Visual Commonsense Generation
5. The Possible, the Plausible, and the Desirable: Event-Based Modality Detection for Language Processing
6. Factuality Assessment as Modal Dependency Parsing
7. Multi-stage Pre-training over Simplified Multimodal Pre-training Models
8. LayoutLMv2: Multi-modal Pre-training for Visually-rich Document Understanding
9. UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning
10. Missing Modality Imagination Network for Emotion Recognition with Uncertain Missing Modalities
11. Competence-based Multimodal Curriculum Learning for Medical Report Generation
12. MultiMET: A Multimodal Dataset for Metaphor Understanding
13. Constructing Multi-Modal Dialogue Dataset by Replacing Text with Semantically Relevant Images
14. Learning Language and Multimodal Privacy-Preserving Markers of Mood from Mobile Data
15. CTFN: Hierarchical Learning for Multimodal Sentiment Analysis Using Coupled-Translation Fusion Network
16. MMGCN: Multimodal Fusion via Deep Graph Convolution Network for Emotion Recognition in Conversation
17. Cross-modal Memory Networks for Radiology Report Generation
18. Multi-perspective Coherent Reasoning for Helpfulness Prediction of Multimodal Reviews
19. Good for Misconceived Reasons: An Empirical Revisiting on the Need for Visual Context in Multimodal Machine Translation
20. Multimodal Multi-Speaker Merger & Acquisition Financial Modeling: A New Task, Dataset, and Neural Baselines
21. More than Text: Multi-modal Chinese Word Segmentation
22. Constructing Multi-Modal Dialogue Dataset by Replacing Text with Semantically Relevant Images

Findings of ACL 21

23. Multimodal Incremental Transformer with Visual Grounding for Visual Dialogue Generation
24. GeoQA: A Geometric Question Answering Benchmark Towards Multimodal Numerical Reasoning
25. Read, Listen, and See: Leveraging Multimodal Information Helps Chinese Spell Checking
26. Deciphering Implicit Hate: Evaluating Automated Detection Algorithms for Multimodal Hate
27. Entheos: A Multimodal Dataset for Studying Enthusiasm
28. Multimodal Fusion with Co-Attention Networks for Fake News Detection
29. GEM: A General Evaluation Benchmark for Multimodal Tasks
30. Transformer-Exclusive Cross-Modal Representation for Vision and Language
31. Probing Multi-modal Machine Translation with Pre-trained Language Model
32. Multimodal Graph-based Transformer Framework for Biomedical Relation Extraction
33. A Text-Centered Shared-Private Framework via Cross-Modal Prediction for Multimodal Sentiment Analysis

### ACL 20

1. Cross-modal Language Generation using Pivot Stabilization for Web-scale Language Coverage
2. Multimodal Quality Estimation for Machine Translation
3. MMPE: A Multi-Modal Interface for Post-Editing Machine Translation
4. Integrating Multimodal Information in Large Pretrained Transformers
5. MultiQT: Multimodal Learning for Real-Time Question Tracking in Speech
6. Multiresolution and Multimodal Speech Recognition with Transformers
7. A Novel Graph-based Multi-modal Fusion Encoder for Neural Machine Translation
8. Learning to Recover from Multi-Modality Errors for Non-Autoregressive Neural Machine Translation
9. Improving Multimodal Named Entity Recognition via Entity Span Detection with Unified Multimodal Transformer
10. CH-SIMS: A Chinese Multimodal Sentiment Analysis Dataset with Fine-grained Annotations of Modality
11. Reasoning with Multimodal Sarcastic Tweets via Modeling Cross-Modality Contrast and Semantic Association
12. Multimodal Transformer for Multimodal Machine Translation
13. Sentiment and Emotion help Sarcasm? A Multi-task Learning Framework for Multi-Modal Sarcasm, Sentiment and Emotion Analysis
14. Towards Emotion-aided Multi-modal Dialogue Act Classification
15. A Recipe for Creating Multimodal Aligned Datasets for Sequential Tasks
16. Benchmarking Multimodal Regex Synthesis with Complex Structures
17. Clue: Cross-modal Coherence Modeling for Caption Generation
18. Multimodal Neural Graph Memory Networks for Visual Question Answering
19. Cross-Modality Relevance for Reasoning on Language and Vision
20. Unsupervised Multimodal Neural Machine Translation with Pseudo Visual Pivoting
### IJCAI 22
1. Cross-modal Representation Learning and Relation Reasoning for Bidirectional Adaptive Manipulation
2. Unsupervised Misaligned Infrared and Visible Image Fusion via Cross-Modality Image Generation and Registration
3. SHAPE: An Unified Approach to Evaluate the Contribution and Cooperation of Individual Modalities
4. MMT: Multi-Way Multi-Modal Transformer for Multimodal Learning
5. AutoAlign: Pixel-Instance Feature Aggregation for Multi-Modal 3D Object Detection
6. Lightweight Bimodal Network for Single-Image Super-Resolution via Symmetric CNN and Recursive Transformer
7. Unsupervised Multi-Modal Medical Image Registration via Discriminator-Free Image-to-Image Translation
8. Targeted Multimodal Sentiment Classification based on Coarse-to-Fine Grained Image-Target Matching
9. Recipe2Vec: Multi-modal Recipe Representation Learning with Graph Neural Networks
10. PlaceNet: Neural Spatial Representation Learning with Multimodal Attention
11. Representation Learning for Compressed Video Action Recognition via Attentive Cross-modal Interaction with Motion Enhancement
12. MA-ViT: Modality-Agnostic Vision Transformers for Face Anti-Spoofing
13. Unsupervised Voice-Face Representation Learning by Cross-Modal Prototype Contrast
14. MFAN: Multi-modal Feature-enhanced Attention Networks for Rumor Detection

### IJCAI 21

1. Dig into Multi-modal Cues for Video Retrieval with Hierarchical Alignment
2. Deep Unified Cross-Modality Hashing by Pairwise Data Alignment
3. Weakly Supervised Dense Video Captioning via Jointly Usage of Knowledge Distillation and Cross-modal Matching
4. MRD-Net: Multi-Modal Residual Knowledge Distillation for Spoken Question Answering
5. Rethinking Label-Wise Cross-Modal Retrieval from A Semantic Sharing Perspective
6. MDNN: A Multimodal Deep Neural Network for Predicting Drug-Drug Interaction Events
7. Modality-aware Style Adaptation for RGB-Infrared Person Re-Identification
8. Multimodal Transformer Network for Pedestrian Trajectory Prediction
9. SalientSleepNet: Multimodal Salient Wave Detection Network for Sleep Staging
10. UIBert: Learning Generic Multimodal Representations for UI Understanding

### IJCAI 20

1. A Similarity Inference Metric for RGB-Infrared Cross-Modality Person Re-identification
2. Set and Rebase: Determining the Semantic Graph Connectivity for Unsupervised Cross-Modal Hashing
3. A Modal Logic for Joint Abilities under Strategy Commitments
4. Embodied Multimodal Multitask Learning
5. Triple-GAIL: A Multi-Modal Imitation Learning Framework with Generative Adversarial Nets
6. “The Squawk Bot”∗ : Joint Learning Of Time Series And Text Data Modalities For Automated Financial Information Filtering
7. Interpretable Multimodal Learning for Intelligent Regulation in Online Payment Systems

### EMNLP 21

1. Improving Multimodal fusion via Mutual Dependency Maximisation
2. Text2Mol: Cross-Modal Molecule Retrieval with Natural Language Queries
3. Iconary: A Pictionary-Based Game for Testing Multimodal Communication with Drawings and Text
4. Multimodal Phased Transformer for Sentiment Analysis
5. CTAL: Pre-training Cross-modal Transformer for Audio-and-Language Representations
6. Vision Guided Generative Pre-trained Language Models for Multimodal Abstractive Summarization
7. How to Leverage Multimodal EHR Data for Better Medical Predictions?
8. Joint Multi-modal Aspect-Sentiment Analysis with Auxiliary Cross-modal Relation Detection
9. Multi-Modal Open-Domain Dialogue
10. SIMMC 2.0: A Task-oriented Dialog Dataset for Immersive Multimodal Conversations
11. Hitting your MARQ: Multimodal ARgument Quality Assessment in Long Debate Video
12. NewsCLIPpings: Automatic Generation of Out-of-Context Multimodal Media
13. Vision Matters When It Should: Sanity Checking Multimodal Machine Translation Models
14. Unimodal and Crossmodal Refinement Network for Multimodal Sequence Fusion
15. Improving Multimodal Fusion with Hierarchical Mutual Information Maximization for Multimodal Sentiment Analysis
16. On Pursuit of Designing Multi-modal Transformer for Video Grounding
17. Vision-and-Language or Vision-for-Language? On Cross-Modal Influence in Multimodal Transformers
18. M-Arg: Multimodal Argument Mining Dataset for Political Debates with Audio and Transcripts
19. VQA-MHUG: A Gaze Dataset to Study Multimodal Neural Attention in Visual Question Answering
20. Point-of-Interest Type Prediction using Text and Images（首个根据推特的文本和图像判断POI类型）

Findings of EMNLP 21

20. Self-supervised Contrastive Cross-Modality Representation Learning for Spoken Question Answering
21. Cross-Modal Retrieval Augmentation for Multi-Modal Classification
22. What Does Your Smile Mean? Jointly Detecting Multi-Modal Sarcasm and Sentiment Using Quantum Probability
23. Entity-level Cross-modal Learning Improves Multi-modal Machine Translation
24. Which is Making the Contribution: Modulating Unimodal and Cross-modal Dynamics for Multimodal Sentiment Analysis
25. Optimal Neural Program Synthesis from Multimodal Specifications
26. MIRTT: Learning Multimodal Interaction Representations from Trilinear Transformers for Visual Question Answering
27. DialogueTRM: Exploring Multi-Modal Emotion Dynamics in Conversations
28. An animated picture says at least a thousand words: Selecting Gif-based Replies in Multimodal Dialog
29. MURAL: Multimodal, Multitask Retrieval Across Languages
30. MSD: Saliency-aware Knowledge Distillation for Multimodal Understanding
31. MOMENTA: A Multimodal Framework for Detecting Harmful Memes and Their Targets

### EMNLP 20

1. X-LXMERT: Paint, Caption and Answer Questions with Multi-Modal Transformers
2. Generating Image Descriptions via Sequential Cross-Modal Alignment Guided by Human Gaze
3. Multimodal Routing: Improving Local and Global Interpretability of Multimodal Language Analysis
4. Multi-modal Multi-label Emotion Detection with Modality and Label Dependence
5. Multistage Fusion with Forget Gate for Multimodal Summarization in Open-Domain Videos
6. Detecting Cross-Modal Inconsistency to Defend Against Neural Fake News
8. VMSMO: Learning to Generate Multimodal Summary for Video-based News Articles
9. Does my multimodal model learn cross-modal interactions? It’s harder to tell than you might think!
10. CMU-MOSEAS: A Multimodal Language Dataset for Spanish, Portuguese, German and French
11. Cross-Media Keyphrase Prediction: A Unified Framework with Multi-Modality Multi-Head Attention and Image Wordings
12. Multimodal Joint Attribute Prediction and Value Extraction for E-commerce Product
13. Unsupervised Natural Language Inference via Decoupled Multimodal Contrastive Learning
14. MAF: Multimodal Alignment Framework for Weakly-Supervised Phrase Grounding

Findings of EMNLP 20

15. Open-Ended Visual Question Answering by Multi-Modal Domain Adaptation
16. Dual Low-Rank Multimodal Fusion
17. DocStruct: A Multimodal Method to Extract Hierarchy Structure in Document for General Form Understanding
18. Modeling Intra and Inter-modality Incongruity for Multi-Modal Sarcasm Detection
19. MultiDM-GCN: Aspect-guided Response Generation in Multi-domain Multi-modal Dialogue System using Graph Convolutional Network
20. Fine-Grained Grounding for Multimodal Speech Recognition
21. MMFT-BERT: Multimodal Fusion Transformer with BERT Encodings for Visual Question Answering

### NAACL 22

1. Analyzing Modality Robustness in Multimodal Sentiment Analysis
2. Beyond Emotion: A Multi-Modal Dataset for Human Desire Understanding
3. Twitter-COMMs: Detecting Climate, COVID, and Military Multimodal Misinformation
4. A Study of Syntactic Multi-Modality in Non-Autoregressive Machine Translation
5. Modal Dependency Parsing via Language Model Priming
6. Multimodal Dialogue State Tracking
7. GMN: Generative Multi-modal Network for Practical Document Information Extraction
8. A Computational Acquisition Model for Multimodal Word Categorization
9. COGMEN: COntextualized GNN based Multimodal Emotion recognitioN
10. Cross-modal Contrastive Learning for Speech Translation
11. Visual Commonsense in Pretrained Unimodal and Multimodal Models
12. MCSE: Multimodal Contrastive Learning of Sentence Embeddings
13. JointLK: Joint Reasoning with Language Models and Knowledge Graphs for Commonsense Question Answering（作者把LM和KG看做是两个modality）
14. KAT: A Knowledge Augmented Transformer for Vision-and-Language

Findings of NNACL 22

13. Multimodal Intent Discovery from Livestream Videos
14. Learning to Embed Multi-Modal Contexts for Situated Conversational Agents
15. MM-Claims: A Dataset for Multimodal Claim Detection in Social Media
16. Cross-Lingual Cross-Modal Consolidation for Effective Multilingual Video Corpus Moment Retrieval
17. CLMLF:A Contrastive Learning and Multi-Layer Fusion Method for Multimodal Sentiment Detection

### NAACL 21

1. MTAG: Modal-Temporal Attention Graph for Unaligned Human Multimodal Language Sequences
2. Improving Cross-Modal Alignment in Vision Language Navigation via Syntactic Information
3. Multilingual Multimodal Pre-training for Zero-Shot Cross-Lingual Transfer of Vision-Language Models
4. MUSER: MUltimodal Stress Detection using Emotion Recognition as an Auxiliary Task
5. Cross-lingual Cross-modal Pretraining for Multimodal Retrieval
6. An Empirical Investigation of Bias in the Multimodal Analysis of Financial Earnings Calls
7. Multimodal End-to-End Sparse Model for Emotion Recognition
8. MIMOQA: Multimodal Input Multimodal Output Question Answering
9. Towards Sentiment and Emotion aided Multi-modal Speech Act Classification in Twitter
10. MM-AVS: A Full-Scale Dataset for Multi-modal Summarization
11. Larger-Context Tagging: When and Why Does It Work?

### NAACL

Quantifying the visual concreteness of words and topics in multimodal datasets. NAACL 18

### WWW 22

1. Multimodal Continual Graph Learning with Neural Architecture Search
2. Modality Matches Modality: Pretraining Modality-Disentangled Item Representations for Recommendation
3. Cross-modal Ambiguity Learning for Multimodal Fake News Detection
4. A Duo-generative Approach to Explainable Multimodal COVID-19 Misinformation Detection
5. On Explaining Multimodal Hateful Meme Detection Models

### WWW 21

1. High-Dimensional Sparse Cross-Modal Hashing with Fine-Grained Similarity Embedding

### WWW 20

1. Nowhere to Hide: Cross-modal Identity Leakage between Biometrics and Devices
2. Adversarial Multimodal Representation Learning for Click-Through Rate Prediction
3. Learning from Cross-Modal Behavior Dynamics with Graph-Regularized Neural Contextual Bandit
4. Learning to Respond with Stickers: A Framework of Unifying Multi-Modality in Multi-Turn Dialog
5. Domain Adaptive Multi-Modality Neural Attention Network for Financial Forecasting
6. A Multimodal Variational Encoder-Decoder Framework for Micro-video Popularity Prediction
7. Multimodal Post Attentive Profiling for Influencer Marketing
8. TransModality: An End2End Fusion Method with Transformer for Multimodal Sentiment Analysis

### ACM MM 21

1. Theophany: Multimodal Speech Augmentation in Instantaneous Privacy Channels
2. Multimodal Asymmetric Dual Learning for Unsupervised Eyeglasses Removal
3. HetEmotionNet: Two-Stream Heterogeneous Graph Recurrent Neural Network for Multi-modal Emotion Recognition
4. Graph Convolutional Multi-modal Hashing for Flexible Multimedia Retrieval
5. A Stepwise Matching Method for Multimodal Image based on Cascaded Network
6. Learning What and When to Drop: Adaptive Multimodal and Contextual Dynamics for Emotion Recognition in Conversation
7. Differentiated Learning for Multi-Modal Domain Adaptation
8. Database-adaptive Re-ranking for Enhancing Cross-modal Image Retrieval
9. Exploiting BERT For Multimodal Target Sentiment Classification Through Input Space Translation
10. Pre-training Graph Transformer with Multimodal Side Information for Recommendation
11. Simplifying Multimodal Emotion Recognition with Single Eye Movement Modality
12. M3TR: Multi-modal Multi-label Recognition with Transformer
13. Multimodal Dialog System: Relational Graph-based Context-aware Question Understanding
14. Towards a Unified Middle Modality Learning for Visible-Infrared Person Re-Identification
15. ROSITA: Enhancing Vision-and-Language Semantic Alignments via Cross- and Intra-modal Knowledge Integration
16. Joint-teaching: Learning to Refine Knowledge for Resource-constrained Unsupervised Cross-modal Retrieval
17. Cross-modal Consensus Network for Weakly Supervised Temporal Action Localization
18. Searching a Hierarchically Aggregated Fusion Architecture for Fast Multi-Modality Image Fusion
19. Deep Self-Supervised t-SNE for Multi-modal Subspace Clustering
20. Multimodal Video Summarization via Time-Aware Transformers
21. Local Graph Convolutional Networks for Cross-Modal Hashing
22. Cross-modality Discrepant Interaction Network for RGB-D Salient Object Detection
23. Conceptual and Syntactical Cross-modal Alignment with Cross-level Consistency for Image-Text Matching
24. Unsupervised Cross-Modal Distillation for Thermal Infrared Tracking
25. Understanding Chinese Video and Language via Contrastive Multimodal Pre-Training
26. Cross-Modal Retrieval and Synthesis (X-MRS): Closing the Modality Gap in Shared Representation Learning
27. Multimodal Compatibility Modeling via Exploring the Consistent and Complementary Correlations
28. Missing Data Imputation for Solar Yield Prediction using Temporal Multi-Modal Variational Auto-Encoder
29. Cross-Modal Recipe Embeddings by Disentangling Recipe Contents and Dish Styles
30. Cross-modal Self-Supervised Learning for Lip Reading: When Contrastive Learning meets Adversarial Training
31. Multi-Modal Multi-Instance Learning for Retinal Disease Recognition
32. Transformer-based Feature Reconstruction Network for Robust Multimodal Sentiment Analysis
33. Efficient Multi-Modal Fusion with Diversity Analysis
34. SINGA-Easy: An Easy-to-Use Framework for MultiModal Analysis
35. CoCo-BERT: Improving Video-Language Pre-training with Contrastive Cross-modal Matching and Denoising
36. DRDF: Determining the Importance of Different Multimodal Information with Dual-Router Dynamic Framework
37. MCCN: Multimodal Coordinated Clustering Network for Large-Scale Cross-modal Retrieval
38. Heterogeneous Feature Fusion and Cross-modal Alignment for Composed Image Retrieval
39. Exploring Graph-Structured Semantics for Cross-Modal Retrieval
40. Cross Modal Compression: Towards Human-comprehensible Semantic Compression
41. Sensor-Augmented Egocentric-Video Captioning with Dynamic Modal Attention
42. Cascade Cross-modal Attention Network for Video Actor and Action Segmentation from a Sentence
43. Cross-Modal Joint Prediction and Alignment for Composed Query Image Retrieval
44. MM-Flow: Multi-modal Flow Network for Point Cloud Completion
45. Meta Self-Paced Learning for Cross-Modal Matching
46. Learning Disentangled Factors from Paired Data in Cross-Modal Retrieval: An Implicit Identifiable VAE Approach
47. Multi-Modal Sarcasm Detection with Interactive In-Modal and Cross-Modal Graphs
48. Fine-grained Cross-modal Alignment Network for Text-Video Retrieval
49. Cross-Modal Generalization: Learning in Low Resource Modalities via Meta-Alignment
50. Hierarchical Multi-Task Learning for Diagram Question Answering with Multi-Modal Transformer
51. Product-oriented Machine Translation with Cross-modal Cross-lingual Pre-training
52. Graph Neural Networks for Knowledge Enhanced Visual Representation of Paintings

### ACM MM 20

1. MM-Hand: 3D-Aware Multi-Modal Guided Hand Generative Network for 3D Hand Pose Synthesis
2. Cross-Modal Omni Interaction Modeling for Phrase Grounding
3. VideoIC: A Video Interactive Comments Dataset and Multimodal Multitask Learning for Comments Generation
4. Multimodal Dialog Systems via Capturing Context-aware Dependencies of Semantic Elements
5. Semi-supervised Multi-modal Emotion Recognition with Cross-Modal Distribution Matching
6. Adaptive Multimodal Fusion for Facial Action Units Recognition
7. Crossing You in Style: Cross-modal Style Transfer from Music to Visual Arts
8. Incomplete Cross-modal Retrieval with Dual-Aligned Variational Autoencoders
9. Joint Attribute Manipulation and Modality Alignment Learning for Composing Text and Image to Image Retrieval
10. Supervised Hierarchical Deep Hashing for Cross-Modal Retrieval
11. Multi-modal Attentive Graph Pooling Model for Community Question Answer Matching
12. Towards Modality Transferable Visual Information Representation with Optimal Model Compression
13. Deep Multimodal Neural Architecture Search
14. Cross-domain Cross-modal Food Transfer
15. Finding Achilles’ Heel: Adversarial Attack on Multi-modal Action Recognition
16. Cross-Modal Relation-Aware Networks for Audio-Visual Event Localization
17. Learning Deep Multimodal Feature Representation with Asymmetric Multi-layer Fusion
18. Deep Multi-modality Soft-decoding of Very Low Bit-rate Face Videos
19. Jointly Cross- and Self-Modal Graph Attention Network for Query-Based Moment Localization
20. STRONG: Spatio-Temporal Reinforcement Learning for Cross-Modal Video Moment Localization
21. Improving Intra- and Inter-Modality Visual Relation for Image Captioning
22. Multimodal Attention with Image Text Spatial Relationship for OCR-Based Image Captioning
23. ADHD Intelligent Auxiliary Diagnosis System Based on Multimodal Information Fusion (Demo)
24. A Cross-modality and Progressive Person Search System (Demo)
25. Multimodal Deep Learning for Social Media Popularity Prediction With Attention Mechanism
26. Video Relation Detection with Trajectory-aware Multi-modal Features
27. XlanV Model with Adaptively Multi-Modality Feature Fusing for Video Captioning
28. A Quantitative Comparison of Different Machine Learning Approaches for Human Spermatozoa Quality Prediction Using Multimodal Datasets
29. Learning Self-Supervised Multimodal Representations of Human Behaviour
30. Cross-modal Non-linear Guided Attention and Temporal Coherence in Multi-modal Deep Video Models
31. Look, Read and Feel: Benchmarking Ads Understanding with Multimodal Multitask Learning
32. Down to the Last Detail: Virtual Try-on with Fine-grained Details
33. MEmoR: A Dataset for Multimodal Emotion Reasoning in Videos
34. Modeling both Intra- and Inter-modal Influence for Real-Time Emotion Detection in Conversations
35. Transformer-based Label Set Generation for Multi-modal Multi-label Emotion Detection
36. CM-BERT: Cross-Modal BERT for Text-Audio Sentiment Analysis
37. Label Embedding Online Hashing for Cross-Modal Retrieval
38. Class-Aware Modality Mix and Center-Guided Metric Learning for Visible-Thermal Person Re-Identification
39. RGB2LIDAR: Towards Solving Large-Scale Cross-Modal Visual Localization
40. MMFL: Multimodal Fusion Learning for Text-Guided Image Inpainting
41. MISA: Modality-Invariant and -Specific Representations for Multimodal Sentiment Analysis
42. Multi-modal Cooking Workflow Construction for Food Recipes
43. Adaptive Temporal Triplet-loss for Cross-modal Embedding Learning
44. Deep-Modal: Real-Time Impact Sound Synthesis for Arbitrary Shapes
45. K-armed Bandit based Multi-modal Network Architecture Search for Visual Question Answering
46. Dynamic Context-guided Capsule Network for Multimodal Machine Translation
47. KBGN: Knowledge-Bridge Graph Network for Adaptive Vision-Text Reasoning in Visual Dialogue
48. Learning Modality-Invariant Latent Representations for Generalized Zero-shot Learning
49. Boosting Continuous Sign Language Recognition via Cross Modality Augmentation
50. Towards More Explainability: Concept Knowledge Mining Network for Event Recognition
51. Memory-Based Network for Scene Graph with Unbalanced Relations

### NeurIPS 21

1. Exploring Cross-Video and Cross-Modality Signals for Weakly-Supervised Audio-Visual Video Parsing
2. Cross-Modal Domain Adaptation for Cost-Efficient Visual Reinforcement Learning
3. End-to-end Multi-modal Video Temporal Grounding
4. Multi-modal Dependency Tree for Video Captioning
5. Perceptual Score: What Data Modalities Does Your Model Perceive?
6. UFC-BERT: Unifying Multi-Modal Controls for Conditional Image Synthesis
7. Learning with Noisy Correspondence for Cross-modal Matching
8. Probing Inter-modality: Visual Parsing with Self-Attention for Vision-Language Pre-training
9. Modality-Agnostic Topology Aware Localization
10. Explainable Semantic Space by Grounding Language to Vision with Cross-Modal Contrastive Learning
11. Raw Nav-merge Seismic Data to Subsurface Properties with MLP based Multi-Modal Information Unscrambler
12. What Makes Multi-modal Learning Better than Single (Provably)

### NeurIPS 020

1. Labelling unlabelled videos from scratch with multi-modal self-supervision
2. Self-Supervised Learning by Cross-Modal Audio-Video Clustering
3. CodeCMR: Cross-Modal Retrieval For Function-Level Binary Source Code Matching
4. A Contour Stochastic Gradient Langevin Dynamics Algorithm for Simulations of Multi-modal Distributions
5. An implicit function learning approach for parametric modal regression
6. Removing Bias in Multi-modal Classifiers: Regularization by Maximizing Functional Entropies

### AAAI 22

1. Event-Image Fusion Stereo Using Cross-Modality Feature Propagation
2. Cross-Modal Mutual Learning for Audio-Visual Speech Recognition and Manipulation
3. Cross-Modal Federated Human Activity Recognition via Modality-Agnostic and Modality-Specific Representation Learning
4. Detecting Human-Object Interactions with Object-Guided Cross-Modal Calibrated Semantics
5. Show Your Faith: Cross-Modal Conﬁdence-Aware Network for Image-Text Matching
6. Modality-Adaptive Mixup and Invariant Decomposition for RGB-Infrared Person Re-identiﬁcation
7. MuMu: Cooperative Multitask Learning-Based Guided Multimodal Fusion
8. Cross-Modal Object Tracking: Modality-Aware Representations and A Uniﬁed Benchmark
9. You Only Infer Once: Cross-Modal Meta-Transfer for Referring Video Object Segmentation
10. Multi-Modal Perception Attention Network with Self-Supervised Learning for Audio-Visual Speaker Tracking
11. Visual Sound Localization in the Wild by Cross-Modal Interference Erasing
12. TVT: Three-Way Vision Transformer through Multi-Modal Hypersphere Learning for Zero-Shot Sketch-Based Image Retrieval
13. Interact, Embed, and EnlargE: Boosting Modality-Specific Representations for Multi-Modal Person Re-identification
14. MAGIC: Multimodal relAtional Graph adversarIal inferenCe for Diverse and Unpaired Text-Based Image Captioning
15. Promoting Single-Modal Optical Flow Network for Diverse Cross-Modal Flow Estimation
16. Event-Aware Multimodal Mobility Nowcasting
17. Online Enhanced Semantic Hashing: Towards Effective and Efficient Retrieval for Streaming Multi-Modal Data
18. AXM-Net: Implicit Cross-Modal Feature Alignment for Person Re-identiﬁcation
19. Monocular Camera-Based Point-Goal Navigation by Learning Depth Channel and Cross-Modality Pyramid Fusion
20. Multimodal Adversarially Learned Inference with Factorized Discriminators
21. Learning Aligned Cross-Modal Representation for Generalized Zero-Shot Classification
22. Regularized Modal Regression on Markov-Dependent Observations: A Theoretical Assessment
23. Multi-Head Modularization to Leverage Generalization Capability in Multi-Modal Networks
24. BM-NAS: Bilevel Multimodal Neural Architecture Search
25. Tailor Versatile Multi-Modal Learning for Multi-Label Emotion Recognition
26. Bi-CMR: Bidirectional Reinforcement Guided Hashing for Effective Cross-Modal Retrieval
27. Cross-Modal Coherence for Text-to-Image Retrieval
28. Nice Perfume. How Long Did You Marinate in It? Multimodal Sarcasm Explanation
29. Are Vision-Language Transformers Learning Multimodal Representations? A Probing Perspective
30. Hierarchical Cross-Modality Semantic Correlation Learning Model for Multimodal Summarization
31. UniMS: A Uniﬁed Framework for Multimodal Summarization with Knowledge Distillation
32. Evaluating Explainable AI on a Multi-Modal Medical Imaging Task: Can Existing Algorithms Fulfill Clinical Requirements?
33. Sentiment and Emotion-Aware Multi-Modal Complaint Identiﬁcation
34. D-vlog: Multimodal Vlog Dataset for Depression Detection
35. An End-to-End Traditional Chinese Medicine Constitution Assessment System Based on Multimodal Clinical Feature Representation and Fusion
36. ALLURE * : A Multi-Modal Guided Environment for Helping Children Learn to Solve a Rubik’s Cube with Automatic Solving and Interactive Explanations
37. A Multimodal Fusion-Based LNG Detection for Monitoring Energy Facilities (Student Abstract)
38. Using Multimodal Data and AI to Dynamically Map Flood Risk
39. College Student Retention Risk Analysis from Educational Database Using Multi-Task Multi-Modal Neural Fusion

### AAAI 21

1. Embracing Domain Differences in Fake News: Cross-domain Fake News Detection using Multi-modal Data
2. Dynamic Graph Representation Learning for Video Dialog via Multi-Modal Shufﬂed Transformers
3. SMIL: Multimodal Learning with Severely Missing Modality
4. CHEF: Cross-Modal Hierarchical Embeddings for Food Domain Retrieval
5. Dual Adversarial Graph Neural Networks for Multi-label Cross-modal Retrieval
6. Deep Probabilistic Imaging: Uncertainty Quantiﬁcation and Multi-modal Solution Characterization for Computational Imaging
7. Efficient Object-Level Visual Context Modeling for Multimodal Machine Translation: Masking Irrelevant Objects Helps Grounding
8. Conﬁdence-aware Non-repetitive Multimodal Transformers for TextCaps
9. Amodal Segmentation Based on Visible Region Segmentation and Shape Prior
10. Multimodal Fusion via Teacher-Student Network for Indoor Action Recognition
11. Demodalizing Face Recognition with Synthetic Samples
12. Joint Color-irrelevant Consistency Learning and Identity-aware Modality Adaptation for Visible-infrared Cross Modality Person Re-identiﬁcation
13. Robust Multi-Modality Person Re-identiﬁcation
14. Deep Graph-neighbor Coherence Preserving Network for Unsupervised Cross-modal Hashing
15. Learning Intuitive Physics with Multimodal Generative Models
16. VMLoc: Variational Fusion For Learning-Based Multimodal Camera Localization
17. Noise Estimation Using Density Estimation for Self-Supervised Multimodal Learning
18. Deep Mutual Information Maximin for Cross-Modal Clustering
19. MUFASA: Multimodal Fusion Architecture Search for Electronic Health Records
20. Enhanced Audio Tagging via Multi- to Single-Modal Teacher-Student Mutual Learning
21. Learning Modality-Speciﬁc Representations with Self-Supervised Multi-Task Learning for Multimodal Sentiment Analysis
22. Theoretical Analyses of Multi-Objective Evolutionary Algorithms on Multi-Modal Objectives
23. Humor Knowledge Enriched Transformer for Understanding Multimodal Humor
24. Audio-Oriented Multimodal Machine Comprehension via Dynamic Inter- and Intra-modality Attention
25. MELINDA: A Multimodal Dataset for Biomedical Experiment Method Classification
26. Multi-modal Multi-label Emotion Recognition with Heterogeneous Hierarchical Message Passing
27. LAMS: A Location-aware Approach for Multimodal Summarization (Student Abstract)
28. Fashion Focus: Multi-modal Retrieval System for Video Commodity Localization in E-commerce
29. Data-Driven Multimodal Patrol Planning for Anti-poaching
30. Screening for Depressed Individuals by Using Multimodal Social Media Data
31. Multi-modal User Intent Classiﬁcation Under the Scenario of Smart Factory (Student Abstract)

### AAAI 20

1. Infrared-Visible Cross-Modal Person Re-Identiﬁcation with an X Modality
2. MANYMODAL QA: Modality Disambiguation and QA over Diverse Inputs
3. Privacy Enhanced Multimodal Neural Representations for Emotion Recognition
4. Modality-Balanced Models for Visual Dialogue
5. Aspect-Aware Multimodal Summarization for Chinese E-Commerce Products
6. Semi-Supervised Multi-Modal Learning with Balanced Spectral Decomposition
7. Modality to Modality Translation: An Adversarial Representation Learning and Graph Fusion Network for Multimodal Fusion
8. Cross-Modal Attention Network for Temporal Inconsistent Audio-Visual Event Localization
9. Crisis-DIAS: Towards Multimodal Damage Analysis - Deployment, Challenges and Assessment
10. Towards Cross-Modality Medical Image Segmentation with Online Mutual Knowledge Distillation
11. Learning Multi-Modal Biomarker Representations via Globally Aligned Longitudinal Enrichments
12. Urban2Vec: Incorporating Street View Imagery and POIs for Multi-Modal Urban Neighborhood Embedding
13. M3ER: Multiplicative Multimodal Emotion Recognition using Facial, Textual, and Speech Cues
14. Cross-Modal Subspace Clustering via Deep Canonical Correlation Analysis
15. Learning Relationships between Text, Audio, and Video via Deep Canonical Correlation for Multimodal Language Analysis
16. Visual Agreement Regularized Training for Multi-Modal Machine Translation
17. Learning Long- and Short-Term User Literal-Preference with Multimodal Hierarchical Transformer Network for Personalized Image Caption
18. Multimodal Summarization with Guidance of Multimodal Reference
19. Factorized Inference in Deep Markov Models for Incomplete Multimodal Time Series
20. MULE: Multimodal Universal Language Embedding
21. Unicoder-VL: A Universal Encoder for Vision and Language by Cross-Modal Pre-Training
22. Attention-Based Multi-Modal Fusion Network for Semantic Scene Completion
23. Multimodal Structure-Consistent Image-to-Image Translation
24. Learning Cross-Aligned Latent Embeddings for Zero-Shot Cross-Modal Retrieval
25. Learning Cross-Modal Context Graph for Visual Grounding
26. Multimodal Interaction-Aware Trajectory Prediction in Crowded Space
27. Cross-Modality Paired-Images Generation for RGB-Infrared Person Re-Identification
28. Adaptive Cross-Modal Embeddings for Image-Text Alignment
29. Mining on Heterogeneous Manifolds for Zero-Shot Cross-Modal Image Retrieval
30. Cross-Modality Attention with Semantic Graph Embedding for Multi-Label Classification
31. Adaptive Unimodal Cost Volume Filtering for Deep Stereo Matching
32. Diana’s World: A Situated Multimodal Interactive Agent
33. Interpreting Multimodal Machine Learning Models Trained for Emotion Recognition to Address Robustness and Privacy Concerns
34. Trimodal Attention Module for Multimodal Sentiment Analysis (Student Abstract)
35. SpotFake+: A Multimodal Framework for Fake News Detection via Transfer Learning (Student Abstract)

### SIGIR 22

1. CRET: Cross-Modal Retrieval Transformer for Efficient Text-Video Retrieval
2. Multimodal Disentanglement Variational AutoEncoders for Zero-Shot Cross-Modal Retrieval
3. Bit-aware Semantic Transformer Hashing for Multi-modal Retrieval
4. V2P: Vision-to-Prompt based Multi-Modal Product Summary Generation
5. Progressive Learning for Image Retrieval with Hybrid-Modality Queries
6. Tag-assisted Multimodal Sentiment Analysis under Uncertain Missing Modalities
7. A Multitask Framework for Sentiment, Emotion and Sarcasm aware Cyberbullying Detection from Multi-modal Code-Mixed Memes
8. Multi-modal Graph Contrastive Learning for Micro-video Recommendation
9. Cross-Probe BERT for Fast Cross-Modal Search
10. MM-Rec: Visiolinguistic Model Empowered Multimodal News Recommendation
11. Modality-Balanced Embedding for Video Retrieval
12. An Efficient Fusion Mechanism for Multimodal Low-resource Setting
13. Next Point-of-Interest Recommendation with Auto-Correlation Enhanced Multi-Modal Transformer Network
14. MET-Meme: a Multimodal Meme Dataset Rich in Metaphors
15. MuMiN: A Large-Scale Multilingual Multimodal Fact-Checked Misinformation Social Network Dataset
16. Golden Retriever: A Real-Time Multi-Modal Text-Image Retrieval System with the Ability to Focus
17. An Intelligent Advertisement Short Video Production System via Multi-Modal Retrieval

### SIGIR 21

1. DepressionNet: A Novel Summarization Boosted Deep Framework for Depression Detection on Social Media
2. Hierarchical Multi-modal Contextual Attention Network for Fake News Detection
3. Hybrid Fusion with Intra- and Cross-Modality Attention for Image-Recipe Retrieval
4. Multimodal Activation: Awakening Dialog Robots without Wake Words
5. Privacy Protection in Deep Multi-modal Retrieval
6. MMConv: An Environment for Multimodal Conversational Search across Multiple Domains
7. Multi-Modal Supplementary-Complementary Summarization using Multi-Objective Optimization
8. Dynamic Modality Interaction Modeling for Image-Text Retrieval
9. Hierarchical Cross-Modal Graph Consistency Learning for Video-Text Retrieval
10. PAN: Prototype-based Adaptive Network for Robust Cross-modal Retrieval
11. Heterogeneous Attention Network for Effective and Efficient Cross-modal Retrieval
12. Towards Multi-Modal Conversational Information Seeking
13. FedCMR: Federated Cross-Modal Retrieval
14. Cross-Graph Attention Enhanced Multi-Modal Correlation Learning for Fine-Grained Image-Text Retrieval
15. Deep Music Retrieval for Fine-Grained Videos by Exploiting Cross-Modal-Encoded Voice-Overs
16. AliMe Avatar: Multi-modal Content Production and Presentation for Live-streaming E-commerce
17. QuTI! Quantifying Text-Image Consistency in Multimodal Documents

### SIGIR 20

1. Fashion Compatibility Modeling through a Multi-modal Try-on-guided Scheme
2. Tree-Augmented Cross-Modal Encoding for Complex-Query Video Retrieval
3. Nonlinear Robust Discrete Hashing for Cross-Modal Retrieval
4. Joint-modal Distribution-based Similarity Hashing for Large-scale Unsupervised Deep Cross-modal Retrieval
5. Web Table Retrieval using Multimodal Deep Learning
6. Correlated Features Synthesis and Alignment for Zero-shot Cross-modal Retrieval
7. MGNN: A Multimodal Graph Neural Network for Predicting the Survival of Cancer Patients
8. Multi-Modal Summary Generation using Multi-Objective Optimization
9. Multi-Level Multimodal Transformer Network for Multimodal Recipe Comprehension
10. MHM: Multi-modal Clinical Data based Hierarchical Multi-label Diagnosis Prediction
11. FashionBERT: Text and Image Matching with Adaptive Loss for Cross-modal Retrieval 

### CVPR 22

1. Cross-modal Map Learning for Vision and Language Navigation
2. PhoCaL: A Multi-Modal Dataset for Category-Level Object Pose Estimation with Photometrically Challenging Objects
3. Everything at Once – Multi-modal Fusion Transformer for Video Retrieval
4. CrossPoint: Self-Supervised Cross-Modal Contrastive Learning for 3D Point Cloud Understanding
5. Versatile Multi-Modal Pre-Training for Human-Centric Perception
6. Lite-MDETR: A Lightweight Multi-Modal Detector
7. Cross Modal Retrieval with Querybank Normalisation
8. Modeling Motion with Multi-Modal Features for Text-Based Video Segmentation
9. Tencent-MVSE: A Large-Scale Benchmark Dataset for Multi-Modal Video Similarity Evaluation
10. Open-Vocabulary Instance Segmentation via Robust Cross-Modal Pseudo-Labeling
11. RFNet: Unsupervised Network for Mutually Reinforcing Multi-modal Image Registration and Fusion
12. EI-CLIP: Entity-aware Interventional Contrastive Learning for E-commerce Cross-modal Retrieval
13. Learning Hierarchical Cross-Modal Association for Co-Speech Gesture Generation
14. X-Trans2Cap: Cross-Modal Knowledge Transfer using Transformer for 3D Dense Captioning
15. MM-TTA: Multi-Modal Test-Time Adaptation for 3D Semantic Segmentation
16. Interact before Align: Leveraging Cross-Modal Knowledge for Domain Adaptive Action Recognition
17. Cross-Modal Transferable Adversarial Attacks from Images to Videos
18. Open-Domain, Content-based, Multi-modal Fact-checking of Out-of-Context Images via Online Resources
19. Cross-Modal Perceptionist: Can Face Geometry be Gleaned from Voices?
20. Cross-modal Representation Learning for Zero-shot Action Recognition
21. Audio-visual Generalised Zero-shot Learning with Cross-modal Attention and Language
22. Robust Cross-Modal Representation Learning with Progressive Self-Distillation
23. Multi-modal Alignment using Representation Codebook
24. Wnet: Audio-Guided Video Object Segmentation via Wavelet-Based Cross-Modal Denoising Networkss
25. Text2Pos: Text-to-Point-Cloud Cross-Modal Localization
26. Reading to Listen at the Cocktail Party: Multi-Modal Speech Separation
27. Cross-modal Background Suppression for Audio-Visual Event Localization
28. Mutual Quantization for Cross-Modal Search with Noisy Labels
29. Learning Modal-Invariant and Temporal-Memory for Video-based Visible-Infrared Person Re-Identification
30. Multi-modal Extreme Classification
31. COTS: Collaborative Two-Stream Vision-Language Pre-Training Model for Cross-Modal Retrieval
32. ViSTA: Vision and Scene Text Aggregation for Cross-Modal Retrieval
33. CroMo: Cross-Modal Learning for Monocular Depth Estimation
34. CAT-Det: Contrastively Augmented Transformer for Multi-modal 3D Object Detection
35. Beyond a Pre-Trained Object Detector: Cross-Modal Textual and Visual Context for Image Captioning
36. X-Pool: Cross-Modal Language-Video Attention for Text-Video Retrieval
37. UMT: Unified Multi-modal Transformers for Joint Video Moment Retrieval and Highlight Detection
38. Multi-Modal Dynamic Graph Transformer for Visual Grounding
39. DeepFusion: Lidar-Camera Deep Fusion for Multi-Modal 3D Object Detection
40. M3L: Language-based Video Editing via Multi-Modal Multi-Level Transformers
41. Generalizable Cross-modality Medical Image Segmentation via Style Augmentation and Dual Normalization
42. Weakly Paired Associative Learning for Sound and Image Representations via Bimodal Associative Memory
43. ADAPT: Vision-Language Navigation with Modality-Aligned Action Prompts
44. Dual-Key Multimodal Backdoors for Visual Question Answering
45. CrossLoc: Scalable Aerial Localization Assisted by Multimodal Synthetic Data
46. Learning based Multi-modality Image and Video Compression
47. Modality-Agnostic Learning for Radar-Lidar Fusion in Vehicle Detection
48. WALT: Watch And Learn 2D amodal representation from Time-lapse imagery
49. Towards Multimodal Depth Estimation from Light Fields
50. End-to-End Referring Video Object Segmentation with Multimodal Transformers
51. FMCNet: Feature-Level Modality Compensation for Visible-Infrared Person Re-Identification
52. Show Me What and Tell Me How: Video Synthesis via Multimodal Conditioning
53. Amodal Segmentation through Out-of-Task and Out-of-Distribution Generalization with a Bayesian Model
54. Unimodal-Concentrated Loss: Fully Adaptive Label Distribution Learning for Ordinal Regression
55. Expanding Large Pre-trained Unimodal Models with Multimodal Information Injection for Image-Text Multimodal Classification
56. End-to-end Generative Pretraining for Multimodal Video Captioning
57. XMP-Font: Self-Supervised Cross-Modality Pre-training for Few-Shot Font Generation
58. Multimodal Dynamics: Dynamical Fusion for Trustworthy Multimodal Classification
59. The Auto Arborist Dataset: A Large-Scale Benchmark for Multiview Urban Forest Monitoring Under Domain Shift
60. MNSRNet: Multimodal Transformer Network for 3D Surface Super-Resolution
61. OMNIVORE: A Single Model for Many Visual Modalities
62. Motron: Multimodal Probabilistic Human Motion Forecasting
63. Amodal Panoptic Segmentation
64. Multimodal Colored Point Cloud to Image Alignment
65. Boosting 3D Object Detection by Simulating Multimodality on Point Clouds
66. ContIG: Self-supervised Multimodal Contrastive Learning for Medical Imaging with Genetics
67. Egocentric Scene Understanding via Multimodal Spatial Rectifier
68. Are Multimodal Transformers Robust to Missing Modality?
69. A Simple Multi-Modality Transfer Learning Baseline for Sign Language Translation
70. Multimodal Material Segmentation
71. Target-aware Dual Adversarial Learning and a Multi-scenario Multi-Modality Benchmark to Fuse Infrared and Visible for Object Detection
72. Balanced Multimodal Learning via On-the-fly Gradient Modulation
73. Multimodal Token Fusion for Vision Transformers
74. Learnable Irrelevant Modality Dropout for Multimodal Action Recognition on Modality-Specific Annotated Videos
75. XYLayoutLM: Towards Layout-Aware Multimodal Networks For Visually-Rich Document Understanding
76. STCrowd: A Multimodal Dataset for Pedestrian Perception in Crowded Scenes
77. 3MASSIV: Multilingual, Multimodal and Multi-Aspect dataset of Social Media Short Videos
78. M5Product: Self-harmonized Contrastive Learning for E-commercial Multi-modal Pretraining. CVPR 22. [代码](https://xiaodongsuper.github.io/M5Product_dataset/)
    - 作者创建了一个包括了音频、图像、属性、视频等信息的大规模多模态数据集。虽然从全文来看没有声明是一个MMKG，但是如果从广义的角度看，也是属于KG
79. VisualHow: Multimodal Problem Solving
80. WebQA: Multihop and Multimodal QA
81. Query and Attention Augmentation for Knowledge-Based Explainable Reasoning
82. Open-Vocabulary One-Stage Detection with Hierarchical Visual-Language Knowledge Distillation

### CVPR 21

1. Shared Cross-Modal Trajectory Prediction for Autonomous Driving
2. Robust Multimodal Vehicle Detection in Foggy Weather Using Complementary Lidar and Radar Signals
3. EvDistill: Asynchronous Events to End-task Learning via Bidirectional Reconstruction-guided Cross-modal Knowledge Distillation
4. Cross-Modal Contrastive Learning for Text-to-Image Generation
5. Looking into Your Speech: Learning Cross-modal Affinity for Audio-visual Speech Separation
6. Deep RGB-D Saliency Detection with Depth-Sensitive Attention and Automatic Multi-Modal Fusion
7. Farewell to Mutual Information: Variational Distillation for Cross-Modal Person Re-Identification
8. Multi-Modal Relational Graph for Cross-Modal Video Moment Retrievals
9. Progressive Modality Reinforcement for Human Multimodal Emotion Recognition from Unaligned Multimodal Sequences
10. ABMDRNet: Adaptive-weighted Bi-directional Modality Difference Reduction Network for RGB-T Semantic Segmentation
11. How2Sign: A Large-scale Multimodal Dataset for Continuous American Sign Language
12. Cross-Modal Center Loss for 3D Cross-Modal Retrieval
13. Defending Multimodal Fusion Models against Single-Source Adversaries
14. StEP: Style-based Encoder Pre-training for Multi-modal Image Synthesiss
15. M3P: Learning Universal Representations via Multitask Multilingual Multimodal Pre-training
16. UC2 : Universal Cross-lingual Cross-modal Vision-and-Language Pre-training
17. Discover Cross-Modality Nuances for Visible-Infrared Person Re-Identification
18. Cross-Modal Collaborative Representation Learning and a Large-Scale RGBT Benchmark for Crowd Counting
19. Learning Cross-Modal Retrieval with Noisy Labels
20. Can audio-visual integration strengthen robustness under multimodal attacks?
21. Single Pair Cross-Modality Super Resolution
22. Multimodal Contrastive Training for Visual Representation Learning
23. VX2TEXT: End-to-End Learning of Video-Based Text Generation From Multimodal Inputs
24. Multi-Modal Fusion Transformer for End-to-End Autonomous Driving
25. Multimodal Motion Prediction with Stacked Transformers
26. Cross Modal Focal Loss for RGBD Face Anti-Spoofing
27. Probabilistic Embeddings for Cross-Modal Retrieval
28. There is More than Meets the Eye: Self-Supervised Multi-Object Detection and Tracking with Sound by Distilling Multimodal Knowledge
29. Audio-Visual Instance Discrimination with Cross-Modal Agreement
30. Learning from the Master: Distilling Cross-modal Advanced Knowledge for Lip Reading
31. LaPred: Lane-Aware Prediction of Multi-Modal Future Trajectories of Dynamic Agents
32. Adaptive Cross-Modal Prototypes for Cross-Domain Visual-Language Retrieval
33. Revamping Cross-Modal Recipe Retrieval with Hierarchical Transformers and Self-supervised Learning
34. VISUALVOICE: Audio-Visual Speech Separation with Cross-Modal Consistency
35. Deep Lucas-Kanade Homography for Multimodal Image Alignment
36. Distilling Audio-Visual Knowledge by Compositional Contrastive Learning
37. Improving Weakly Supervised Visual Grounding by Contrastive Knowledge Distillation
38. Amalgamating Knowledge from Heterogeneous Graph Neural Networks

### CVPR 20

1. Multi-Modal Domain Adaptation for Fine-Grained Action Recognition
2. Creating Something from Nothing: Unsupervised Knowledge Distillation for Cross-Modal Hashing
3. Multimodal Future Localization and Emergence Prediction for Objects in Egocentric View With a Reachability Prior
4. Cross-modal Deep Face Normals with Deactivable Skip Connections
5. Monocular Real-time Hand Shape and Motion Capture using Multi-modal Data
6. Semantically Multi-modal Image Synthesis
7. Knowledge as Priors: Cross-Modal Knowledge Generalization for Datasets without Superior Knowledge
8. Cross-Modal Pattern-Propagation for RGB-T Tracking
9. Iterative Answer Prediction with Pointer-Augmented Multimodal Transformers for TextVQA
10. Modality Shifting Attention Network for Multi-modal Video Question Answering
11. A Local-to-Global Approach to Multi-modal Movie Scene Segmentation
12. Hi-CMD: Hierarchical Cross-Modality Disentanglement for Visible-Infrared Person Re-Identification
13. Speech2Action: Cross-modal Supervision for Action Recognition
14. Solving Mixed-modal Jigsaw Puzzle for Fine-Grained Sketch-Based Image Retrieval
15. Referring Image Segmentation via Cross-Modal Progressive Comprehension
16. Cross-Modal Cross-Domain Moment Alignment Network for Person Search
17. Vision-Dialog Navigation by Exploring Cross-modal Memory
18. A Real-Time Cross-modality Correlation Filtering Method for Referring Expression Comprehension
19. Multi-Modality Cross Attention Network for Image and Sentence Matchings
20. nuScenes: A multimodal dataset for autonomous driving
21. Seeing Through Fog Without Seeing Fog: Deep Multimodal Sensor Fusion in Unseen Adverse Weather
22. End-to-End Adversarial-Attention Network for Multi-Modal Clustering
23. xMUDA: Cross-Modal Unsupervised Domain Adaptation for 3D Semantic Segmentation
24. IMRAM: Iterative Matching with Recurrent Attention Memory for Cross-Modal Image-Text Retrieval∗
25. What Makes Training Multi-modal Classification Networks Hard?
26. Multi-Modal Graph Neural Network for Joint Reasoning on Vision and Scene Texts
27. Universal Weighting Metric Learning for Cross-Modal Matching
28. MMTM: Multimodal Transfer Module for CNN Fusion
29. Cross-Modality Person Re-Identification With Shared-Specific Feature Transfer
30. Unsupervised Multi-Modal Image Registration via Geometry Preserving Image-to-Image Translation
31. CoverNet: Multimodal Behavior Prediction using Trajectory Sets
32. EmotiCon: Context-Aware Multimodal Emotion Recognition using Frege’s Principle
33. Discriminative Multi-Modality Speech Recognition
34. MCEN: Bridging Cross-Modal Gap between Cooking Recipes and Dish Images with Latent Variable Model
35. Hypergraph Attention Networks for Multimodal Learning
36. Multimodal Categorization of Crisis Events in Social Media
37. Transform and Tell: Entity-Aware News Image Captioning

### KDD 22

1. Graph-Flashback Network for Next Location Recommendation
2. ERNIE-GeoL: A Geography-and-Language Pre-trained Model and its Applications in Baidu Maps
3. Graph Neural Networks for Multimodal Single-Cell Data Integration
4. External Knowledge Infusion for Tabular Pre-training Models with Dual-adapters

### KDD 21

1. Web-Scale Generic Object Detection at Microsoft Bing
2. Cross-Network Learning with Partially Aligned Graph Convolutional Networks
3. Triplet Attention: Rethinking the similarity in Transformers

### ICCV

1. VLG-Net: Video-Language Graph Matching Network for Video Grounding. ICCV 21
2. Visual-Textual Attentive Semantic Consistency for Medical Report Generation. ICCV 21
3. Public Life in Public Space (PLPS): A multi-task, multi-group video dataset for public life research. ICCV 21
4. Self-Motivated Communication Agent for Real-World Vision-Dialog Navigation. ICCV 21
5. VrR-VG: Refocusing Visually-Relevant Relationships. ICCV 19
6. Concept Generalization in Visual Representation Learning. ICCV 21
7. DocFormer: End-to-End Transformer for Document Understanding. ICCV 21
8. Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models. ICCV 15
9. Virtual Multi-Modality Self-Supervised Foreground Matting for Human-Object Interaction. ICCV 21
10. Smile, Be Happy :) Emoji Embedding for Visual Sentiment Analysis. ICCV Workshop 19
11. Explain Me the Painting: Multi-Topic Knowledgeable Art Description Generation. ICCV 19

### ECCV

1. VisualCOMET: Reasoning About the Dynamic Context of a Still Image. ECCV 20
2. Fashionpedia: Ontology, Segmentation, and an Attribute Localization Dataset. ECCV 20
3. Learning Type-Aware Embeddings for Fashion Compatibility. ECCV 18
4. A Dataset and Baselines for Visual Question Answering on Art. ECCV 20 Workshop
5. Learning to Scale Multilingual Representations for Vision-Language Tasks. ECCV 20
6. MaxViT: Multi-Axis Vision Transformer. ECCV 22

### CIKM 

1. SciClops: Detecting and Contextualizing Scientific Claims for Assisting Manual Fact-Checking. CIKM 21
2. WebKE: Knowledge Extraction from Semi-structured Web with Pre-trained Markup Language Model. CIKM 21
3. MLM: A Benchmark Dataset for Multitask Learning with Multiple Languages and Modalities. CIKM 20
4. Multi-modal Dictionary BERT for Cross-modal Video Search in Baidu Advertising. CIKM 20
5. IMAS++: An Intelligent Medical Analysis System Enhanced with Deep Graph Neural Networks. CIKM 21
6. Recipe Representation Learning with Networks. CIKM 21
7. Student Can Also be a Good Teacher: Extracting Knowledge from Vision-and-Language Model for Cross-Modal Retrieval. CIKM 21
8. Improving Chinese Character Representation with Formation Graph Attention Network. CIKM 21
9. VidLife: A Dataset for Life Event Extraction from Videos. CIKM 21
10. Learning Chinese Word Embeddings from Stroke, Structure and Pinyin of Characters. CIKM 19

### WSDM

1. Representation Interpretation with Spatial Encoding and Multimodal Analytics. WSDM 19
2. Beyond Statistical Relations: Integrating Knowledge Relations into Style Correlations for Multi-Label Music Style Classification. WSDM 20
3. Speaker and Time-aware Joint Contextual Learning for Dialogue-act Classification in Counselling Conversations. WSDM 22.
4. VISIR: Visual and Semantic Image Label Refinement. WSDM 18
5. Product Knowledge Graph Embedding for E-commerce. WSDM 20

### ICMR

1. Know Yourself and Know Others: Efficient Common Representation Learning for Few-shot Cross-modal Retrieval. ICMR 21
2. Personal Knowledge Base Construction from Multimodal Data. ICMR 21
3. Image Emotion Distribution Learning with Graph Convolutional Network. ICMR 19
4. HSGMP: Heterogeneous Scene Graph Message Passing for Cross-modal Retrieval. ICMR 21
5. HLVU : A New Challenge to Test Deep Understanding of Movies the Way Humans do. ICMR 20
6. Context-Aware Embeddings for Automatic Art Analysis. ICMR 19
7. Semantic Gated Network for Efficient News Representation. ICMR 20
8. Ten Questions in Lifelog Mining and Information Recall. ICMR 21
9. Video2Subtitle: Matching Weakly-Synchronized Sequences via Dynamic Temporal Alignment. ICMR 22
10. Improve Image Captioning by Modeling Dynamic Scene Graph Extension. ICMR 22
11. SenseMood: Depression Detection on Social Media. ICMR 20
12. Incorporating Semantic Knowledge for Visual Lifelog Activity Recognition. ICMR 20

### COLING

1. MEISD: A Multimodal Multi-Label Emotion, Intensity and Sentiment Dialogue Dataset for Emotion Recognition and Sentiment Analysis in Conversations
2. Are Visual-Linguistic Models Commonsense Knowledge Bases? COLING 22
3. Extracting a Knowledge Base of COVID-19 Events from Social Media. COLING 22
4. Read Extensively, Focus Smartly: A Cross-document Semantic Enhancement Method for Visual Documents NER. COLING 22
5. Decoupling Mixture-of-Graphs: Unseen Relational Learning for Knowledge Graph Completion by Fusing Ontology and Textual Experts. COLING 22
6. A Relation Extraction Dataset for Knowledge Extraction from Web Tables. COLING 22
7. Virtual Knowledge Graph Construction for Zero-Shot Domain-Specific Document Retrieval. COLING 22
8. KC-ISA: An Implicit Sentiment Analysis Model Combining Knowledge Enhancement and Context Features. COLING 22
9. Learning from Adjective-Noun Pairs: A Knowledge-enhanced Framework for Target-Oriented Multimodal Sentiment Classification. COLING 22
10. Extracting a Knowledge Base of COVID-19 Events from Social Media. COLING 22
11. Mind the Gap! Injecting Commonsense Knowledge for Abstractive Dialogue Summarization. CLOING 22
12. Multilingual and Multimodal Topic Modelling with Pretrained Embeddings. COLING 22
13. Knowledge-injected Prompt Tuning for Event Detection. COLING 22
14. Section-Aware Commonsense Knowledge-Grounded Dialogue Generation with Pre-trained Language Model. COLING 22
15. Enhancing Clinical BERT Embedding using a Biomedical Knowledge Base. COLING 20]
16. 

