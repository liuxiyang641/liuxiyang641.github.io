---
title: LLM-IE2
published: false
date: 2023-10-23 16:48:52
categories:
tags:
---

# 基于LLM的Information Extraction 2

基于LLM的信息抽取工作总结合集2。

<!--more-->

## Paraphrase NER

When and how to paraphrase for named entity recognition?

ACL 2023，{% post_link nlp/when-how-paraphrase-NER  [详细博客] %}。

> While paraphrasing is a promising approach for data augmentation in classification tasks, its effect on named entity recognition (NER) is not investigated systematically due to the difficulty of **span-level label preservation**. In this paper, **we utilize simple strategies to annotate entity spans in generations and compare established and novel methods of paraphrasing in NLP such as back translation, specialized encoder-decoder models such as Pegasus, and GPT-3 variants for their effectiveness in improving downstream performance for NER** across different levels of gold annotations and paraphrasing strength on 5 datasets. We thoroughly explore the influence of paraphrasers, dynamics between paraphrasing strength and gold dataset size on the NER performance with visualizations and statistical testing. We find that the choice of the paraphraser greatly impacts NER performance, with one of the **larger GPT-3 variants exceedingly capable of generating high quality paraphrases, yielding statistically significant improvements in NER performance with increasing paraphrasing strength,** while other paraphrasers show more mixed results. Additionally, inline auto annotations generated by larger GPT-3 are strictly better than heuristic based annotations. We also find diminishing benefits of paraphrasing as gold annotations increase for most datasets. Furthermore, while most paraphrasers promote entity memorization in NER, the proposed GPT-3 configuration performs most favorably among the compared paraphrasers when tested on unseen entities, with memorization reducing further with paraphrasing strength. Finally, we explore mention replacement using GPT-3, which provides additional benefits over base paraphrasing for specific datasets.

作者选择了5个不同领域的NER数据集。

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230916214229221.png"   style="zoom:40%;" />

作者先对比两个已有的Paraphrasers工具：

- 基于Back-translation（BT）：For our experiments we use pre-trained English-German and German-English models (∼738M parameters) available from Huggingface model hub via Tiedemann and Thottingal (2020) and the model architecture used is BART (Lewis et al., 2019).
- 基于PEGASUS：We use an off-the-shelf version of PEGASUS fine-tuned for paraphrasing released on Huggingface model hub. 3

然后，作者利用两个GPT-3模型：`text-ada-001` (∼350M parameters), and `text-davinci-002` (∼175B parameters)。使用的temperature为0.8。

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230916214754741-20231023165922734.png"   style="zoom:40%;" />

作者关注数据增强可能带来的一个问题Entity Memorization。即目前基于改写的数据增强方法，没有改变entity mention，生成的data中出现了entity的重复。因此作者想检查模型是不是直接记住了entity和它对应的label，而不是学会从feature推测label。

如果是记忆，那么model意味着模型走了捷径shortcut learning [*Shortcut learning in deep neural networks. Nature 2020*]，那么此时model应该无法准确处理没有见过的entity。

因此，作者又进行了在test set中，不同entity type里，没有在训练集里出现过的entity作为新的测试集unseen entity (UE) test sets。

为了缓解entity memorization问题，作者提出了一种解决方法Mention replacement（MR）。那就是不要重复entity mention，用GPT生成新的entity mention，然后去替换生成句子中的entity mention：

> In particular, for every entity mention in the gold set, we prompt GPT-3 DaVinci model to generate entity mentions that are similar to the gold entity mention, while also providing a phrase level definition of the entity type being replaced.

使用到的prompt：

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230916223735026.png"  style="zoom:50%;" />

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230916223753805.png"   style="zoom:50%;" />

作者选择了5个不同领域的NER数据集，微调distilbert-base-cased作为NER model。

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230916214229221-20230917171137150.png"   style="zoom:40%;" />

## UnleashLLM

How to Unleash the Power of Large Language Models for Few-shot Relation Extraction?

浙大zjunlp，SustaiNLP 2023，[代码](https://github.com/zjunlp/DeepKE/tree/main/example/llm)。

> Scaling language models have revolutionized widespread NLP tasks, yet little comprehensively explored few-shot relation extraction with large language models. In this paper, **we investigate principal methodologies, in-context learning and data generation, for few-shot relation extraction via GPT-3.5 through exhaustive experiments.** To enhance few-shot performance, we further propose task-related instructions and schema-constrained data generation. We observe that in-context learning can achieve performance on par with previous prompt learning approaches, and data generation with the large language model can boost previous solutions to obtain new state-of-the-art few-shot results on four widely-studied relation extraction datasets. We hope our work can inspire future research for the capabilities of large language models in few-shot relation extraction.

作者探究了如何利用LLM模型去执行few shot RE任务，主要是两个不同的角度：

- 使用in-context learning让LLM直接进行RE
- 利用LLM生成训练数据，提升之前基于SLM的few-shot方法性能

下面是方法图：

![image-20230518225716044](https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230518225716044-20231023165922925.png)

作者实现是基于`text-davinci-003`，有以下细节：

- prompt时加入实体类型和任务描述一般会提升LLM的RE效果。受限于输入长度限制，作者主要是进行one-shot的任务。
- ICL的demonstrations是针对每种relation type，随机的从训练集中找样例。
- 进行数据生成时，作者是以few-shot的样例作为demos输入来获得更多的数据，然后与原来的训练数据一起训练基于SLM的之前模型。

![image-20230518230345906](https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230518230345906-20231023165923047.png)

## Is GPT-3 good data annotator 

Is GPT-3 a Good Data Annotator?. 南洋理工与阿里达摩，ACL 2023，[代码](https://github.com/DAMO-NLP-SG/LLM-Data-Annotator)。

{% post_link llm/is-gpt3-good-data-annotator  [详细博客] %}。

> Data annotation is the process of labeling data that could be used to train machine learning models. Having high-quality annotation is crucial, as it allows the model to learn the relationship between the input data and the desired output. GPT-3, a large-scale language model developed by OpenAI, has demonstrated impressive zero- and few-shot performance on a wide range of NLP tasks. It is therefore natural to wonder whether it can be used to effectively annotate data for NLP tasks. **In this paper, we evaluate the performance of GPT-3 as a data annotator by comparing it with traditional data annotation methods and analyzing its output on a range of tasks.** Through this analysis, we aim to provide insight into the potential of GPT-3 as a general-purpose data annotator in NLP.

为什么要讨论数据标注问题？因为从大的方面讲，AI技术应该面向社会各界提供服务（论文中称为The democratization of artificial intelligence）。但是一个AI model往往需要大量的标注数据。

> The democratization of artificial intelligence (AI) (Garvey, 2018; Rubeis et al., 2022) aims to provide access to AI technologies to all members of society, including individuals, small- and medium-sized enterprises (SMEs), academic research labs, and nonprofit organizations.

标注数据的获得需要很高成本：

- labor costs associated with the labeling process
- the time and resources required to hire, train and manage annotators. 
- Additionally, there may be costs associated with the annotation tools and infrastructure needed to support the annotation process.

对于个人和小公司来说这种成本往往是不可接受的。

另一方面，GPT-3等大模型有很多knowledge，可以执行广泛的NLP任务；但是在production环境中，使用BERT-base等small的model可能是更加合理的（个人认为这种small model在极端情况下，需要高响应的场景中也不实用）。

所以，论文作者就关注利用GPT-3生成/标注训练数据，去更好的训练small model以降低标注数据获取成本。

作者的标注数据获取方法有三种：

- prompt-guided unlabeled data annotation (PGDA)：**tagging-based** approach，让LLM直接对in-domain unlabelled data进行标注
- prompt-guided training data generation (PGDG)：**generation-based** approach，让LLM生成带有label的数据
- dictionary-assisted training data generation (DADG)：**generation-based** approach，利用external knowledge source去辅助生成带有label的数据。先在Wikidata中查询相关的样例，然后让GPT模仿生成数据。这样做的好处是对于一些LLM的预训练数据没有包括/占比较少，学习效果不好的domain，更能够生成可信的结果。

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230905163214676-20231018161200247.png"   style="zoom:40%;" />

三种思路对于不同的任务有不同的prompt，但是都采用了in-context learning的形式。

一些实验设置：

- GPT-3使用`text-davinci-003`，使用其生成的data去训练small model
- small model是`BERT-base`
- DADG利用到的外部知识源是Wikidata

下面是记录的利用GPT生成NER数据和RE数据的PGDG方法的prompt，其它prompt参见论文：

RE任务：第一步让GPT生成特定relation的head/tail entity；第二步让GPT根据head/tail entity去创造包含这两个实体的sentence；从论文描述中看，应该是随机找的demonstrations。

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230905164544743-20231018161200293.png"   style="zoom:30%;" />

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230905164602059-20231018161200344.png"   style="zoom:30%;" />

NER任务：第一步让GPT生成不同entity type下的可能entity；第二步让GPT利用生成的entity去生成对应的sentence：

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230905165740644-20231018161200393.png"   style="zoom:30%;" />

## Data Generation for clinical NER and RE

Does Synthetic Data Generation of LLMs Help Clinical Text Mining?

arXiv 2023-04

> Recent advancements in large language models (LLMs) have led to the development of highly potent models like OpenAI’s ChatGPT. These models have exhibited exceptional performance in a variety of tasks, such as question answering, essay composition, and code generation. However, their effectiveness in the healthcare sector remains uncertain. **In this study, we seek to investigate the potential of LLMs to aid in clinical text mining by examining their ability to extract structured information from unstructured healthcare texts, with a focus on biological named entity recognition and relation extraction.** However, our preliminary results indicate that employing LLMs directly for these tasks resulted in poor performance and raised privacy concerns associated with uploading patients’ information to the LLM API. To overcome these limitations, we propose a new training paradigm that involves generating a vast quantity of high-quality synthetic data with labels utilizing LLMs and fine-tuning a local model for the downstream task. Our method has resulted in significant improvements in the performance of downstream tasks, improving the F1-score from 23.37% to 63.99% for the named entity recognition task and from 75.86% to 83.59% for the relation extraction task. Furthermore, **generating data using LLMs can significantly reduce the time and effort required for data collection and labeling, as well as mitigate data privacy concerns.** In summary, the proposed framework presents a promising solution to enhance the applicability of LLM models to clinical text mining.

作者先是尝试了ChatGPT在clinical NER和RE任务上，zero-shot ICL设置下和目前SOTA的差距：

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230925170021648-20230925171040987.png"   style="zoom:50%;" />

在clinical NER和RE上，作者发现效果并不好，这当然很正常，ChatGPT并不是专门为clinical domain训练的，而执行这一domain肯定需要大量的domain knowledge；同时直接调用LLM的API存在隐私泄露问题。因此作者尝试利用LLM去生成一系列的训练数据，而不是直接进行任务。用LLM生成数据去训练一个小模型，小模型可以直接本地部署，避免了隐私泄露问题。

作者用prompt engineering创造合适的prompt：

- 询问GPT “Provide five concise prompts or templates that can be used to generate data samples of [Task Descriptions].”
- 用每个prompt生成10个句子，然后人工检查下句子质量，选择效果最好的prompt
- 然后让GPT基于前面选择的最好的prompt，继续提供新的prompt。这一过程持续3遍

作者找到的最合适的prompt（没有demonstrations）：

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230925170508864-20230925171041027.png"   style="zoom:50%;" />

NER任务是根据entity直接生成句子；RE任务是输入头尾实体，判断某个relation是否存在

可视化结果显示，不控制的情况下，GPT自己发挥生成的句子和原来的sentence肯定有分布上的差别：

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230925170635577-20231023170047082.png"   style="zoom:50%;" />

## SynthIE

Exploiting Asymmetry for Synthetic Training Data Generation: SynthIE and the Case of Information Extraction

arXiv 2023.03，[代码](https://github.com/epfl-dlab/SynthIE)。

使用LLM模型生成更多的IE任务训练数据，从而进一步提升模型性能。

> Large language models (LLMs) show great potential for synthetic data generation. This work shows that useful data can be synthetically generated even for tasks that cannot be solved directly by the LLM: we show that, for problems with structured outputs, it is possible to prompt an LLM to perform the task in the opposite direction, to generate plausible text for the target structure. Leveraging the asymmetry in task difficulty makes it possible to produce large-scale, high-quality data for complex tasks. We demonstrate the effectiveness of this approach on closed information extraction, where collecting groundtruth data is challenging, and no satisfactory dataset exists to date. We synthetically generate a dataset of 1.8M data points, demonstrate its superior quality compared to existing datasets in a human evaluation and use it to finetune small models (220M and 770M parameters). The models we introduce, SynthIE, outperform existing baselines of comparable size with a substantial gap of 57 and 79 absolute points in micro and macro F1, respectively. Code, data, and models are available at https://github.com/epfl-dlab/SynthIE.

motivation:

对于LLM模型来说，存在一些比较hard的task，直接利用LLM模型可能无法很好的直接解决，很多这样的NLP任务是要求输入自然语言的文本，输出格式化结果。作者认为，对于LLM模型来说，输入自然语言，获得结构化输出比较难，但是反过来输入结构化输入，输出对应的自然语言描述相对简单。这就是本文讨论的LLM的不对称性：

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230517173725283.png"   style="zoom:30%;" />

作者认为IE任务对于LLM来说就是这样的hard task，IE任务数据构造需要大量的人工，另外构建的质量也不一定很高。比如根据评估，IE任务下最大的数据集REBEL文本中70%的信息没有被抽取到，45%的三元组实际上没有在文本中出现。因此，作者就尝试利用LLM模型生成训练数据，而不是直接执行训练任务，下面是流程图：

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230517173826523.png" style="zoom:40%;" />

核心是两步，第一步是采样用来生成文本的三元组集合，在这一步作者核心考虑是怎么样保证三元组是连续的，也就是怎么样让三元组集合是常常在文本中一起出现的。作者通过在Wikidata knowledge graph上进行随机游走采样保证三元组之间存在关联。

其次还要考虑均匀度和覆盖度，让很少出现的实体或关系也能够被采样到。作者在随机游走K轮后，给从未被采样的entity更高的概率，已经被采样过的entity更低的概率。

第二步是根据三元组集合生成对应的文本。下面是一个示例：

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230518173205557.png"   style="zoom:50%;" />

作者使用的是text-davinci-003和code-davinci-002，生成了两个对应的数据集SynthIE-Text和SynthIE-Code。一个示例如下：

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230518173251020.png"   style="zoom:20%;" />

为了评估生成数据的结果，作者除了人工评估外，还使用人工生成的训练数据加入到原来的数据集中提升之前方法的效果：

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230518171257333.png"  style="zoom:30%;" />

不过个人感觉作者的实现提升效果不明显，而且随机偏差太大。

## DeepTagger

DeepTagger: Knowledge Enhanced Named Entity Recognition for Web-Based Ads Queries

arXiv 2023-06

> Named entity recognition (NER) is a crucial task for online advertisement. State-of-the-art solutions leverage pre-trained language models for this task. However, three major challenges remain unresolved: **web queries differ from natural language, on which pre-trained models are trained; web queries are short and lack contextual information; and labeled data for NER is scarce.** We propose DeepTagger, a knowledge-enhanced NER model for web-based ads queries. The proposed knowledge enhancement framework leverages both model-free and model-based approaches. For model-free enhancement, we collect unlabeled web queries to augment domain knowledge; and we collect web search results to enrich the information of ads queries. **We further leverage effective prompting methods to automatically generate labels using large language models such as ChatGPT.** Additionally, we adopt a model-based knowledge enhancement method based on adversarial data augmentation. We employ a three-stage training framework to train DeepTagger models. Empirical results in various NER tasks demonstrate the effectiveness of the proposed framework.

这篇文章集中在对Web queries进行NER场景上。这种任务通常发生在广告领域，对于用户输入的查询，需要识别实体，然后打广告。

Web queries与一般的完整natural language的区别：

- First, there is a domain shift between web queries and natural language. 大部分的web查询文本不是完整的句子，没有动词/形容词；同时会包含产品、品牌等在一般领域内不常见的实体
- Second, web queries are short and lack information. web查询文本很短，可能只有4-5个单词，没有很多的semantic components
- The third problem is label scarcity. 没有足够的针对web查询文本的标签数据

作者先是利用web search的结果的title来增加query的语义：

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230911223831963.png"   style="zoom:30%;" />

然后作者想办法获取不够准确的weakly-labeled data：

- 人类标注的也有可能有很多错误，作者把人类标注的data也作为一类weakly-labeled data

- 利用ChatGPT这类大模型进行初步NER标注，作者使用fixed CoT prompting方法；基于web query的返回结果的title来人工构造CoT：

  <img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230911224040473.png"   style="zoom:30%;" />

作者的strong-labeled data是指一小部分的人类专家标注的数据；为了增强利用这部分数据，作者使用了对抗数据增强的方法。原理是对某个data的微小的改动，不应该改变它对应的预测结果，也就是说对于数据点的邻居，模型应该给出一样的预测结果。这样增大最靠近决策边界的data point和决策边界的距离，让决策边界更加平滑，提升模型的鲁棒性[*Learning from rules generalizing labeled exemplars. ICLR 2020*]：

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230911224452680.png"   style="zoom:30%;" />

对于每个labeled data，生成它最难以被准确预测的邻居点：

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230911224159401.png"   style="zoom:30%;" />

最后，作者的NER model的训练流程：

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230911225016455.png"   style="zoom:50%;" />

UnleashLLM

How to Unleash the Power of Large Language Models for Few-shot Relation Extraction?

浙大zjunlp，SustaiNLP 2023，[代码](https://github.com/zjunlp/DeepKE/tree/main/example/llm)。

> Scaling language models have revolutionized widespread NLP tasks, yet little comprehensively explored few-shot relation extraction with large language models. In this paper, **we investigate principal methodologies, in-context learning and data generation, for few-shot relation extraction via GPT-3.5 through exhaustive experiments.** To enhance few-shot performance, we further propose task-related instructions and schema-constrained data generation. We observe that in-context learning can achieve performance on par with previous prompt learning approaches, and data generation with the large language model can boost previous solutions to obtain new state-of-the-art few-shot results on four widely-studied relation extraction datasets. We hope our work can inspire future research for the capabilities of large language models in few-shot relation extraction.

作者探究了如何利用LLM模型去执行few shot RE任务，主要是两个不同的角度：

- 使用in-context learning让LLM直接进行RE
- 利用LLM生成训练数据，提升之前基于SLM的few-shot方法性能

下面是方法图：

![image-20230518225716044](https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230518225716044.png)

作者实现是基于`text-davinci-003`，有以下细节：

- prompt时加入实体类型和任务描述一般会提升LLM的RE效果。受限于输入长度限制，作者主要是进行one-shot的任务。
- ICL的demonstrations是针对每种relation type，随机的从训练集中找样例。
- 进行数据生成时，作者是以few-shot的样例作为demos输入来获得更多的数据，然后与原来的训练数据一起训练基于SLM的之前模型。

![image-20230518230345906](https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230518230345906.png)

## Paraphrase NER

When and how to paraphrase for named entity recognition?

ACL 2023，{% post_link nlp/when-how-paraphrase-NER  [详细博客] %}。

> While paraphrasing is a promising approach for data augmentation in classification tasks, its effect on named entity recognition (NER) is not investigated systematically due to the difficulty of **span-level label preservation**. In this paper, **we utilize simple strategies to annotate entity spans in generations and compare established and novel methods of paraphrasing in NLP such as back translation, specialized encoder-decoder models such as Pegasus, and GPT-3 variants for their effectiveness in improving downstream performance for NER** across different levels of gold annotations and paraphrasing strength on 5 datasets. We thoroughly explore the influence of paraphrasers, dynamics between paraphrasing strength and gold dataset size on the NER performance with visualizations and statistical testing. We find that the choice of the paraphraser greatly impacts NER performance, with one of the **larger GPT-3 variants exceedingly capable of generating high quality paraphrases, yielding statistically significant improvements in NER performance with increasing paraphrasing strength,** while other paraphrasers show more mixed results. Additionally, inline auto annotations generated by larger GPT-3 are strictly better than heuristic based annotations. We also find diminishing benefits of paraphrasing as gold annotations increase for most datasets. Furthermore, while most paraphrasers promote entity memorization in NER, the proposed GPT-3 configuration performs most favorably among the compared paraphrasers when tested on unseen entities, with memorization reducing further with paraphrasing strength. Finally, we explore mention replacement using GPT-3, which provides additional benefits over base paraphrasing for specific datasets.

作者选择了5个不同领域的NER数据集。

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230916214229221-20230917171504576.png"   style="zoom:40%;" />

作者先对比两个已有的Paraphrasers工具：

- 基于Back-translation（BT）：For our experiments we use pre-trained English-German and German-English models (∼738M parameters) available from Huggingface model hub via Tiedemann and Thottingal (2020) and the model architecture used is BART (Lewis et al., 2019).
- 基于PEGASUS：We use an off-the-shelf version of PEGASUS fine-tuned for paraphrasing released on Huggingface model hub. 

然后，作者利用两个GPT-3模型：`text-ada-001` (∼350M parameters), and `text-davinci-002` (∼175B parameters)。使用的temperature为0.8，进行one-shot ICL。

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230916214754741.png"   style="zoom:40%;" />

作者关注数据增强可能带来的一个问题Entity Memorization。即目前基于改写的数据增强方法，没有改变entity mention，生成的data中出现了entity的重复。因此作者想检查模型是不是直接记住了entity和它对应的label，而不是学会从feature推测label。

如果是记忆，那么model意味着模型走了捷径shortcut learning [*Shortcut learning in deep neural networks. Nature 2020*]，那么此时model应该无法准确处理没有见过的entity。

因此，作者又进行了在test set中，不同entity type里，没有在训练集里出现过的entity作为新的测试集unseen entity (UE) test sets。

为了缓解entity memorization问题，作者提出了一种解决方法Mention replacement（MR）。那就是不要重复entity mention，用GPT生成新的entity mention，然后去替换生成句子中的entity mention：

> In particular, for every entity mention in the gold set, we prompt GPT-3 DaVinci model to generate entity mentions that are similar to the gold entity mention, while also providing a phrase level definition of the entity type being replaced.

使用到的prompt：

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230916223735026-20230917171504656.png"  style="zoom:40%;" />

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230916223753805-20230917171504707.png"   style="zoom:40%;" />

作者选择了5个不同领域的NER数据集，微调`distilbert-base-cased`作为NER model。

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230916214229221-20230917171137150-20230917171504792.png"   style="zoom:40%;" />

## $\mbox{S}^2$ynRE

S2ynRE: Two-stage Self-training with Synthetic data for Low-resource Relation Extraction

中科大，ACL 2023，[代码](https://github.com/BenfengXu/S2ynRE)。

> Current relation extraction methods suffer from the inadequacy of large-scale annotated data. While distant supervision alleviates the problem of data quantities, there still exists domain disparity in data qualities due to its reliance on domain-restrained knowledge bases. In this work, **we propose S2ynRE, a framework of two-stage Self-training with Synthetic data for Relation Extraction.** We first leverage the capability of large language models to adapt to the target domain and automatically synthesize large quantities of coherent, realistic training data. We then propose an accompanied two-stage self-training algorithm that iteratively and alternately learns from synthetic and golden data together. We conduct comprehensive experiments and detailed ablations on popular relation extraction datasets to demonstrate the effectiveness of the proposed framework. Code is available at https://github.com/BenfengXu/S2ynRE.

对于RE任务来说，高质量有标注的data获取很难，之前一种解决这个问题的思路是远监督distant supervision，尽管远监督获得了效果的提升，但是远监督的数据不能够保证和下游任务的schema、context分布特征等是相符的：

> Although this line of methods have seen certain improvements, they still inevitably raise the concern that the distantly annotated data can vary considerably from downstream tasks both in target schema and in context distributions, thus may not be able to offer optimal transferability.

换句话说，要获得理想的领域特征一致的远监督数据本身也可能是比较难的。

因此，作者顺着最近的一些利用LLM生成text data的工作的思路，考虑使用LM来生成数据。作者的贡献主要有两点：

- 利用GPT-3.5和finetuned GPT-2 Large去适应target domain distribution，然后生成无label的RE data
- 提出了a two-stage self-training训练策略，更好的利用生成的无标注数据和原有标注数据

作者的RE任务是给定头尾实体，预测relation。

利用GPT-2 Large生成数据，首先按照language modeling的loss在训练集上微调；然后在推理阶段，输入`<bos>`开始进行采样生成new data。

利用GPT-3生成数据，采用5-shot ICL，随机找demonstrations的策略：

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230926161739020.png"  style="zoom:50%;" />

注意这里prompt对于结果的可控，只是通过一些指令性的表述，如`similar topic, domain and the same sub-obj format`。

然后是如何利用生成的无标注data，一般的策略是self-training，即给无标注data伪标注然后和原有data混合，训练小模型，训练好的小模型再重新标注无标注data。

作者认为这种直接将生成的数据加入到原有的数据方法前提是，要求生成的数据需要和原来的数据有一样的分布。

相反，作者将无标注数据和有标注数据分开，先使用gold data训练多个teacher model，然后标注生成的data，注意是soft label；然后用一个新初始化的student model在带有soft label的生成数据上训练，更新参数；之后继续在gold data上训练，更新后的model重新标注生成的data；这样迭代式的训练：

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230926162246951.png"   style="zoom:50%;" />

对于实验结果具体可以参考原paper，这里提供几个值得记录的结果：

作者使用BERT+Linear作为RE model。

直接用GPT不一定能够超过finetuned LM来生成data，下面的结果没有找到是具体哪个dataset上的测试结果：

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230926162817492.png"  style="zoom:50%;" />

作者使用type-token ratio ([*Evaluating story generation systems using automated linguistic analyses. 2017*]; *Data augmentation using pre-trained transformer models. 2020*)来评估diversity。

## Synthetic data: subjectivity

Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations

Purdue University, 作者评论接收至EMNLP 2023。{% post_link llm/synthetic-data-llm-sub [详细博客] %}。

> The collection and curation of high-quality training data is crucial for developing text classification models with superior performance, but it is often associated with significant costs and time investment. Researchers have recently explored using large language models (LLMs) to generate synthetic datasets as an alternative approach. However, **the effectiveness of the LLM-generated synthetic data in supporting model training is inconsistent across different classification tasks.** To better understand factors that moderate the effectiveness of the LLM-generated synthetic data, in this study, we look into how the performance of models trained on these synthetic data may vary with the subjectivity of classification. Our results indicate that subjectivity, at both the task level and instance level, is negatively associated with the performance of the model trained on synthetic data. We conclude by discussing the implications of our work on the potential and limitations of leveraging LLM for synthetic data generation.

**Issue**: 目前在不同的task里，对于使用LLM生成的data是否能够和真实人工标注的data相比，没有定论。

**Solution**: 作者认为出现这种现象的原因之一和具体text classification任务的主观程度subjectivity有关，实验发现主观性越强的分类任务，LLM生成数据的效果也会越差。

作者采用了zero-shot和few-shot ICL两种设置。

对于zero-shot ICL prompt：

- “context prompt” relevant to the targeted domain of interest is used to set the context. 与具体task context相关的prompt
- the “data generation prompt”, is provided to the LLM, instructing the model to generate texts with a specific style, label (with respect to the classification task of interest), and word limit. 提供具体的label、生成字数限制等要求的prompt
- a “diversity prompt” to the LLM—“Can you provide something more diverse compared to the previously generated data?”—aiming to increase the diversity of the synthetic data generated. 生成具体的几个text data后，提示LLM生成更多不同的text data

对于few-shot ICL prompt：

- “context prompt”与前面zero-shot ICL一样
- 随机采样的几个demonstrations，其中说明了对应的label
- 还强制限制了不允许仅仅是修改原来的句子，而是期望生成更多的具体data，比如`You should imitate the example I have provided, but you cannot simply modify or rewrite the example I have given`

下面是不同task用到的具体prompt：

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20231021233456402-20231023165740875.png"  style="zoom:50%;" />

分别独立的在真实数据、生成数据上进行训练的实验结果（对于关系分类任务，只讨论了FewRel 2.0数据集中‘country’, ‘league’, ‘screenwriter’, and ‘tributary’的4种relation。每种relation生成3000条数据）：

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20231021230350077-20231021233707905-20231023165740953.png"  style="zoom:50%;" />

观察：

- 直接使用真实数据训练的效果最好
- few-shot ICL生成数据效果比zero-shot ICL效果好
- LLM在生成带有更多人类主观性的数据上，效果更差

为什么任务的主观程度会增大LLM生成数据的效果？作者提供了两个解释：

1. highly subjective tasks often require a deep understanding of nuanced human emotions and contextual subtleties, as well as the ability to discern and accurately interpret different perspectives. 越主观，越要求对于人类情感等有非常微妙的理解
2. it may be challenging for LLMs to generate synthetic data to recover such potentially biased “majority view,” especially if the LLMs are trained to maintain neutrality. 大多数的任务实例是利用众包标注的，也就是说在数据集里的gold label可能只反映了多个人的主要投票意见。对于LLM来说，要生成反映这种majority view的句子可能比较难。

使用一小部分真实数据进行训练，然后拿这一小部分真实数据作为demonstrations进行数据增强的实验结果：

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20231021230653949-20231021233708003-20231023165741015.png" style="zoom:50%;" />

观察：

- 在关系抽取任务上，作者的这种比较简单的生成数据的方法，没有明显提升效果

## STAR

STAR: Improving Low-Resource Information Extraction by Structure-to-Text Data Generation with Large Language Models. arXiv 2023-09. University of California

> Information extraction tasks such as event extraction require an in-depth understanding of the output structure and subtask dependencies. They heavily rely on task-specific training data in the form of (passage, target structure) pairs to obtain reasonable performance. However, obtaining such data through human annotation is costly, leading to a pressing need for low-resource information extraction approaches that require minimal human labeling for real-world applications. Fine-tuning supervised models with synthesized training data would be a generalizable method, but **the existing data generation methods either still rely on large-scale ground-truth data or cannot be applied to complicated IE tasks due to their poor performance. **To address these challenges, we propose STAR, a data generation method that leverages Large Language Models (LLMs) to synthesize data instances given limited seed demonstrations, thereby boosting low-resource information extraction performance. **Our approach involves generating target structures (Y) followed by generating passages (X), all accomplished with the aid of LLMs.** We design fine-grained step-by-step instructions to obtain the initial data instances. We further reduce errors and improve data quality through self-reflection error identification and self-refinement with iterative revision. Our experiments show that the data generated by S TAR significantly improves the performance of low-resource event extraction and relation extraction tasks, even surpassing the effectiveness of human-curated data. Human assessment of the data quality shows STAR-generated data exhibits higher passage quality and better align with the task definitions compared with the human-curated data.

**Issue**: 现有的解决低资源IE任务两种思路：

1. 迁移学习的方法
2. 将IE任务转化为另一种data-rich task的形式

但是这两种思路都要求有source task的训练数据，并且要求source task和IE task之间是相容的。

另外的一种思路是数据增强，包括：

- $X_{gold}\rightarrow Y$: annotate unlabeled examples with existing models as weak annotators
- $X_{gold}\rightarrow X^\prime$: produce analogous input
- $Y_{gold}\rightarrow X$: generate input assuming the labels are available

但是现有的数据增强IE方法只适用于简单的基于分类的IE任务，如RE。不适用于EE这样包含了不同子任务，并且子任务之间存在依赖的复杂IE任务。

**Soluation**: 作者提出利用LLM来生成IE任务训练数据的流程，STAR（Structure-to-Text DatA GeneRation），执行$Y\rightarrow X$生成，也就是从非已有的真实label到数据text的生成。

作者的方法流程：

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20231023160605019.png" style="zoom:50%;" />

下面描述作者3个主要流程，以事件抽取EE为例：

Target Structure Generation：生成target structure $Y$

1. Trigger candidate generation. 生成触发词，prompt构成：

   - a definition of the selected event type
   - a few $k$ passages that contain event triggers of the selected event type as demonstrations

2. Argument candidate generation. 生成论元，prompt构成：

   - definition of the argument role under this specific event type
   - the entity type we are looking for (e.g. a vehicle)
   - 没有对应的demonstrations

3. Creating target structure and distribution control. 随机采样触发词和对应的论元，生成target structure，特别注意控制比例：

   - 每个event type生成相同数量例子，并且生成的触发词数量到100个，是现有ACE05数据集里不同event type对应的触发词数量的1.4x-50x倍大。

   - argument hallucination ratio: 按照不同概率，随机的移除一部分argument values，保证最后生成数据里，某个event type既有many arguments的例子，也有只有few arguments的例子

   - event density: 从$0$-$5$，让target structure包含不同event数量。

Instruction-Guided Passage Generation：根据target structure $Y$，生成initial passage $X_0$。具体的prompt包括下面几部分：

- Task-level instruction. 任务相关semantic，具体包括
  - a definition of “event”, “trigger”, “participant arguments” and “attribute arguments”;
  - an overall task requirement that the goal is to generate a sentence containing the event trigger words and arguments; 
  - hallucination clarification that instructs the model not to generate arguments of certain roles if we explicitly provide that “the argument is None”;
  - multiple event clarification that information from multiple events should be contained in a single passage.
- Event type-level instruction. label相关semantic，具体包括
  - the name and definition of the event type
  - the name and definition of each possible argument roles
- Instance-level verbalizer. demonstrations的format，具体包括
  - the number of events in the passage;
  - the content of the event target structure; 
  - the passage $X$ with tags wrapping triggers and arguments to explicit hint the LLM about the roles and positions of the keywords，如`<Plaintiff>He</Plaintiff> threatened to <Trigger>sue</Trigger> the company if the company did not refund his money.`。

Self-refinement by Self-reflection：improves the initial generation results through iterative updates (Madaan et al. 2023; Wang et al. 2022; Liu, Sferrazza, and Abbeel 2023)

作者针对EE任务，设计了一系列评估dimension：

- whether the trigger/argument mention is a subsequence of the passage; 
- whether a trigger is used to initiate an occurrence; 
- whether an argument is used as an event participant or attribute of the specific event; 
- whether the argument is serving the required argument role; 
- whether the passage contains information that could be served as an argument that should not appear; 
- whether POS tags of the argument mentions in the passage context match the provided ones.

对于每个评估角度，都query LLM去检查生成的passage $X$，如`Is ‘Syria’ a DESTINATION argument describing the event triggered by ‘flee’?`。LLM的回答，利用一个外部的NLI model MultiNLI [*BroadCoverage Challenge Corpus for Sentence Understanding through Inference. 2018*]去判断LLM的回复是否包含了含义`Yes, it is.`。

如果发现了问题，就采用对应的intervention templates去修正Passage $X$，如`The passage contains a hallucinated argument CRIME incorrectly, remove CRIME information for event triggered by ‘jailed’`。

上面的过程是针对EE任务，下面是作者在RE任务上的尝试：

1. target structure generation: we generate entity candidates using seed data instances’ entities as incontext examples. We then randomly pair entity candidates and assign a relation between the two entities. 对于每种relation type，给定包含头尾实体和例子，让LLM生成更多的相似实体
2. initial passage generation: we use relation type definition instead. 给定relation type的定义
3. self-refinement: 
   - whether the given entities are contained in the generated passage. 生成的句子是否包括了目标entity
   - whether there is a relation between them 头尾实体是否存在某种relation
   - whether they hold the certain relation provided in $Y$. 头尾实体是否描述了具体的target relation type

作者的实验基于`gpt-3.5-turbo-0301`和`gpt-4-0314`。$k$表示每种label使用的demonstrations数量，$N$表示生成的数据，最终在不同label包括$k+N$数据量的数据上进行训练。

在事件抽取ACE05数据集上的表现：

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20231023164040881.png"  style="zoom:50%;" />

观察：

- 每个label生成50条数据用于训练，LLM生成的数据训练小模型的效果，超越了人类标注的real data。（但如果是使用全部的训练数据呢？）
- 使用上下文学习，模拟demonstrations是有意义的。如果只是提供task/label instruction，则很难生成合适的数据

在RE任务TACRED数据集上的表现：

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20231023164319493.png" style="zoom:40%;" />
