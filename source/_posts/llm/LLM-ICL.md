---
title: LLM-ICL
published: true
date: 2023-06-01 20:07:01
categories:
- Paper
- LLM
- ICL
tags:
- LLM
- ICL
---

# LLM的in-context learning相关论文

<!--more-->

## APE

Large Language Models are Human-Level Prompt Engineers

ICLR 2023，University of Toronto，[代码](https://github.com/keirp/automatic_prompt_engineer)。

> By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, **we propose Automatic Prompt Engineer  (APE) for automatic instruction generation and selection.** In our method, we treat the instruction as the “program,” optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. **Extensive experiments show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 24/24 Instruction Induction tasks and 17/21 curated BIG-Bench tasks.** We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that APE-engineered prompts are able to improve few-shot learning performance (by simply prepending them to standard in-context learning prompts), find better zero-shot chain-of-thought prompts, as well as steer models toward truthfulness and/or informativeness.

作者提出一种从几个样例中自动创建task instruction的training free的方法APE（Automatic Prompt Engineer）。

由于LLM对于prompt的理解和人类不一样，因此简单的语言prompt不一定能够得到理想的结果。因此往往需要大量的人工去设计prompt，去搜索寻找最好的prompt实践。作者将自动寻找最能够激发LLM执行具体task能力的prompt的过程看做是一个black-box optimization problem，称之为natural language program synthesis。

作者提出的APE方案如下：

![image-20230601201707682](https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230601201707682.png)

简单来说就是3步，

1. 让LLM从几个样例中产生一系列的task instructions
2. 让LLM使用不同的task instructions，评估不同task instructions的效果。先选一个子集评估所有的instruction，然后对于score比较高的instructions，再选新的子集评估筛选，直至选出一定数量的候选instructions
3. [可选] 为了防止在某些任务下，LLM一开始没有找到合适的instructions，重新采样。让LLM在当前score最高的instructions附近采样

第1步采样初始的task instructions，作者使用了三种方式：

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230601202801649.png"   style="zoom:40%;" />

第一种forward mode prompt是最直接的，让LLM续写query，但是instructions只会出现在句子最后。作者提出第2种reverse mode prompt让LLM填空，让instructions可以出现在句子的任意地方。最后是对于一些已经发现了效果比较好的模板，可以加进来（具体在实验中可能作者只是在TruthfulQA数据集下使用了？）。

第2步怎么样评价不同instructions的效果，当然最直接的是根据任务指标来验证。在一般情况下，作者建议可以直接使用0-1 loss来给不同生成的instructions打分。还可以使用log probability。同时，让每个instructions都在所有的训练样例下进行评估是不实际的，因此作者就提出使用训练子集来筛选合适的instructions。如果一个子集筛选过后的instructions还是太多，就继续采样新的不重叠的训练子集，继续筛选instructions。

第3步是针对一些情况下，LLM生成的所有instructions没有找到合适的instructions，让LLM继续在当前效果最好的instructions周围进行寻找。we consider exploring the search space locally around the current best candidates. 这一步是可选的，因为作者发现不断增加更多的迭代步骤没有带来更多的提升，同时这种提升也不是在所有task中都会出现。（在实验中APE默认无迭代，APE-IT是加入了迭代）

zero-shot instruction induction实验结果来看，APE方法在很多任务上已经达到了人工prompting的效果：

![image-20230601204311989](https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230601204311989.png)

生成更多的候选instructions带来了更好的效果，同时iterative search对于那些一开始没有办法找到合适的instructions的task有提升作用，而在另外task上仅仅是生成一次就效果比较好了：

![image-20230601204823591](https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230601204823591.png)

## SG-ICL

Self-Generated In-Context Learning: Leveraging Auto-regressive Language Models as a Demonstration Generator

NAACL 2022 Workshop，首尔大学

> Large-scale pre-trained language models (PLMs) are well-known for being capable of solving a task simply by conditioning a few input-label pairs dubbed demonstrations on a prompt without being explicitly tuned for the desired downstream task. Such a process (i.e., in-context learning), however, naturally leads to high reliance on the demonstrations which are usually selected from external datasets. In this paper, **we propose self-generated in-context learning (SG-ICL), which generates demonstrations for in-context learning from PLM itself to minimize the reliance on the external demonstration.** We conduct experiments on four different text classiﬁcation tasks and show SG-ICL signiﬁcantly outperforms zero-shot learning and is generally worth approximately 0.6 gold training samples. Moreover, our generated demonstrations show more consistent performance with low variance compared to randomly selected demonstrations from the training dataset.

利用LLM自动生成demonstrations来增强zero-shot ICL的能力（作者声称是首个这么做的工作）。

![image-20230601223224356](https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230601223224356.png)

需要注意的一点是，在使用LLM生成demonstrations的时候，是输入了对应的test instance和期望生成的class一起生成的。

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230601223450763.png"   style="zoom:40%;" />

作者在实验部分使用LLM位每个test instance生成8个构造的demonstrations，发现效果和在进行5-shot的LLM ICL效果差不多，因此作者认为1个LLM生成的demonstration价值相当于0.6个gold training sample：

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230601223612766.png"  style="zoom:40%;" />

## Self-Instruct

ACL 2023，华盛顿大学，[代码](https://github.com/ yizhongw/self-instruct)。

{% post_link llm/Self-Instruct [个人详细博客] %}

> Large “instruction-tuned” language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute improvement over the original model on SUPER-NATURALINSTRUCTIONS, on par with the performance of InstructGPT 001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with SELF-INSTRUCT outperforms using existing public instruction datasets by a large margin, leaving only a 5% absolute gap behind InstructGPT 001 . SELF-INSTRUCT provides an almost annotation-free method for aligning pretrained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning.

人工生成instructions一方面代价很大，另一方面人工生成的instructions难以保证quantity, diversity, and creativity。

作者提出使用LLM从已有的task instruction出发，自动生成新的task instruction和对应的input-output，然后过滤掉不符合规则的新task instructions，再加入到已有的task instructions集合中。作者在这个自动构造的instruction data上fine-tuning GPT3，发现效果提升了33%，非常接近InstructGPT001的效果。

作者提出的方法：

![image-20230603150047353](https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230603150047353.png)

首先，作者拥有一个task pool，包括175 tasks (1 instruction and 1 instance for each task)。这175个初始的task instructions都是由本文作者自己创建的。

然后，作者从task pool中随机抽取8个task instructions（6 are from the human-written tasks, and 2 are from the model-generated tasks）。下面是产生新task instruction的prompt：

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230603150335100.png" style="zoom: 25%;" />

之后，作者使用LLM判断新产生的instruction是否是一个classification task（using 12 classification instructions and 19 non-classification instructions）：

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230603150505630.png"   style="zoom:25%;" />

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230603150518579.png" alt="image-20230603150518579" style="zoom:25%;" />

随后，对于新产生的task instruction，用LLM生成新的对应的instance。对于生成任务，作者先生成input，再生成output，作者称为Input-first Approach：

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230603150903068.png"   style="zoom:25%;" />

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230603150941339.png"  style="zoom:25%;" />

对于分类任务，作者发现如果是先生成input，LLM总是会倾向于生成某一个label的输入。因此作者使用LLM先生成output label，再让LLM生成input，作者称为Output-first Approach：

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230603151018452.png"   style="zoom:25%;" />

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230603151030519.png"   style="zoom:25%;" />

对于LLM生成的task instruction、input和output，需要通过一些规则过滤，比如：

- 只有当和已有的task instruction相似度全部比较低（$\mbox{ROUGE-L}< 0.7$）的时候，一个新task instruction会被添加到task pool里
- We also exclude instructions that contain some specific keywords (e.g., image, picture, graph) that usually can not be processed by LMs.
- When generating new instances for each instruction, we filter out instances that are exactly the same or those with the same input but different outputs.
- Invalid generations are identified and filtered out based on heuristics (e.g., instruction is too long or too short, instance output is a repetition of the input).

作者从原始的175个task出发，最后构造了5万多的task，并且差异性也比较大：

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230603151912820.png"   style="zoom:30%;" />

在SuperNI数据集上的实验结果：

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230603152723289.png"   style="zoom:30%;" />

SuperNI数据集大多是已有的NLP任务，为了进一步评估模型在实际使用场景下的价值，作者人工创建了一个包括252 task的新数据集。

