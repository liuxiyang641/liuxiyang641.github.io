---
title: MKGformer
notshow: false
date: 2022-09-02 14:28:25
categories:
- Paper
- MultiModal
tags:
- Transformer
- KG
- MultiModal
---

# Hybrid Transformer with Multi-level Fusion for Multimodal Knowledge Graph Completion

SIGIR 2022

[https://github.com/zjunlp/MKGformer](https://github.com/zjunlp/MKGformer)

ä½œè€…æå‡ºäº†ä¸€ç§åŸºäºTransformerçš„èƒ½å¤Ÿé€‚ç”¨äºä¸åŒå¤šæ¨¡æ€çŸ¥è¯†å›¾è°±é¢„æµ‹ä»»åŠ¡çš„æ–¹æ³•ï¼ŒMKGformerã€‚å¯¹äºä¸åŒçš„é¢„æµ‹ä»»åŠ¡ï¼Œä½œè€…é€šè¿‡å®šä¹‰è¾“å…¥æ•°æ®å’Œè¾“å‡ºæ•°æ®æ‹¥æœ‰ç›¸åŒçš„æ ¼å¼ï¼Œä»è€Œåˆ°è¾¾ä¸æ”¹å˜æ¨¡å‹ç»“æ„ï¼Œè¿˜èƒ½å¤ŸåŒæ—¶ç”¨äºä¸åŒé¢„æµ‹ä»»åŠ¡ï¼›å…¶æ¬¡ï¼Œä½œè€…æå‡ºäº†ä¸€ç§åœ¨textå’Œimageæ¨¡æ€ä¹‹é—´ï¼Œè¿›è¡Œmulti-levelæ··åˆçš„Transformerç»“æ„ã€‚

{% post_link collection/KGE-Collection %}

ä½œè€…åœ¨å¤šæ¨¡æ€KGè¡¥å…¨ã€å¤šæ¨¡æ€å…³ç³»æŠ½å–å’Œå¤šæ¨¡æ€å‘½åå®ä½“è¯†åˆ«ä¸‰ä¸ªä»»åŠ¡çš„æœ‰ç›‘ç£å­¦ä¹ å’Œä½èµ„æºå­¦ä¹ çš„åœºæ™¯ä¸Šè¿›è¡Œäº†å®éªŒã€‚

> Multimodal Knowledge Graphs (MKGs), which organize visualtext factual knowledge, have recently been successfully applied to tasks such as information retrieval, question answering, and recommendation system. Since most MKGs are far from complete, extensive knowledge graph completion studies have been proposed focusing on the multimodal entity, relation extraction and link prediction. However, different tasks and modalities require changes to the model architecture, and not all images/objects are relevant to text input, which hinders the applicability to diverse real-world scenarios. In this paper, we propose a hybrid transformer with multi-level fusion to address those issues. Specifically, we leverage a hybrid transformer architecture with unified input-output for diverse multimodal knowledge graph completion tasks. Moreover, we propose multi-level fusion, which integrates visual and text representation via coarse-grained prefix-guided interaction and fine-grained correlation-aware fusion modules. We conduct extensive experiments to validate that our MKGformer can obtain SOTA performance on four datasets of multimodal link prediction, multimodal RE, and multimodal NER.

<!--more-->

## Introduction

ä½œè€…è®¤ä¸ºç›®å‰çš„å¤šæ¨¡æ€KGCä»»åŠ¡å­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š

1. Architecture universalityï¼šä¸åŒçš„KGCä»»åŠ¡ï¼Œå¯¹äºä¸åŒæ¨¡æ€éœ€è¦è®¾è®¡ä¸åŒçš„ç¼–ç å™¨ï¼Œä»è€Œé™åˆ¶äº†æ¨¡å‹çš„é€šç”¨æ€§å’Œæ˜“ç”¨æ€§ã€‚
2. Modality contradictionï¼šå¤§å¤šçš„multimodal KGCçš„æ–¹æ³•å¾ˆå¤§ç¨‹åº¦ä¸Šå¿½ç•¥äº†å›¾åƒä¿¡æ¯å¯èƒ½å¸¦æ¥çš„å™ªéŸ³é—®é¢˜ï¼Œå› ä¸ºåœ¨å¤šæ¨¡æ€KGä¸­ï¼Œä¸€ä¸ªå®ä½“å¯èƒ½ä¼šå…³è”åˆ°å¤šä¸ªä¸åŒçš„imageï¼Œå®é™…ä¸Šåªæœ‰éƒ¨åˆ†çš„å›¾åƒä¿¡æ¯å¯èƒ½æ‰æ˜¯æ‰€éœ€çš„ã€‚

ä¸ºäº†è§£å†³ä¸Šè¿°é—®é¢˜ï¼Œä½œè€…æå‡ºäº†ï¼š

1. ä¹‹å‰æœ‰ç ”ç©¶è€…å‘ç°ï¼Œé¢„è®­ç»ƒæ¨¡å‹èƒ½å¤Ÿåœ¨Transformerçš„self-attentionå±‚å’Œfeed-forwardå±‚æ¿€æ´»å’Œè¾“å…¥æ•°æ®ç›¸å…³çš„knowledgeã€‚å› æ­¤ï¼Œä½œè€…å°è¯•åŸºäºTransformeræ¶æ„ï¼ŒåŒæ—¶å­¦ä¹ textualå’Œvisualçš„ä¿¡æ¯ã€‚
2. ä½œè€…æå‡ºçš„MKGformerï¼Œæœ‰ä¸¤ä¸ªæ ¸å¿ƒç»“æ„ï¼Œprefix-guided interaction module (PGI)å’Œcorrelation-aware fusion module (CAF)ã€‚å‰è€…ç”¨äºpre-reduceä¸åŒæ¨¡æ€çš„heterogeneityï¼Œåè€…ç”¨æ¥è¿›ä¸€æ­¥é™ä½æ¨¡å‹å¯¹äºirrelevant image/textçš„é”™è¯¯æ•æ„Ÿæ€§ã€‚

## Approach

æ€»ä½“ç»“æ„ï¼š

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220902145750837.png"  style="zoom:40%;" />

### Unified Multimodal KGC Framework

å¯¹äºæ–‡æœ¬ï¼Œä½¿ç”¨BERTè¿›è¡Œç¼–ç ï¼ˆT-Encoderï¼‰ï¼›å¯¹äºå›¾åƒï¼Œä½¿ç”¨ViT (*An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale*)è¿›è¡Œç¼–ç ï¼ˆV-Encoderï¼‰ã€‚å…ˆåˆ†åˆ«ç‹¬ç«‹è¿›è¡Œå‡ å±‚çš„å­¦ä¹ ä¹‹åï¼Œåœ¨æœ€å$M$å±‚ï¼Œåˆ©ç”¨ä½œè€…æå‡ºçš„M-Encoderè¿›è¡Œæ¨¡æ€æ··åˆã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œè¿™é‡Œçš„M-Encoderå¹¶ä¸æ˜¯é¢å¤–çš„å±‚ï¼Œè€Œæ˜¯ä½œè€…åœ¨BERTå’ŒViTçš„æ¶æ„åŸºç¡€ä¸Šï¼Œç›´æ¥è¿›è¡Œäº†æ”¹è¿›ï¼Œè®©ä¸åŒæ¨¡æ€æ¨¡å‹ä¹‹é—´èƒ½å¤Ÿè¿›è¡Œä¿¡æ¯æµé€šã€‚

æ¨¡å‹å¯¹äºè¾“å…¥å’Œè¾“å…¥æ•°æ®æ ¼å¼çš„å˜å½¢ï¼Œé¦–å…ˆæ˜¯æœ‰ä¸‰ä¸ªé¢„æµ‹ä»»åŠ¡ï¼š

1. Multimodal Link Prediction is the most popular task for multimodal KGC, which focuses on predicting the tail entity given the head entity and the query relation, denoted by $(ğ‘’_â„ ,ğ‘Ÿ, ?)$. é¢„æµ‹æœªçŸ¥factã€‚å¤šæ¨¡æ€å¸¦æ¥çš„æ–°æ¡ä»¶æ˜¯ï¼Œæ¯ä¸ªå®ä½“å¯èƒ½æ‹¥æœ‰å¤šä¸ªimage $I_h$ã€‚

2. Multimodal  Relation Extraction aims at linking relation mentions from text to a canonical relation type in a knowledge graph. ç»™å®šä¸€æ®µæè¿°æ–‡æœ¬$T$ï¼Œå·²çŸ¥å…¶ä¸­çš„å¤´å°¾å®ä½“$(e_h,e_t)$ï¼Œé¢„æµ‹å®ä½“é—´çš„å…³ç³»$r$ã€‚å¤šæ¨¡æ€å¸¦æ¥çš„æ–°æ¡ä»¶æ˜¯ï¼Œæè¿°æ–‡æœ¬æœ‰å¯¹åº”çš„image $I$ã€‚
3. Multimodal Named Entity Recognition is the task of extracting named entities from text sequences and corresponding images. ä»ä¸€ä¸ªtokenåºåˆ—ä¸­$T=\{w_1,\dots,w_n\}$ï¼Œé¢„æµ‹å¯¹åº”çš„æ ‡ç­¾åºåˆ—$y={y_1,\dots,y_n}$ã€‚å¤šæ¨¡æ€å¸¦æ¥çš„æ¡ä»¶æ˜¯ï¼Œæè¿°æ–‡æœ¬æœ‰å¯¹åº”çš„image $I$ã€‚

å¯¹äºè¾“å…¥æ•°æ®å’Œé¢„æµ‹æ•°æ®çš„å˜å½¢ï¼š

1. å¯¹äºå¤šæ¨¡æ€é“¾è·¯é¢„æµ‹ï¼Œä½œè€…é¦–å…ˆè®¾è®¡äº†ç‰¹åˆ«çš„ä¸€æ­¥æ“ä½œï¼ŒImage-text Incorporated Entity Modelingï¼Œå…·ä½“è€Œè¨€ï¼Œåœ¨ä¿æŒæ•´ä¸ªæ¨¡å‹å‚æ•°ä¸åŠ¨çš„æƒ…å†µä¸‹ï¼Œåªè®­ç»ƒå­¦ä¹ æ–°å‡ºç°çš„entity embeddingã€‚è¿™æ ·æ˜¯çš„æ–‡æœ¬ä¿¡æ¯å’Œè§†è§‰ä¿¡æ¯éƒ½èƒ½å¤Ÿèåˆåˆ°entity embeddingä¸Šã€‚å¯¹äºå®ä½“$e_i$å…³è”çš„å›¾åƒï¼Œè¾“å…¥åˆ°V-Encoderï¼›å¯¹äºå®ä½“$e_i$çš„æ–‡æœ¬æè¿°$d_{e_i}=(w_1,\dots,w_n)$ï¼Œæ”¹é€ ä¸ºï¼š

   <img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220902153930132.png" style="zoom:50%;" />

   ç„¶åé¢„æµ‹$[mask]$æ˜¯å®ä½“$e_i$çš„æ¦‚ç‡ã€‚

   <img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220902154509283.png"   style="zoom:50%;" />

   éšåï¼Œæ­£å¼å¼€å§‹é¢„æµ‹missing entityï¼Œå°†$(ğ‘’_â„ ,ğ‘Ÿ, ?)$å˜å½¢ä¸ºï¼š

   <img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220902154627839.png"   style="zoom:50%;" />

2. å¯¹äºå¤šæ¨¡æ€å‘½åå®ä½“è¯†åˆ«ï¼Œä½œè€…åˆ©ç”¨CRFå‡½æ•°ï¼ˆ*Neural Architectures for Named Entity Recognition.*ï¼‰è¿›è¡Œé¢„æµ‹ï¼ˆè¿™ä¸ªæ²¡çœ‹è¿‡..ï¼‰

3. å¯¹äºå¤šæ¨¡æ€å…³ç³»æŠ½å–ï¼Œä½œè€…åœ¨åŸæ¥çš„æ–‡æœ¬æè¿°ä¸Šï¼ŒåŠ å…¥$[CLS]$ tokenï¼Œæœ€åé¢„æµ‹$[CLS]$æ˜¯ç›®æ ‡å…³ç³»çš„æ¦‚ç‡ï¼š

   <img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220902155007744.png"   style="zoom:50%;" />

### Hybrid Transformer Architecture

é¦–å…ˆæ˜¯åŸå§‹çš„Transformerç»“æ„ï¼ŒMHAè¡¨ç¤ºå¤šå¤´æ³¨æ„åŠ›ï¼ŒFFNè¡¨ç¤ºå‰é¦ˆç½‘ç»œã€‚

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220902155227398.png"   style="zoom:50%;" />

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220902155249701.png"   style="zoom:50%;" />

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220902155307966.png"   style="zoom:50%;" />

V-Encoderï¼ŒViTï¼š

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220902155348746.png"   style="zoom:50%;" />

T-Encoderï¼ŒBERTï¼š

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220902155427336.png"   style="zoom:50%;" />

M-Encoderï¼Œåœ¨V-Encoderå’ŒT-Encoderä¹‹é—´ï¼Œå…ˆPGIï¼Œå†CAFï¼š

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220902155825058.png"   style="zoom:50%;" />

### Insights of M-Encoder

#### PGI

å¯¹äºPGIï¼ˆPrefix-guided Interaction Moduleï¼‰ï¼Œä½œè€…æ˜¯å—åˆ°äº†å‰é¢ç ”ç©¶çš„å½±å“ï¼ˆ*Prefix-Tuning: Optimizing Continuous Prompts for Generation*å’Œ*Towards a Unified View of Parameter-Efficient Transfer Learning.*ï¼‰ã€‚

ä½œè€…åœ¨è‡ªæ³¨æ„åŠ›å±‚ï¼Œè®©visual Transformerä¾§è€ƒè™‘èšåˆtextualä¿¡æ¯ï¼Œé€šè¿‡è®©visual queryå’Œtextual keyï¼Œtextual valueè¿›è¡Œæ“ä½œã€‚å®é™…ä¸Šæ˜¯è¯¢é—®å½“å‰çš„patch imageå’Œå“ªäº›tokenæ›´æ¥è¿‘ï¼Œç„¶åèšåˆtoken embeddingã€‚è§†è§‰ä¾§çš„queryï¼Œæ–‡æœ¬ä¾§çš„keyï¼Œæ–‡æœ¬ä¾§çš„valueï¼š

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220902161732093.png"   style="zoom:50%;" />

å¾ˆç®€å•çš„æ“ä½œï¼Œåº”è¯¥æ˜¯ç›´æ¥æ‹¼æ¥ã€‚ä½œè€…è¿›ä¸€æ­¥æ¨ç®—å…¬å¼ä¸ºï¼š

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220902161833850.png"   style="zoom:50%;" />

è¿™é‡Œæˆ‘æ²¡æœ‰ç›´æ¥æ¨ç®—å‡ºæ¥ã€‚ä½†æ˜¯ä»ä½œè€…æ¨ç®—å‡ºçš„å¯ä»¥çœ‹å‡ºæ¥ï¼Œå®è´¨ä¸Šå®ƒæ˜¯é™ä½äº†åŸæ¥å•çº¯çš„visual attentionï¼Œå¢åŠ äº†æ–‡æœ¬-å›¾åƒçš„è·¨æ¨¡æ€æ³¨æ„åŠ›ã€‚

#### CAF

å¯¹äºCAFï¼ˆCorrelation-aware Fusion Moduleï¼‰ï¼Œä½œè€…å—åˆ°å‰é¢ç ”ç©¶çš„å½±å“ï¼Œä¹‹å‰æœ‰äººå‘ç°Transformerä¸­çš„FFNå±‚èƒ½å¤Ÿå­¦ä¹ åˆ°task-specific textual patternï¼ˆ*Transformer Feed-Forward Layers Are Key-Value Memories*ï¼‰ã€‚å› æ­¤ä½œè€…é€šè¿‡è®¡ç®—token embeddingå’Œpatch embeddingä¹‹é—´çš„ç›¸ä¼¼æ€§çŸ©é˜µæ¥è¡¡é‡è§†è§‰ä¿¡æ¯çš„é‡è¦æ€§ã€‚

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220902163451512.png" alt="image-20220902163451512" style="zoom:50%;" />

ç„¶åèšåˆè§†è§‰ä¿¡æ¯ï¼Œæ–‡æœ¬ä¾§çš„queryï¼Œè§†è§‰ä¾§çš„keyï¼Œè§†è§‰ä¾§çš„valueï¼š

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220902163506163.png" alt="image-20220902163506163" style="zoom:50%;" />

ä¸Šè¿°è¿‡ç¨‹å®é™…å’Œè‡ªæ³¨æ„åŠ›çš„è¿‡ç¨‹æ˜¯ä¸€æ ·çš„ã€‚æœ€åæŠŠèšåˆçš„è§†è§‰ä¿¡æ¯å’ŒåŸæ¥çš„æ–‡æœ¬ä¿¡æ¯æ‹¼æ¥åˆ°ä¸€èµ·ï¼š

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220902163525975.png"   style="zoom:50%;" />



å›é¡¾ä¸‹ä¸Šè¿°ä¸¤ä¸ªè¿‡ç¨‹ï¼Œä½œè€…éƒ½æ˜¯æ²¡æœ‰ç›´æ¥åˆ›å»ºæ–°çš„layerè¿›è¡Œä¿¡æ¯èåˆï¼Œè€Œæ˜¯é€šè¿‡è®©ä¿¡æ¯åœ¨dual Transformerä¹‹é—´è¿›è¡Œæµé€šã€‚å› ä¸ºä½œè€…æå‡ºå›¾åƒçš„ä¿¡æ¯å™ªéŸ³å¾ˆå¤§ï¼Œå¯¹è‡ªæ³¨æ„åŠ›å±‚å’Œå…¨è¿æ¥å±‚çš„æ”¹é€ éƒ½æ˜¯å›´ç»•è¿™ä¸€ç‚¹æ¥çš„ã€‚å…ˆåœ¨æ³¨æ„åŠ›å±‚è®©æ–‡æœ¬ä¿¡æ¯æµé€šåˆ°è§†è§‰ä¿¡æ¯ä¸Šï¼Œè®©V-Encoderä¾§èƒ½å¤Ÿè€ƒè™‘æ–‡æœ¬ä¿¡æ¯ï¼Œè€Œä¸æ˜¯å•çº¯åœ¨patchä¹‹é—´èšåˆä¿¡æ¯ã€‚è¯•æƒ³ä¸‹ï¼Œå¦‚æœè®©è§†è§‰ä¿¡æ¯æµé€šåˆ°æ–‡æœ¬ä¿¡æ¯ä¸Šï¼Œé‚£ä¹ˆå°±æ„å‘³ç€è§†è§‰çš„å™ªéŸ³ç›´æ¥åŠ å…¥åˆ°äº†æ–‡æœ¬ä¾§ï¼Œä¸å¤ªåˆé€‚ã€‚éšåï¼Œåœ¨å…¨è¿æ¥å±‚è®©å·²ç»è€ƒè™‘äº†æ–‡æœ¬ä¿¡æ¯çš„è§†è§‰ä¿¡æ¯ï¼Œå†æµé€šå›æ–‡æœ¬ä¾§ï¼Œè¿›ä¸€æ­¥é™ä½è§†è§‰å™ªéŸ³ã€‚

## Experiments

### Experimental Setup

æ•°æ®é›†ï¼š

- é“¾è·¯é¢„æµ‹ï¼šWN18-IMGå’ŒFB15k-237-IMGï¼Œéƒ½æ˜¯åŸæ¥çš„æ•°æ®é›†çš„å®ä½“åˆ†åˆ«å…³è”åˆ°äº†10ä¸ªimageã€‚
- å…³ç³»æŠ½å–ï¼šMNREæ•°æ®é›†ï¼Œäººå·¥æ„é€ ï¼Œæ¥æºTwitterã€‚
- å‘½åå®ä½“è¯†åˆ«ï¼šTwitter-2017ï¼ŒåŒ…æ‹¬äº†2016-2017å¹´é—´ç”¨æˆ·çš„å¤šæ¨¡æ€postsã€‚

è®­ç»ƒè®¾ç½®ï¼š

åœ¨æ‰€æœ‰çš„æƒ…å†µä¸‹ï¼ŒM-Encoderä¿æŒ3å±‚ï¼ŒåŸºäºBERT_baseå’ŒViT-B/32ã€‚

### Overall Performance

é“¾è·¯é¢„æµ‹ï¼š

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220902165320438.png"   style="zoom:40%;" />

å…³ç³»æŠ½å–å’Œå‘½åå®ä½“è¯†åˆ«ï¼š

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220902165357479.png"   style="zoom:40%;" />

### Low-Resource Evaluation

ä½œè€…è®¤ä¸ºå¯¹æ–‡æœ¬å’Œå›¾åƒï¼Œä½¿ç”¨ç±»ä¼¼çš„ç½‘ç»œç»“æ„è¿›è¡Œå¤„ç†ï¼Œé™ä½äº†å·®å¼‚æ€§ï¼Œåœ¨ä½èµ„æºé¢„æµ‹ä»»åŠ¡ä¸­è¿™ç§ä½œç”¨æ›´åŠ çªå‡ºã€‚åœ¨æ•°æ®é‡æ›´å°‘çš„æƒ…å†µä¸‹ï¼Œéœ€è¦æƒ³åŠæ³•æ›´å¥½çš„å¤„ç†æ•°æ®æ¨¡æ€ä¹‹é—´çš„å·®å¼‚æ€§ï¼Œå› æ­¤æ¨¡å‹å¯¹äºä¸åŒæ¨¡æ€çš„å·®å¼‚æ€§çš„å¤„ç†èƒ½åŠ›å¯èƒ½éœ€è¦æ›´åŠ çªå‡ºã€‚

åœ¨ä½èµ„æºçš„è®¾ç½®ä¸‹ï¼Œä½œè€…å‘ç°ç›´æ¥æŠŠè§†è§‰-è¯­è¨€é¢„è®­ç»ƒæ¨¡å‹åº”ç”¨åˆ°KGCä»»åŠ¡ä¸Šï¼Œå¹¶æ²¡æœ‰è¡¨ç°å‡ºç‰¹åˆ«ä¼˜è¶Šçš„æ€§èƒ½ã€‚ä½œè€…è®¤ä¸ºå¯èƒ½æ˜¯åŸæ¥çš„é¢„è®­ç»ƒæ•°æ®å’ŒKGCä»»åŠ¡ç›¸å…³æ€§ä¸æ˜¯ç‰¹åˆ«ç›¸å…³çš„åŸå› ã€‚

ä½èµ„æºé“¾è·¯é¢„æµ‹ï¼š

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220902165501174.png"  style="zoom:40%;" />

ä½èµ„æºå…³ç³»æŠ½å–ï¼š

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220902165640257.png"   style="zoom:40%;" />

ä½èµ„æºå‘½åå®ä½“è¯†åˆ«ï¼š

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220902165544824.png"   style="zoom:40%;" />

### Ablation Study

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220902165723859.png"   style="zoom:40%;" />

### Case Analysis for Image-text Relevance

<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220902165850066.png"   style="zoom:50%;" />

ä»è¿™ä¸ªå®é™…æ¡ˆä¾‹å¯ä»¥çœ‹å‡ºï¼Œå›¾åƒç¡®å®å’Œæ•´ä¸ªæè¿°æ–‡æœ¬æ˜¯ç›¸å…³çš„ï¼Œä½†æ˜¯å›¾åƒä¸ä¸€å®šèƒ½å¤Ÿå¯¹åº”åˆ°æ‰€éœ€è¦çš„å®ä½“ã€‚å¹¶ä¸”ï¼Œä¸€ä¸ªå›¾åƒä¸­å­˜åœ¨å¾ˆå¤šä¸éœ€è¦çš„å™ªéŸ³ã€‚
