<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.0">

<link rel="preconnect" href="https://fonts.googleapis.com" crossorigin>
<link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin>
  <link rel="apple-touch-icon" sizes="180x180" href="/images/lxy-apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/lxy-favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/lxy-favicon-16x16.png">
  <link rel="mask-icon" href="/images/lxy-favicon.ico" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" integrity="sha256-DfWjNxDkM94fVBWx1H5BMMp0Zq7luBlV8QRcSES7s+0=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"liuxiyang641.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.12.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"width":300},"copycode":{"enable":true,"style":"mac"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":5,"unescape":true,"preload":false}}</script><script src="https://cdn.jsdelivr.net/npm/hexo-theme-next@8.12.2/source/js/config.min.js"></script>

    <meta name="description" content="Multimodal Machine Learning 对目前的多模态机器学习相关文献进行的调研list。">
<meta property="og:type" content="blog">
<meta property="og:title" content="MMML-survey-list">
<meta property="og:url" content="https://liuxiyang641.github.io/collection/MMML-survey-list/index.html">
<meta property="og:site_name" content="Liu Xiyang">
<meta property="og:description" content="Multimodal Machine Learning 对目前的多模态机器学习相关文献进行的调研list。">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2022-06-19T08:07:15.000Z">
<meta property="article:modified_time" content="2022-10-10T05:50:15.828Z">
<meta property="article:author" content="Liu Xiyang">
<meta property="article:tag" content="Collection">
<meta property="article:tag" content="Reading-list">
<meta property="article:tag" content="MMKG">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://liuxiyang641.github.io/collection/MMML-survey-list/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://liuxiyang641.github.io/collection/MMML-survey-list/","path":"collection/MMML-survey-list/","title":"MMML-survey-list"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>MMML-survey-list | Liu Xiyang</title>
  




<link rel="stylesheet" type="text/css" href="/css/injector/main.css" /><link rel="preload" as="style" href="/css/injector/light.css" /><link rel="preload" as="style" href="/css/injector/dark.css" />
  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Liu Xiyang</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">25</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">33</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">112</span></a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#multimodal-machine-learning"><span class="nav-number">1.</span> <span class="nav-text">Multimodal Machine Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#tutorial-and-course"><span class="nav-number">1.1.</span> <span class="nav-text">Tutorial and Course</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#paper"><span class="nav-number">1.2.</span> <span class="nav-text">Paper</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#acl-22"><span class="nav-number">1.2.1.</span> <span class="nav-text">ACL 22</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#acl-21"><span class="nav-number">1.2.2.</span> <span class="nav-text">ACL 21</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#acl-20"><span class="nav-number">1.2.3.</span> <span class="nav-text">ACL 20</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ijcai-21"><span class="nav-number">1.2.4.</span> <span class="nav-text">IJCAI 21</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ijcai-20"><span class="nav-number">1.2.5.</span> <span class="nav-text">IJCAI 20</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#emnlp-21"><span class="nav-number">1.2.6.</span> <span class="nav-text">EMNLP 21</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#emnlp-20"><span class="nav-number">1.2.7.</span> <span class="nav-text">EMNLP 20</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#naacl-22"><span class="nav-number">1.2.8.</span> <span class="nav-text">NAACL 22</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#naacl-21"><span class="nav-number">1.2.9.</span> <span class="nav-text">NAACL 21</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#naacl"><span class="nav-number">1.2.10.</span> <span class="nav-text">NAACL</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#www-22"><span class="nav-number">1.2.11.</span> <span class="nav-text">WWW 22</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#www-21"><span class="nav-number">1.2.12.</span> <span class="nav-text">WWW 21</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#www-20"><span class="nav-number">1.2.13.</span> <span class="nav-text">WWW 20</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#acm-mm-21"><span class="nav-number">1.2.14.</span> <span class="nav-text">ACM MM 21</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#acm-mm-20"><span class="nav-number">1.2.15.</span> <span class="nav-text">ACM MM 20</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#neurips-21"><span class="nav-number">1.2.16.</span> <span class="nav-text">NeurIPS 21</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#neurips-020"><span class="nav-number">1.2.17.</span> <span class="nav-text">NeurIPS 020</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#aaai-22"><span class="nav-number">1.2.18.</span> <span class="nav-text">AAAI 22</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#aaai-21"><span class="nav-number">1.2.19.</span> <span class="nav-text">AAAI 21</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#aaai-20"><span class="nav-number">1.2.20.</span> <span class="nav-text">AAAI 20</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sigir-22"><span class="nav-number">1.2.21.</span> <span class="nav-text">SIGIR 22</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sigir-21"><span class="nav-number">1.2.22.</span> <span class="nav-text">SIGIR 21</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sigir-20"><span class="nav-number">1.2.23.</span> <span class="nav-text">SIGIR 20</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#cvpr-22"><span class="nav-number">1.2.24.</span> <span class="nav-text">CVPR 22</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#cvpr-21"><span class="nav-number">1.2.25.</span> <span class="nav-text">CVPR 21</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#cvpr-20"><span class="nav-number">1.2.26.</span> <span class="nav-text">CVPR 20</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#kdd-22"><span class="nav-number">1.2.27.</span> <span class="nav-text">KDD 22</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#kdd-21"><span class="nav-number">1.2.28.</span> <span class="nav-text">KDD 21</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#iccv"><span class="nav-number">1.2.29.</span> <span class="nav-text">ICCV</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#eccv"><span class="nav-number">1.2.30.</span> <span class="nav-text">ECCV</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#cikm"><span class="nav-number">1.2.31.</span> <span class="nav-text">CIKM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#wsdm"><span class="nav-number">1.2.32.</span> <span class="nav-text">WSDM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#icmr"><span class="nav-number">1.2.33.</span> <span class="nav-text">ICMR</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#coling"><span class="nav-number">1.2.34.</span> <span class="nav-text">COLING</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Liu Xiyang"
      src="/images/lxy-avatar.jpg">
  <p class="site-author-name" itemprop="name">Liu Xiyang</p>
  <div class="site-description" itemprop="description">Try your best to be an ordinary man.</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">112</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">33</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">25</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/liuxiyang641" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;liuxiyang641" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:liuxiyang@buaa.edu.cn" title="E-Mail → mailto:liuxiyang@buaa.edu.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license site-overview-item animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdn.jsdelivr.net/npm/@creativecommons/vocabulary@2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>



        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://liuxiyang641.github.io/collection/MMML-survey-list/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/lxy-avatar.jpg">
      <meta itemprop="name" content="Liu Xiyang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Liu Xiyang">
      <meta itemprop="description" content="Try your best to be an ordinary man.">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="MMML-survey-list | Liu Xiyang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          MMML-survey-list<a href="https://github.com/liuxiyang641/liuxiyang641.github.io/edit/hexo/source/_posts/collection/MMML-survey-list.md" class="post-edit-link" title="编辑" rel="noopener" target="_blank"><i class="fa fa-pen-nib"></i></a>
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2022-06-19 16:07:15" itemprop="dateCreated datePublished" datetime="2022-06-19T16:07:15+08:00">2022-06-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2022-10-10 13:50:15" itemprop="dateModified" datetime="2022-10-10T13:50:15+08:00">2022-10-10</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Reading-list/" itemprop="url" rel="index"><span itemprop="name">Reading-list</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Reading-list/MMKG/" itemprop="url" rel="index"><span itemprop="name">MMKG</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>51k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>46 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="multimodal-machine-learning">Multimodal Machine Learning</h1>
<p>对目前的多模态机器学习相关文献进行的调研list。</p>
<span id="more"></span>
<h2 id="tutorial-and-course">Tutorial and Course</h2>
<table>
<colgroup>
<col style="width: 20%" />
<col style="width: 19%" />
<col style="width: 29%" />
<col style="width: 30%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">地址</th>
<th style="text-align: center;">标题</th>
<th>内容</th>
<th>评价</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><a target="_blank" rel="noopener" href="https://cmu-multicomp-lab.github.io/mmml-tutorial/schedule/">课程地址</a></td>
<td style="text-align: center;"><strong>T</strong>utorial on <strong>M</strong>ulti<strong>M</strong>odal <strong>M</strong>achine <strong>L</strong>earning</td>
<td>CMU 2022年最新的MMML tutorial，包括了slides和videos，主要介绍多模态机器学习相关概念和关键挑战；值得新人阅读。</td>
<td>内容较为全面，并且从专业的角度看到MMML的发展现状和挑战。作者说了很快会有一个600+引用文献的survey出现，值得期待。</td>
</tr>
<tr class="even">
<td style="text-align: center;"><a target="_blank" rel="noopener" href="https://cmu-multicomp-lab.github.io/adv-mmml-course/spring2022/">课程地址</a></td>
<td style="text-align: center;"><strong>A</strong>dvanced <strong>T</strong>opics in <strong>M</strong>ulti<strong>M</strong>odal <strong>M</strong>achine <strong>L</strong>earning</td>
<td>CMU 2022春季的MMML各个领域最新的研究挑战；没有视频，但是提供了讲义。</td>
<td>还未阅读，目前看到讲义内容不多。</td>
</tr>
<tr class="odd">
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h2 id="paper">Paper</h2>
<h3 id="acl-22">ACL 22</h3>
<ol type="1">
<li>Cross-Modal Discrete Representation Learning</li>
<li>Finding Structural Knowledge in Multimodal-BERT</li>
<li>Guided Attention Multimodal Multitask Financial Forecasting with Inter-Company Relationships and Global and Local News</li>
<li>Leveraging Unimodal Self-Supervised Learning for Multimodal Audio-Visual Speech Recognition</li>
<li>Leveraging Visual Knowledge in Language Tasks: An Empirical Study on Intermediate Pre-training for Cross-modal Knowledge Transfer</li>
<li>M3ED: Multi-modal Multi-scene Multi-label Emotional Dialogue Database</li>
<li>Modeling Temporal-Modal Entity Graph for Procedural Multimodal Machine Comprehension</li>
<li>MSCTD: A Multimodal Sentiment Chat Translation Dataset</li>
<li>Multi-Modal Sarcasm Detection via Cross-Modal Graph Convolutional Network</li>
<li>Multimodal Dialogue Response Generation</li>
<li>Multimodal fusion via cortical network inspired losses</li>
<li>Multimodal Sarcasm Target Identification in Tweets</li>
<li>On Vision Features in Multimodal Machine Translation</li>
<li>Phone-ing it in: Towards Flexible, Multi-Modal Language Model Training using Phonetic Representations of Data</li>
<li>Premise-based Multimodal Reasoning: Conditional Inference on Joint Textual and Visual Clues</li>
<li>RoCBert: Robust Chinese Bert with Multimodal Contrastive Pretraining</li>
<li>SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing</li>
<li>Understanding Multimodal Procedural Knowledge by Sequencing Multimodal Instructional Manuals</li>
<li>UniXcoder: Unified Cross-Modal Pre-training for Code Representation</li>
<li>When did you become so smart, oh wise one?! Sarcasm Explanation in Multi-modal Multi-party Dialogues</li>
<li>XDBERT: Distilling Visual Information to BERT from Cross-Modal Systems to Improve Language Understanding</li>
<li>M-SENA: An Integrated Platform for Multimodal Sentiment Analysis</li>
<li>MMCoQA: Conversational Question Answering over Text, Tables, and Images</li>
</ol>
<p>Findings of ACL 22</p>
<ol start="24" type="1">
<li>Reinforced Cross-modal Alignment for Radiology Report Generation</li>
<li>Cross-Modal Cloze Task: A New Task to Brain-to-Word Decoding</li>
<li>Sentiment Word Aware Multimodal Refinement for Multimodal Sentiment Analysis with ASR Errors</li>
<li>Modality-specific Learning Rates for Effective Multimodal Additive Late-fusion</li>
<li>Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation</li>
<li>Assessing Multilingual Fairness in Pre-trained Multimodal Representations</li>
<li>Modular and Parameter-Efficient Multimodal Fusion with Prompting</li>
<li>Comprehensive Multi-Modal Interactions for Referring Image Segmentation</li>
<li>Attention as Grounding: Exploring Textual and Cross-Modal Attention on Entities and Relations in Language-and-Vision Transformer</li>
<li>Improving Candidate Retrieval with Entity Profile Generation for Wikidata Entity Linking（把mention和Wikidata中的实体联系在一起，没有使用多模态信息）</li>
</ol>
<h3 id="acl-21">ACL 21</h3>
<ol type="1">
<li>Multimodal Sentiment Detection Based on Multi-channel Graph Neural Networks</li>
<li>Self-Supervised Multimodal Opinion Summarization</li>
<li>Learning Relation Alignment for Calibrated Cross-modal Retrieval</li>
<li>KM-BART: Knowledge Enhanced Multimodal BART for Visual Commonsense Generation</li>
<li>The Possible, the Plausible, and the Desirable: Event-Based Modality Detection for Language Processing</li>
<li>Factuality Assessment as Modal Dependency Parsing</li>
<li>Multi-stage Pre-training over Simplified Multimodal Pre-training Models</li>
<li>LayoutLMv2: Multi-modal Pre-training for Visually-rich Document Understanding</li>
<li>UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning</li>
<li>Missing Modality Imagination Network for Emotion Recognition with Uncertain Missing Modalities</li>
<li>Competence-based Multimodal Curriculum Learning for Medical Report Generation</li>
<li>MultiMET: A Multimodal Dataset for Metaphor Understanding</li>
<li>Constructing Multi-Modal Dialogue Dataset by Replacing Text with Semantically Relevant Images</li>
<li>Learning Language and Multimodal Privacy-Preserving Markers of Mood from Mobile Data</li>
<li>CTFN: Hierarchical Learning for Multimodal Sentiment Analysis Using Coupled-Translation Fusion Network</li>
<li>MMGCN: Multimodal Fusion via Deep Graph Convolution Network for Emotion Recognition in Conversation</li>
<li>Cross-modal Memory Networks for Radiology Report Generation</li>
<li>Multi-perspective Coherent Reasoning for Helpfulness Prediction of Multimodal Reviews</li>
<li>Good for Misconceived Reasons: An Empirical Revisiting on the Need for Visual Context in Multimodal Machine Translation</li>
<li>Multimodal Multi-Speaker Merger &amp; Acquisition Financial Modeling: A New Task, Dataset, and Neural Baselines</li>
<li>More than Text: Multi-modal Chinese Word Segmentation</li>
<li>Constructing Multi-Modal Dialogue Dataset by Replacing Text with Semantically Relevant Images</li>
</ol>
<p>Findings of ACL 21</p>
<ol start="23" type="1">
<li>Multimodal Incremental Transformer with Visual Grounding for Visual Dialogue Generation</li>
<li>GeoQA: A Geometric Question Answering Benchmark Towards Multimodal Numerical Reasoning</li>
<li>Read, Listen, and See: Leveraging Multimodal Information Helps Chinese Spell Checking</li>
<li>Deciphering Implicit Hate: Evaluating Automated Detection Algorithms for Multimodal Hate</li>
<li>Entheos: A Multimodal Dataset for Studying Enthusiasm</li>
<li>Multimodal Fusion with Co-Attention Networks for Fake News Detection</li>
<li>GEM: A General Evaluation Benchmark for Multimodal Tasks</li>
<li>Transformer-Exclusive Cross-Modal Representation for Vision and Language</li>
<li>Probing Multi-modal Machine Translation with Pre-trained Language Model</li>
<li>Multimodal Graph-based Transformer Framework for Biomedical Relation Extraction</li>
<li>A Text-Centered Shared-Private Framework via Cross-Modal Prediction for Multimodal Sentiment Analysis</li>
</ol>
<h3 id="acl-20">ACL 20</h3>
<ol type="1">
<li>Cross-modal Language Generation using Pivot Stabilization for Web-scale Language Coverage</li>
<li>Multimodal Quality Estimation for Machine Translation</li>
<li>MMPE: A Multi-Modal Interface for Post-Editing Machine Translation</li>
<li>Integrating Multimodal Information in Large Pretrained Transformers</li>
<li>MultiQT: Multimodal Learning for Real-Time Question Tracking in Speech</li>
<li>Multiresolution and Multimodal Speech Recognition with Transformers</li>
<li>A Novel Graph-based Multi-modal Fusion Encoder for Neural Machine Translation</li>
<li>Learning to Recover from Multi-Modality Errors for Non-Autoregressive Neural Machine Translation</li>
<li>Improving Multimodal Named Entity Recognition via Entity Span Detection with Unified Multimodal Transformer</li>
<li>CH-SIMS: A Chinese Multimodal Sentiment Analysis Dataset with Fine-grained Annotations of Modality</li>
<li>Reasoning with Multimodal Sarcastic Tweets via Modeling Cross-Modality Contrast and Semantic Association</li>
<li>Multimodal Transformer for Multimodal Machine Translation</li>
<li>Sentiment and Emotion help Sarcasm? A Multi-task Learning Framework for Multi-Modal Sarcasm, Sentiment and Emotion Analysis</li>
<li>Towards Emotion-aided Multi-modal Dialogue Act Classification</li>
<li>A Recipe for Creating Multimodal Aligned Datasets for Sequential Tasks</li>
<li>Benchmarking Multimodal Regex Synthesis with Complex Structures</li>
<li>Clue: Cross-modal Coherence Modeling for Caption Generation</li>
<li>Multimodal Neural Graph Memory Networks for Visual Question Answering</li>
<li>Cross-Modality Relevance for Reasoning on Language and Vision</li>
<li>Unsupervised Multimodal Neural Machine Translation with Pseudo Visual Pivoting ### IJCAI 22</li>
<li>Cross-modal Representation Learning and Relation Reasoning for Bidirectional Adaptive Manipulation</li>
<li>Unsupervised Misaligned Infrared and Visible Image Fusion via Cross-Modality Image Generation and Registration</li>
<li>SHAPE: An Unified Approach to Evaluate the Contribution and Cooperation of Individual Modalities</li>
<li>MMT: Multi-Way Multi-Modal Transformer for Multimodal Learning</li>
<li>AutoAlign: Pixel-Instance Feature Aggregation for Multi-Modal 3D Object Detection</li>
<li>Lightweight Bimodal Network for Single-Image Super-Resolution via Symmetric CNN and Recursive Transformer</li>
<li>Unsupervised Multi-Modal Medical Image Registration via Discriminator-Free Image-to-Image Translation</li>
<li>Targeted Multimodal Sentiment Classification based on Coarse-to-Fine Grained Image-Target Matching</li>
<li>Recipe2Vec: Multi-modal Recipe Representation Learning with Graph Neural Networks</li>
<li>PlaceNet: Neural Spatial Representation Learning with Multimodal Attention</li>
<li>Representation Learning for Compressed Video Action Recognition via Attentive Cross-modal Interaction with Motion Enhancement</li>
<li>MA-ViT: Modality-Agnostic Vision Transformers for Face Anti-Spoofing</li>
<li>Unsupervised Voice-Face Representation Learning by Cross-Modal Prototype Contrast</li>
<li>MFAN: Multi-modal Feature-enhanced Attention Networks for Rumor Detection</li>
</ol>
<h3 id="ijcai-21">IJCAI 21</h3>
<ol type="1">
<li>Dig into Multi-modal Cues for Video Retrieval with Hierarchical Alignment</li>
<li>Deep Unified Cross-Modality Hashing by Pairwise Data Alignment</li>
<li>Weakly Supervised Dense Video Captioning via Jointly Usage of Knowledge Distillation and Cross-modal Matching</li>
<li>MRD-Net: Multi-Modal Residual Knowledge Distillation for Spoken Question Answering</li>
<li>Rethinking Label-Wise Cross-Modal Retrieval from A Semantic Sharing Perspective</li>
<li>MDNN: A Multimodal Deep Neural Network for Predicting Drug-Drug Interaction Events</li>
<li>Modality-aware Style Adaptation for RGB-Infrared Person Re-Identification</li>
<li>Multimodal Transformer Network for Pedestrian Trajectory Prediction</li>
<li>SalientSleepNet: Multimodal Salient Wave Detection Network for Sleep Staging</li>
<li>UIBert: Learning Generic Multimodal Representations for UI Understanding</li>
</ol>
<h3 id="ijcai-20">IJCAI 20</h3>
<ol type="1">
<li>A Similarity Inference Metric for RGB-Infrared Cross-Modality Person Re-identification</li>
<li>Set and Rebase: Determining the Semantic Graph Connectivity for Unsupervised Cross-Modal Hashing</li>
<li>A Modal Logic for Joint Abilities under Strategy Commitments</li>
<li>Embodied Multimodal Multitask Learning</li>
<li>Triple-GAIL: A Multi-Modal Imitation Learning Framework with Generative Adversarial Nets</li>
<li>“The Squawk Bot”∗ : Joint Learning Of Time Series And Text Data Modalities For Automated Financial Information Filtering</li>
<li>Interpretable Multimodal Learning for Intelligent Regulation in Online Payment Systems</li>
</ol>
<h3 id="emnlp-21">EMNLP 21</h3>
<ol type="1">
<li>Improving Multimodal fusion via Mutual Dependency Maximisation</li>
<li>Text2Mol: Cross-Modal Molecule Retrieval with Natural Language Queries</li>
<li>Iconary: A Pictionary-Based Game for Testing Multimodal Communication with Drawings and Text</li>
<li>Multimodal Phased Transformer for Sentiment Analysis</li>
<li>CTAL: Pre-training Cross-modal Transformer for Audio-and-Language Representations</li>
<li>Vision Guided Generative Pre-trained Language Models for Multimodal Abstractive Summarization</li>
<li>How to Leverage Multimodal EHR Data for Better Medical Predictions?</li>
<li>Joint Multi-modal Aspect-Sentiment Analysis with Auxiliary Cross-modal Relation Detection</li>
<li>Multi-Modal Open-Domain Dialogue</li>
<li>SIMMC 2.0: A Task-oriented Dialog Dataset for Immersive Multimodal Conversations</li>
<li>Hitting your MARQ: Multimodal ARgument Quality Assessment in Long Debate Video</li>
<li>NewsCLIPpings: Automatic Generation of Out-of-Context Multimodal Media</li>
<li>Vision Matters When It Should: Sanity Checking Multimodal Machine Translation Models</li>
<li>Unimodal and Crossmodal Refinement Network for Multimodal Sequence Fusion</li>
<li>Improving Multimodal Fusion with Hierarchical Mutual Information Maximization for Multimodal Sentiment Analysis</li>
<li>On Pursuit of Designing Multi-modal Transformer for Video Grounding</li>
<li>Vision-and-Language or Vision-for-Language? On Cross-Modal Influence in Multimodal Transformers</li>
<li>M-Arg: Multimodal Argument Mining Dataset for Political Debates with Audio and Transcripts</li>
<li>VQA-MHUG: A Gaze Dataset to Study Multimodal Neural Attention in Visual Question Answering</li>
<li>Point-of-Interest Type Prediction using Text and Images（首个根据推特的文本和图像判断POI类型）</li>
</ol>
<p>Findings of EMNLP 21</p>
<ol start="20" type="1">
<li>Self-supervised Contrastive Cross-Modality Representation Learning for Spoken Question Answering</li>
<li>Cross-Modal Retrieval Augmentation for Multi-Modal Classification</li>
<li>What Does Your Smile Mean? Jointly Detecting Multi-Modal Sarcasm and Sentiment Using Quantum Probability</li>
<li>Entity-level Cross-modal Learning Improves Multi-modal Machine Translation</li>
<li>Which is Making the Contribution: Modulating Unimodal and Cross-modal Dynamics for Multimodal Sentiment Analysis</li>
<li>Optimal Neural Program Synthesis from Multimodal Specifications</li>
<li>MIRTT: Learning Multimodal Interaction Representations from Trilinear Transformers for Visual Question Answering</li>
<li>DialogueTRM: Exploring Multi-Modal Emotion Dynamics in Conversations</li>
<li>An animated picture says at least a thousand words: Selecting Gif-based Replies in Multimodal Dialog</li>
<li>MURAL: Multimodal, Multitask Retrieval Across Languages</li>
<li>MSD: Saliency-aware Knowledge Distillation for Multimodal Understanding</li>
<li>MOMENTA: A Multimodal Framework for Detecting Harmful Memes and Their Targets</li>
</ol>
<h3 id="emnlp-20">EMNLP 20</h3>
<ol type="1">
<li>X-LXMERT: Paint, Caption and Answer Questions with Multi-Modal Transformers</li>
<li>Generating Image Descriptions via Sequential Cross-Modal Alignment Guided by Human Gaze</li>
<li>Multimodal Routing: Improving Local and Global Interpretability of Multimodal Language Analysis</li>
<li>Multi-modal Multi-label Emotion Detection with Modality and Label Dependence</li>
<li>Multistage Fusion with Forget Gate for Multimodal Summarization in Open-Domain Videos</li>
<li>Detecting Cross-Modal Inconsistency to Defend Against Neural Fake News</li>
<li>VMSMO: Learning to Generate Multimodal Summary for Video-based News Articles</li>
<li>Does my multimodal model learn cross-modal interactions? It’s harder to tell than you might think!</li>
<li>CMU-MOSEAS: A Multimodal Language Dataset for Spanish, Portuguese, German and French</li>
<li>Cross-Media Keyphrase Prediction: A Unified Framework with Multi-Modality Multi-Head Attention and Image Wordings</li>
<li>Multimodal Joint Attribute Prediction and Value Extraction for E-commerce Product</li>
<li>Unsupervised Natural Language Inference via Decoupled Multimodal Contrastive Learning</li>
<li>MAF: Multimodal Alignment Framework for Weakly-Supervised Phrase Grounding</li>
</ol>
<p>Findings of EMNLP 20</p>
<ol start="15" type="1">
<li>Open-Ended Visual Question Answering by Multi-Modal Domain Adaptation</li>
<li>Dual Low-Rank Multimodal Fusion</li>
<li>DocStruct: A Multimodal Method to Extract Hierarchy Structure in Document for General Form Understanding</li>
<li>Modeling Intra and Inter-modality Incongruity for Multi-Modal Sarcasm Detection</li>
<li>MultiDM-GCN: Aspect-guided Response Generation in Multi-domain Multi-modal Dialogue System using Graph Convolutional Network</li>
<li>Fine-Grained Grounding for Multimodal Speech Recognition</li>
<li>MMFT-BERT: Multimodal Fusion Transformer with BERT Encodings for Visual Question Answering</li>
</ol>
<h3 id="naacl-22">NAACL 22</h3>
<ol type="1">
<li>Analyzing Modality Robustness in Multimodal Sentiment Analysis</li>
<li>Beyond Emotion: A Multi-Modal Dataset for Human Desire Understanding</li>
<li>Twitter-COMMs: Detecting Climate, COVID, and Military Multimodal Misinformation</li>
<li>A Study of Syntactic Multi-Modality in Non-Autoregressive Machine Translation</li>
<li>Modal Dependency Parsing via Language Model Priming</li>
<li>Multimodal Dialogue State Tracking</li>
<li>GMN: Generative Multi-modal Network for Practical Document Information Extraction</li>
<li>A Computational Acquisition Model for Multimodal Word Categorization</li>
<li>COGMEN: COntextualized GNN based Multimodal Emotion recognitioN</li>
<li>Cross-modal Contrastive Learning for Speech Translation</li>
<li>Visual Commonsense in Pretrained Unimodal and Multimodal Models</li>
<li>MCSE: Multimodal Contrastive Learning of Sentence Embeddings</li>
<li>JointLK: Joint Reasoning with Language Models and Knowledge Graphs for Commonsense Question Answering（作者把LM和KG看做是两个modality）</li>
<li>KAT: A Knowledge Augmented Transformer for Vision-and-Language</li>
</ol>
<p>Findings of NNACL 22</p>
<ol start="13" type="1">
<li>Multimodal Intent Discovery from Livestream Videos</li>
<li>Learning to Embed Multi-Modal Contexts for Situated Conversational Agents</li>
<li>MM-Claims: A Dataset for Multimodal Claim Detection in Social Media</li>
<li>Cross-Lingual Cross-Modal Consolidation for Effective Multilingual Video Corpus Moment Retrieval</li>
<li>CLMLF:A Contrastive Learning and Multi-Layer Fusion Method for Multimodal Sentiment Detection</li>
</ol>
<h3 id="naacl-21">NAACL 21</h3>
<ol type="1">
<li>MTAG: Modal-Temporal Attention Graph for Unaligned Human Multimodal Language Sequences</li>
<li>Improving Cross-Modal Alignment in Vision Language Navigation via Syntactic Information</li>
<li>Multilingual Multimodal Pre-training for Zero-Shot Cross-Lingual Transfer of Vision-Language Models</li>
<li>MUSER: MUltimodal Stress Detection using Emotion Recognition as an Auxiliary Task</li>
<li>Cross-lingual Cross-modal Pretraining for Multimodal Retrieval</li>
<li>An Empirical Investigation of Bias in the Multimodal Analysis of Financial Earnings Calls</li>
<li>Multimodal End-to-End Sparse Model for Emotion Recognition</li>
<li>MIMOQA: Multimodal Input Multimodal Output Question Answering</li>
<li>Towards Sentiment and Emotion aided Multi-modal Speech Act Classification in Twitter</li>
<li>MM-AVS: A Full-Scale Dataset for Multi-modal Summarization</li>
<li>Larger-Context Tagging: When and Why Does It Work?</li>
</ol>
<h3 id="naacl">NAACL</h3>
<p>Quantifying the visual concreteness of words and topics in multimodal datasets. NAACL 18</p>
<h3 id="www-22">WWW 22</h3>
<ol type="1">
<li>Multimodal Continual Graph Learning with Neural Architecture Search</li>
<li>Modality Matches Modality: Pretraining Modality-Disentangled Item Representations for Recommendation</li>
<li>Cross-modal Ambiguity Learning for Multimodal Fake News Detection</li>
<li>A Duo-generative Approach to Explainable Multimodal COVID-19 Misinformation Detection</li>
<li>On Explaining Multimodal Hateful Meme Detection Models</li>
</ol>
<h3 id="www-21">WWW 21</h3>
<ol type="1">
<li>High-Dimensional Sparse Cross-Modal Hashing with Fine-Grained Similarity Embedding</li>
</ol>
<h3 id="www-20">WWW 20</h3>
<ol type="1">
<li>Nowhere to Hide: Cross-modal Identity Leakage between Biometrics and Devices</li>
<li>Adversarial Multimodal Representation Learning for Click-Through Rate Prediction</li>
<li>Learning from Cross-Modal Behavior Dynamics with Graph-Regularized Neural Contextual Bandit</li>
<li>Learning to Respond with Stickers: A Framework of Unifying Multi-Modality in Multi-Turn Dialog</li>
<li>Domain Adaptive Multi-Modality Neural Attention Network for Financial Forecasting</li>
<li>A Multimodal Variational Encoder-Decoder Framework for Micro-video Popularity Prediction</li>
<li>Multimodal Post Attentive Profiling for Influencer Marketing</li>
<li>TransModality: An End2End Fusion Method with Transformer for Multimodal Sentiment Analysis</li>
</ol>
<h3 id="acm-mm-21">ACM MM 21</h3>
<ol type="1">
<li>Theophany: Multimodal Speech Augmentation in Instantaneous Privacy Channels</li>
<li>Multimodal Asymmetric Dual Learning for Unsupervised Eyeglasses Removal</li>
<li>HetEmotionNet: Two-Stream Heterogeneous Graph Recurrent Neural Network for Multi-modal Emotion Recognition</li>
<li>Graph Convolutional Multi-modal Hashing for Flexible Multimedia Retrieval</li>
<li>A Stepwise Matching Method for Multimodal Image based on Cascaded Network</li>
<li>Learning What and When to Drop: Adaptive Multimodal and Contextual Dynamics for Emotion Recognition in Conversation</li>
<li>Differentiated Learning for Multi-Modal Domain Adaptation</li>
<li>Database-adaptive Re-ranking for Enhancing Cross-modal Image Retrieval</li>
<li>Exploiting BERT For Multimodal Target Sentiment Classification Through Input Space Translation</li>
<li>Pre-training Graph Transformer with Multimodal Side Information for Recommendation</li>
<li>Simplifying Multimodal Emotion Recognition with Single Eye Movement Modality</li>
<li>M3TR: Multi-modal Multi-label Recognition with Transformer</li>
<li>Multimodal Dialog System: Relational Graph-based Context-aware Question Understanding</li>
<li>Towards a Unified Middle Modality Learning for Visible-Infrared Person Re-Identification</li>
<li>ROSITA: Enhancing Vision-and-Language Semantic Alignments via Cross- and Intra-modal Knowledge Integration</li>
<li>Joint-teaching: Learning to Refine Knowledge for Resource-constrained Unsupervised Cross-modal Retrieval</li>
<li>Cross-modal Consensus Network for Weakly Supervised Temporal Action Localization</li>
<li>Searching a Hierarchically Aggregated Fusion Architecture for Fast Multi-Modality Image Fusion</li>
<li>Deep Self-Supervised t-SNE for Multi-modal Subspace Clustering</li>
<li>Multimodal Video Summarization via Time-Aware Transformers</li>
<li>Local Graph Convolutional Networks for Cross-Modal Hashing</li>
<li>Cross-modality Discrepant Interaction Network for RGB-D Salient Object Detection</li>
<li>Conceptual and Syntactical Cross-modal Alignment with Cross-level Consistency for Image-Text Matching</li>
<li>Unsupervised Cross-Modal Distillation for Thermal Infrared Tracking</li>
<li>Understanding Chinese Video and Language via Contrastive Multimodal Pre-Training</li>
<li>Cross-Modal Retrieval and Synthesis (X-MRS): Closing the Modality Gap in Shared Representation Learning</li>
<li>Multimodal Compatibility Modeling via Exploring the Consistent and Complementary Correlations</li>
<li>Missing Data Imputation for Solar Yield Prediction using Temporal Multi-Modal Variational Auto-Encoder</li>
<li>Cross-Modal Recipe Embeddings by Disentangling Recipe Contents and Dish Styles</li>
<li>Cross-modal Self-Supervised Learning for Lip Reading: When Contrastive Learning meets Adversarial Training</li>
<li>Multi-Modal Multi-Instance Learning for Retinal Disease Recognition</li>
<li>Transformer-based Feature Reconstruction Network for Robust Multimodal Sentiment Analysis</li>
<li>Efficient Multi-Modal Fusion with Diversity Analysis</li>
<li>SINGA-Easy: An Easy-to-Use Framework for MultiModal Analysis</li>
<li>CoCo-BERT: Improving Video-Language Pre-training with Contrastive Cross-modal Matching and Denoising</li>
<li>DRDF: Determining the Importance of Different Multimodal Information with Dual-Router Dynamic Framework</li>
<li>MCCN: Multimodal Coordinated Clustering Network for Large-Scale Cross-modal Retrieval</li>
<li>Heterogeneous Feature Fusion and Cross-modal Alignment for Composed Image Retrieval</li>
<li>Exploring Graph-Structured Semantics for Cross-Modal Retrieval</li>
<li>Cross Modal Compression: Towards Human-comprehensible Semantic Compression</li>
<li>Sensor-Augmented Egocentric-Video Captioning with Dynamic Modal Attention</li>
<li>Cascade Cross-modal Attention Network for Video Actor and Action Segmentation from a Sentence</li>
<li>Cross-Modal Joint Prediction and Alignment for Composed Query Image Retrieval</li>
<li>MM-Flow: Multi-modal Flow Network for Point Cloud Completion</li>
<li>Meta Self-Paced Learning for Cross-Modal Matching</li>
<li>Learning Disentangled Factors from Paired Data in Cross-Modal Retrieval: An Implicit Identifiable VAE Approach</li>
<li>Multi-Modal Sarcasm Detection with Interactive In-Modal and Cross-Modal Graphs</li>
<li>Fine-grained Cross-modal Alignment Network for Text-Video Retrieval</li>
<li>Cross-Modal Generalization: Learning in Low Resource Modalities via Meta-Alignment</li>
<li>Hierarchical Multi-Task Learning for Diagram Question Answering with Multi-Modal Transformer</li>
<li>Product-oriented Machine Translation with Cross-modal Cross-lingual Pre-training</li>
<li>Graph Neural Networks for Knowledge Enhanced Visual Representation of Paintings</li>
</ol>
<h3 id="acm-mm-20">ACM MM 20</h3>
<ol type="1">
<li>MM-Hand: 3D-Aware Multi-Modal Guided Hand Generative Network for 3D Hand Pose Synthesis</li>
<li>Cross-Modal Omni Interaction Modeling for Phrase Grounding</li>
<li>VideoIC: A Video Interactive Comments Dataset and Multimodal Multitask Learning for Comments Generation</li>
<li>Multimodal Dialog Systems via Capturing Context-aware Dependencies of Semantic Elements</li>
<li>Semi-supervised Multi-modal Emotion Recognition with Cross-Modal Distribution Matching</li>
<li>Adaptive Multimodal Fusion for Facial Action Units Recognition</li>
<li>Crossing You in Style: Cross-modal Style Transfer from Music to Visual Arts</li>
<li>Incomplete Cross-modal Retrieval with Dual-Aligned Variational Autoencoders</li>
<li>Joint Attribute Manipulation and Modality Alignment Learning for Composing Text and Image to Image Retrieval</li>
<li>Supervised Hierarchical Deep Hashing for Cross-Modal Retrieval</li>
<li>Multi-modal Attentive Graph Pooling Model for Community Question Answer Matching</li>
<li>Towards Modality Transferable Visual Information Representation with Optimal Model Compression</li>
<li>Deep Multimodal Neural Architecture Search</li>
<li>Cross-domain Cross-modal Food Transfer</li>
<li>Finding Achilles’ Heel: Adversarial Attack on Multi-modal Action Recognition</li>
<li>Cross-Modal Relation-Aware Networks for Audio-Visual Event Localization</li>
<li>Learning Deep Multimodal Feature Representation with Asymmetric Multi-layer Fusion</li>
<li>Deep Multi-modality Soft-decoding of Very Low Bit-rate Face Videos</li>
<li>Jointly Cross- and Self-Modal Graph Attention Network for Query-Based Moment Localization</li>
<li>STRONG: Spatio-Temporal Reinforcement Learning for Cross-Modal Video Moment Localization</li>
<li>Improving Intra- and Inter-Modality Visual Relation for Image Captioning</li>
<li>Multimodal Attention with Image Text Spatial Relationship for OCR-Based Image Captioning</li>
<li>ADHD Intelligent Auxiliary Diagnosis System Based on Multimodal Information Fusion (Demo)</li>
<li>A Cross-modality and Progressive Person Search System (Demo)</li>
<li>Multimodal Deep Learning for Social Media Popularity Prediction With Attention Mechanism</li>
<li>Video Relation Detection with Trajectory-aware Multi-modal Features</li>
<li>XlanV Model with Adaptively Multi-Modality Feature Fusing for Video Captioning</li>
<li>A Quantitative Comparison of Different Machine Learning Approaches for Human Spermatozoa Quality Prediction Using Multimodal Datasets</li>
<li>Learning Self-Supervised Multimodal Representations of Human Behaviour</li>
<li>Cross-modal Non-linear Guided Attention and Temporal Coherence in Multi-modal Deep Video Models</li>
<li>Look, Read and Feel: Benchmarking Ads Understanding with Multimodal Multitask Learning</li>
<li>Down to the Last Detail: Virtual Try-on with Fine-grained Details</li>
<li>MEmoR: A Dataset for Multimodal Emotion Reasoning in Videos</li>
<li>Modeling both Intra- and Inter-modal Influence for Real-Time Emotion Detection in Conversations</li>
<li>Transformer-based Label Set Generation for Multi-modal Multi-label Emotion Detection</li>
<li>CM-BERT: Cross-Modal BERT for Text-Audio Sentiment Analysis</li>
<li>Label Embedding Online Hashing for Cross-Modal Retrieval</li>
<li>Class-Aware Modality Mix and Center-Guided Metric Learning for Visible-Thermal Person Re-Identification</li>
<li>RGB2LIDAR: Towards Solving Large-Scale Cross-Modal Visual Localization</li>
<li>MMFL: Multimodal Fusion Learning for Text-Guided Image Inpainting</li>
<li>MISA: Modality-Invariant and -Specific Representations for Multimodal Sentiment Analysis</li>
<li>Multi-modal Cooking Workflow Construction for Food Recipes</li>
<li>Adaptive Temporal Triplet-loss for Cross-modal Embedding Learning</li>
<li>Deep-Modal: Real-Time Impact Sound Synthesis for Arbitrary Shapes</li>
<li>K-armed Bandit based Multi-modal Network Architecture Search for Visual Question Answering</li>
<li>Dynamic Context-guided Capsule Network for Multimodal Machine Translation</li>
<li>KBGN: Knowledge-Bridge Graph Network for Adaptive Vision-Text Reasoning in Visual Dialogue</li>
<li>Learning Modality-Invariant Latent Representations for Generalized Zero-shot Learning</li>
<li>Boosting Continuous Sign Language Recognition via Cross Modality Augmentation</li>
<li>Towards More Explainability: Concept Knowledge Mining Network for Event Recognition</li>
<li>Memory-Based Network for Scene Graph with Unbalanced Relations</li>
</ol>
<h3 id="neurips-21">NeurIPS 21</h3>
<ol type="1">
<li>Exploring Cross-Video and Cross-Modality Signals for Weakly-Supervised Audio-Visual Video Parsing</li>
<li>Cross-Modal Domain Adaptation for Cost-Efficient Visual Reinforcement Learning</li>
<li>End-to-end Multi-modal Video Temporal Grounding</li>
<li>Multi-modal Dependency Tree for Video Captioning</li>
<li>Perceptual Score: What Data Modalities Does Your Model Perceive?</li>
<li>UFC-BERT: Unifying Multi-Modal Controls for Conditional Image Synthesis</li>
<li>Learning with Noisy Correspondence for Cross-modal Matching</li>
<li>Probing Inter-modality: Visual Parsing with Self-Attention for Vision-Language Pre-training</li>
<li>Modality-Agnostic Topology Aware Localization</li>
<li>Explainable Semantic Space by Grounding Language to Vision with Cross-Modal Contrastive Learning</li>
<li>Raw Nav-merge Seismic Data to Subsurface Properties with MLP based Multi-Modal Information Unscrambler</li>
<li>What Makes Multi-modal Learning Better than Single (Provably)</li>
</ol>
<h3 id="neurips-020">NeurIPS 020</h3>
<ol type="1">
<li>Labelling unlabelled videos from scratch with multi-modal self-supervision</li>
<li>Self-Supervised Learning by Cross-Modal Audio-Video Clustering</li>
<li>CodeCMR: Cross-Modal Retrieval For Function-Level Binary Source Code Matching</li>
<li>A Contour Stochastic Gradient Langevin Dynamics Algorithm for Simulations of Multi-modal Distributions</li>
<li>An implicit function learning approach for parametric modal regression</li>
<li>Removing Bias in Multi-modal Classifiers: Regularization by Maximizing Functional Entropies</li>
</ol>
<h3 id="aaai-22">AAAI 22</h3>
<ol type="1">
<li>Event-Image Fusion Stereo Using Cross-Modality Feature Propagation</li>
<li>Cross-Modal Mutual Learning for Audio-Visual Speech Recognition and Manipulation</li>
<li>Cross-Modal Federated Human Activity Recognition via Modality-Agnostic and Modality-Specific Representation Learning</li>
<li>Detecting Human-Object Interactions with Object-Guided Cross-Modal Calibrated Semantics</li>
<li>Show Your Faith: Cross-Modal Conﬁdence-Aware Network for Image-Text Matching</li>
<li>Modality-Adaptive Mixup and Invariant Decomposition for RGB-Infrared Person Re-identiﬁcation</li>
<li>MuMu: Cooperative Multitask Learning-Based Guided Multimodal Fusion</li>
<li>Cross-Modal Object Tracking: Modality-Aware Representations and A Uniﬁed Benchmark</li>
<li>You Only Infer Once: Cross-Modal Meta-Transfer for Referring Video Object Segmentation</li>
<li>Multi-Modal Perception Attention Network with Self-Supervised Learning for Audio-Visual Speaker Tracking</li>
<li>Visual Sound Localization in the Wild by Cross-Modal Interference Erasing</li>
<li>TVT: Three-Way Vision Transformer through Multi-Modal Hypersphere Learning for Zero-Shot Sketch-Based Image Retrieval</li>
<li>Interact, Embed, and EnlargE: Boosting Modality-Specific Representations for Multi-Modal Person Re-identification</li>
<li>MAGIC: Multimodal relAtional Graph adversarIal inferenCe for Diverse and Unpaired Text-Based Image Captioning</li>
<li>Promoting Single-Modal Optical Flow Network for Diverse Cross-Modal Flow Estimation</li>
<li>Event-Aware Multimodal Mobility Nowcasting</li>
<li>Online Enhanced Semantic Hashing: Towards Effective and Efficient Retrieval for Streaming Multi-Modal Data</li>
<li>AXM-Net: Implicit Cross-Modal Feature Alignment for Person Re-identiﬁcation</li>
<li>Monocular Camera-Based Point-Goal Navigation by Learning Depth Channel and Cross-Modality Pyramid Fusion</li>
<li>Multimodal Adversarially Learned Inference with Factorized Discriminators</li>
<li>Learning Aligned Cross-Modal Representation for Generalized Zero-Shot Classification</li>
<li>Regularized Modal Regression on Markov-Dependent Observations: A Theoretical Assessment</li>
<li>Multi-Head Modularization to Leverage Generalization Capability in Multi-Modal Networks</li>
<li>BM-NAS: Bilevel Multimodal Neural Architecture Search</li>
<li>Tailor Versatile Multi-Modal Learning for Multi-Label Emotion Recognition</li>
<li>Bi-CMR: Bidirectional Reinforcement Guided Hashing for Effective Cross-Modal Retrieval</li>
<li>Cross-Modal Coherence for Text-to-Image Retrieval</li>
<li>Nice Perfume. How Long Did You Marinate in It? Multimodal Sarcasm Explanation</li>
<li>Are Vision-Language Transformers Learning Multimodal Representations? A Probing Perspective</li>
<li>Hierarchical Cross-Modality Semantic Correlation Learning Model for Multimodal Summarization</li>
<li>UniMS: A Uniﬁed Framework for Multimodal Summarization with Knowledge Distillation</li>
<li>Evaluating Explainable AI on a Multi-Modal Medical Imaging Task: Can Existing Algorithms Fulfill Clinical Requirements?</li>
<li>Sentiment and Emotion-Aware Multi-Modal Complaint Identiﬁcation</li>
<li>D-vlog: Multimodal Vlog Dataset for Depression Detection</li>
<li>An End-to-End Traditional Chinese Medicine Constitution Assessment System Based on Multimodal Clinical Feature Representation and Fusion</li>
<li>ALLURE * : A Multi-Modal Guided Environment for Helping Children Learn to Solve a Rubik’s Cube with Automatic Solving and Interactive Explanations</li>
<li>A Multimodal Fusion-Based LNG Detection for Monitoring Energy Facilities (Student Abstract)</li>
<li>Using Multimodal Data and AI to Dynamically Map Flood Risk</li>
<li>College Student Retention Risk Analysis from Educational Database Using Multi-Task Multi-Modal Neural Fusion</li>
</ol>
<h3 id="aaai-21">AAAI 21</h3>
<ol type="1">
<li>Embracing Domain Differences in Fake News: Cross-domain Fake News Detection using Multi-modal Data</li>
<li>Dynamic Graph Representation Learning for Video Dialog via Multi-Modal Shufﬂed Transformers</li>
<li>SMIL: Multimodal Learning with Severely Missing Modality</li>
<li>CHEF: Cross-Modal Hierarchical Embeddings for Food Domain Retrieval</li>
<li>Dual Adversarial Graph Neural Networks for Multi-label Cross-modal Retrieval</li>
<li>Deep Probabilistic Imaging: Uncertainty Quantiﬁcation and Multi-modal Solution Characterization for Computational Imaging</li>
<li>Efficient Object-Level Visual Context Modeling for Multimodal Machine Translation: Masking Irrelevant Objects Helps Grounding</li>
<li>Conﬁdence-aware Non-repetitive Multimodal Transformers for TextCaps</li>
<li>Amodal Segmentation Based on Visible Region Segmentation and Shape Prior</li>
<li>Multimodal Fusion via Teacher-Student Network for Indoor Action Recognition</li>
<li>Demodalizing Face Recognition with Synthetic Samples</li>
<li>Joint Color-irrelevant Consistency Learning and Identity-aware Modality Adaptation for Visible-infrared Cross Modality Person Re-identiﬁcation</li>
<li>Robust Multi-Modality Person Re-identiﬁcation</li>
<li>Deep Graph-neighbor Coherence Preserving Network for Unsupervised Cross-modal Hashing</li>
<li>Learning Intuitive Physics with Multimodal Generative Models</li>
<li>VMLoc: Variational Fusion For Learning-Based Multimodal Camera Localization</li>
<li>Noise Estimation Using Density Estimation for Self-Supervised Multimodal Learning</li>
<li>Deep Mutual Information Maximin for Cross-Modal Clustering</li>
<li>MUFASA: Multimodal Fusion Architecture Search for Electronic Health Records</li>
<li>Enhanced Audio Tagging via Multi- to Single-Modal Teacher-Student Mutual Learning</li>
<li>Learning Modality-Speciﬁc Representations with Self-Supervised Multi-Task Learning for Multimodal Sentiment Analysis</li>
<li>Theoretical Analyses of Multi-Objective Evolutionary Algorithms on Multi-Modal Objectives</li>
<li>Humor Knowledge Enriched Transformer for Understanding Multimodal Humor</li>
<li>Audio-Oriented Multimodal Machine Comprehension via Dynamic Inter- and Intra-modality Attention</li>
<li>MELINDA: A Multimodal Dataset for Biomedical Experiment Method Classification</li>
<li>Multi-modal Multi-label Emotion Recognition with Heterogeneous Hierarchical Message Passing</li>
<li>LAMS: A Location-aware Approach for Multimodal Summarization (Student Abstract)</li>
<li>Fashion Focus: Multi-modal Retrieval System for Video Commodity Localization in E-commerce</li>
<li>Data-Driven Multimodal Patrol Planning for Anti-poaching</li>
<li>Screening for Depressed Individuals by Using Multimodal Social Media Data</li>
<li>Multi-modal User Intent Classiﬁcation Under the Scenario of Smart Factory (Student Abstract)</li>
</ol>
<h3 id="aaai-20">AAAI 20</h3>
<ol type="1">
<li>Infrared-Visible Cross-Modal Person Re-Identiﬁcation with an X Modality</li>
<li>MANYMODAL QA: Modality Disambiguation and QA over Diverse Inputs</li>
<li>Privacy Enhanced Multimodal Neural Representations for Emotion Recognition</li>
<li>Modality-Balanced Models for Visual Dialogue</li>
<li>Aspect-Aware Multimodal Summarization for Chinese E-Commerce Products</li>
<li>Semi-Supervised Multi-Modal Learning with Balanced Spectral Decomposition</li>
<li>Modality to Modality Translation: An Adversarial Representation Learning and Graph Fusion Network for Multimodal Fusion</li>
<li>Cross-Modal Attention Network for Temporal Inconsistent Audio-Visual Event Localization</li>
<li>Crisis-DIAS: Towards Multimodal Damage Analysis - Deployment, Challenges and Assessment</li>
<li>Towards Cross-Modality Medical Image Segmentation with Online Mutual Knowledge Distillation</li>
<li>Learning Multi-Modal Biomarker Representations via Globally Aligned Longitudinal Enrichments</li>
<li>Urban2Vec: Incorporating Street View Imagery and POIs for Multi-Modal Urban Neighborhood Embedding</li>
<li>M3ER: Multiplicative Multimodal Emotion Recognition using Facial, Textual, and Speech Cues</li>
<li>Cross-Modal Subspace Clustering via Deep Canonical Correlation Analysis</li>
<li>Learning Relationships between Text, Audio, and Video via Deep Canonical Correlation for Multimodal Language Analysis</li>
<li>Visual Agreement Regularized Training for Multi-Modal Machine Translation</li>
<li>Learning Long- and Short-Term User Literal-Preference with Multimodal Hierarchical Transformer Network for Personalized Image Caption</li>
<li>Multimodal Summarization with Guidance of Multimodal Reference</li>
<li>Factorized Inference in Deep Markov Models for Incomplete Multimodal Time Series</li>
<li>MULE: Multimodal Universal Language Embedding</li>
<li>Unicoder-VL: A Universal Encoder for Vision and Language by Cross-Modal Pre-Training</li>
<li>Attention-Based Multi-Modal Fusion Network for Semantic Scene Completion</li>
<li>Multimodal Structure-Consistent Image-to-Image Translation</li>
<li>Learning Cross-Aligned Latent Embeddings for Zero-Shot Cross-Modal Retrieval</li>
<li>Learning Cross-Modal Context Graph for Visual Grounding</li>
<li>Multimodal Interaction-Aware Trajectory Prediction in Crowded Space</li>
<li>Cross-Modality Paired-Images Generation for RGB-Infrared Person Re-Identification</li>
<li>Adaptive Cross-Modal Embeddings for Image-Text Alignment</li>
<li>Mining on Heterogeneous Manifolds for Zero-Shot Cross-Modal Image Retrieval</li>
<li>Cross-Modality Attention with Semantic Graph Embedding for Multi-Label Classification</li>
<li>Adaptive Unimodal Cost Volume Filtering for Deep Stereo Matching</li>
<li>Diana’s World: A Situated Multimodal Interactive Agent</li>
<li>Interpreting Multimodal Machine Learning Models Trained for Emotion Recognition to Address Robustness and Privacy Concerns</li>
<li>Trimodal Attention Module for Multimodal Sentiment Analysis (Student Abstract)</li>
<li>SpotFake+: A Multimodal Framework for Fake News Detection via Transfer Learning (Student Abstract)</li>
</ol>
<h3 id="sigir-22">SIGIR 22</h3>
<ol type="1">
<li>CRET: Cross-Modal Retrieval Transformer for Efficient Text-Video Retrieval</li>
<li>Multimodal Disentanglement Variational AutoEncoders for Zero-Shot Cross-Modal Retrieval</li>
<li>Bit-aware Semantic Transformer Hashing for Multi-modal Retrieval</li>
<li>V2P: Vision-to-Prompt based Multi-Modal Product Summary Generation</li>
<li>Progressive Learning for Image Retrieval with Hybrid-Modality Queries</li>
<li>Tag-assisted Multimodal Sentiment Analysis under Uncertain Missing Modalities</li>
<li>A Multitask Framework for Sentiment, Emotion and Sarcasm aware Cyberbullying Detection from Multi-modal Code-Mixed Memes</li>
<li>Multi-modal Graph Contrastive Learning for Micro-video Recommendation</li>
<li>Cross-Probe BERT for Fast Cross-Modal Search</li>
<li>MM-Rec: Visiolinguistic Model Empowered Multimodal News Recommendation</li>
<li>Modality-Balanced Embedding for Video Retrieval</li>
<li>An Efficient Fusion Mechanism for Multimodal Low-resource Setting</li>
<li>Next Point-of-Interest Recommendation with Auto-Correlation Enhanced Multi-Modal Transformer Network</li>
<li>MET-Meme: a Multimodal Meme Dataset Rich in Metaphors</li>
<li>MuMiN: A Large-Scale Multilingual Multimodal Fact-Checked Misinformation Social Network Dataset</li>
<li>Golden Retriever: A Real-Time Multi-Modal Text-Image Retrieval System with the Ability to Focus</li>
<li>An Intelligent Advertisement Short Video Production System via Multi-Modal Retrieval</li>
</ol>
<h3 id="sigir-21">SIGIR 21</h3>
<ol type="1">
<li>DepressionNet: A Novel Summarization Boosted Deep Framework for Depression Detection on Social Media</li>
<li>Hierarchical Multi-modal Contextual Attention Network for Fake News Detection</li>
<li>Hybrid Fusion with Intra- and Cross-Modality Attention for Image-Recipe Retrieval</li>
<li>Multimodal Activation: Awakening Dialog Robots without Wake Words</li>
<li>Privacy Protection in Deep Multi-modal Retrieval</li>
<li>MMConv: An Environment for Multimodal Conversational Search across Multiple Domains</li>
<li>Multi-Modal Supplementary-Complementary Summarization using Multi-Objective Optimization</li>
<li>Dynamic Modality Interaction Modeling for Image-Text Retrieval</li>
<li>Hierarchical Cross-Modal Graph Consistency Learning for Video-Text Retrieval</li>
<li>PAN: Prototype-based Adaptive Network for Robust Cross-modal Retrieval</li>
<li>Heterogeneous Attention Network for Effective and Efficient Cross-modal Retrieval</li>
<li>Towards Multi-Modal Conversational Information Seeking</li>
<li>FedCMR: Federated Cross-Modal Retrieval</li>
<li>Cross-Graph Attention Enhanced Multi-Modal Correlation Learning for Fine-Grained Image-Text Retrieval</li>
<li>Deep Music Retrieval for Fine-Grained Videos by Exploiting Cross-Modal-Encoded Voice-Overs</li>
<li>AliMe Avatar: Multi-modal Content Production and Presentation for Live-streaming E-commerce</li>
<li>QuTI! Quantifying Text-Image Consistency in Multimodal Documents</li>
</ol>
<h3 id="sigir-20">SIGIR 20</h3>
<ol type="1">
<li>Fashion Compatibility Modeling through a Multi-modal Try-on-guided Scheme</li>
<li>Tree-Augmented Cross-Modal Encoding for Complex-Query Video Retrieval</li>
<li>Nonlinear Robust Discrete Hashing for Cross-Modal Retrieval</li>
<li>Joint-modal Distribution-based Similarity Hashing for Large-scale Unsupervised Deep Cross-modal Retrieval</li>
<li>Web Table Retrieval using Multimodal Deep Learning</li>
<li>Correlated Features Synthesis and Alignment for Zero-shot Cross-modal Retrieval</li>
<li>MGNN: A Multimodal Graph Neural Network for Predicting the Survival of Cancer Patients</li>
<li>Multi-Modal Summary Generation using Multi-Objective Optimization</li>
<li>Multi-Level Multimodal Transformer Network for Multimodal Recipe Comprehension</li>
<li>MHM: Multi-modal Clinical Data based Hierarchical Multi-label Diagnosis Prediction</li>
<li>FashionBERT: Text and Image Matching with Adaptive Loss for Cross-modal Retrieval</li>
</ol>
<h3 id="cvpr-22">CVPR 22</h3>
<ol type="1">
<li>Cross-modal Map Learning for Vision and Language Navigation</li>
<li>PhoCaL: A Multi-Modal Dataset for Category-Level Object Pose Estimation with Photometrically Challenging Objects</li>
<li>Everything at Once – Multi-modal Fusion Transformer for Video Retrieval</li>
<li>CrossPoint: Self-Supervised Cross-Modal Contrastive Learning for 3D Point Cloud Understanding</li>
<li>Versatile Multi-Modal Pre-Training for Human-Centric Perception</li>
<li>Lite-MDETR: A Lightweight Multi-Modal Detector</li>
<li>Cross Modal Retrieval with Querybank Normalisation</li>
<li>Modeling Motion with Multi-Modal Features for Text-Based Video Segmentation</li>
<li>Tencent-MVSE: A Large-Scale Benchmark Dataset for Multi-Modal Video Similarity Evaluation</li>
<li>Open-Vocabulary Instance Segmentation via Robust Cross-Modal Pseudo-Labeling</li>
<li>RFNet: Unsupervised Network for Mutually Reinforcing Multi-modal Image Registration and Fusion</li>
<li>EI-CLIP: Entity-aware Interventional Contrastive Learning for E-commerce Cross-modal Retrieval</li>
<li>Learning Hierarchical Cross-Modal Association for Co-Speech Gesture Generation</li>
<li>X-Trans2Cap: Cross-Modal Knowledge Transfer using Transformer for 3D Dense Captioning</li>
<li>MM-TTA: Multi-Modal Test-Time Adaptation for 3D Semantic Segmentation</li>
<li>Interact before Align: Leveraging Cross-Modal Knowledge for Domain Adaptive Action Recognition</li>
<li>Cross-Modal Transferable Adversarial Attacks from Images to Videos</li>
<li>Open-Domain, Content-based, Multi-modal Fact-checking of Out-of-Context Images via Online Resources</li>
<li>Cross-Modal Perceptionist: Can Face Geometry be Gleaned from Voices?</li>
<li>Cross-modal Representation Learning for Zero-shot Action Recognition</li>
<li>Audio-visual Generalised Zero-shot Learning with Cross-modal Attention and Language</li>
<li>Robust Cross-Modal Representation Learning with Progressive Self-Distillation</li>
<li>Multi-modal Alignment using Representation Codebook</li>
<li>Wnet: Audio-Guided Video Object Segmentation via Wavelet-Based Cross-Modal Denoising Networkss</li>
<li>Text2Pos: Text-to-Point-Cloud Cross-Modal Localization</li>
<li>Reading to Listen at the Cocktail Party: Multi-Modal Speech Separation</li>
<li>Cross-modal Background Suppression for Audio-Visual Event Localization</li>
<li>Mutual Quantization for Cross-Modal Search with Noisy Labels</li>
<li>Learning Modal-Invariant and Temporal-Memory for Video-based Visible-Infrared Person Re-Identification</li>
<li>Multi-modal Extreme Classification</li>
<li>COTS: Collaborative Two-Stream Vision-Language Pre-Training Model for Cross-Modal Retrieval</li>
<li>ViSTA: Vision and Scene Text Aggregation for Cross-Modal Retrieval</li>
<li>CroMo: Cross-Modal Learning for Monocular Depth Estimation</li>
<li>CAT-Det: Contrastively Augmented Transformer for Multi-modal 3D Object Detection</li>
<li>Beyond a Pre-Trained Object Detector: Cross-Modal Textual and Visual Context for Image Captioning</li>
<li>X-Pool: Cross-Modal Language-Video Attention for Text-Video Retrieval</li>
<li>UMT: Unified Multi-modal Transformers for Joint Video Moment Retrieval and Highlight Detection</li>
<li>Multi-Modal Dynamic Graph Transformer for Visual Grounding</li>
<li>DeepFusion: Lidar-Camera Deep Fusion for Multi-Modal 3D Object Detection</li>
<li>M3L: Language-based Video Editing via Multi-Modal Multi-Level Transformers</li>
<li>Generalizable Cross-modality Medical Image Segmentation via Style Augmentation and Dual Normalization</li>
<li>Weakly Paired Associative Learning for Sound and Image Representations via Bimodal Associative Memory</li>
<li>ADAPT: Vision-Language Navigation with Modality-Aligned Action Prompts</li>
<li>Dual-Key Multimodal Backdoors for Visual Question Answering</li>
<li>CrossLoc: Scalable Aerial Localization Assisted by Multimodal Synthetic Data</li>
<li>Learning based Multi-modality Image and Video Compression</li>
<li>Modality-Agnostic Learning for Radar-Lidar Fusion in Vehicle Detection</li>
<li>WALT: Watch And Learn 2D amodal representation from Time-lapse imagery</li>
<li>Towards Multimodal Depth Estimation from Light Fields</li>
<li>End-to-End Referring Video Object Segmentation with Multimodal Transformers</li>
<li>FMCNet: Feature-Level Modality Compensation for Visible-Infrared Person Re-Identification</li>
<li>Show Me What and Tell Me How: Video Synthesis via Multimodal Conditioning</li>
<li>Amodal Segmentation through Out-of-Task and Out-of-Distribution Generalization with a Bayesian Model</li>
<li>Unimodal-Concentrated Loss: Fully Adaptive Label Distribution Learning for Ordinal Regression</li>
<li>Expanding Large Pre-trained Unimodal Models with Multimodal Information Injection for Image-Text Multimodal Classification</li>
<li>End-to-end Generative Pretraining for Multimodal Video Captioning</li>
<li>XMP-Font: Self-Supervised Cross-Modality Pre-training for Few-Shot Font Generation</li>
<li>Multimodal Dynamics: Dynamical Fusion for Trustworthy Multimodal Classification</li>
<li>The Auto Arborist Dataset: A Large-Scale Benchmark for Multiview Urban Forest Monitoring Under Domain Shift</li>
<li>MNSRNet: Multimodal Transformer Network for 3D Surface Super-Resolution</li>
<li>OMNIVORE: A Single Model for Many Visual Modalities</li>
<li>Motron: Multimodal Probabilistic Human Motion Forecasting</li>
<li>Amodal Panoptic Segmentation</li>
<li>Multimodal Colored Point Cloud to Image Alignment</li>
<li>Boosting 3D Object Detection by Simulating Multimodality on Point Clouds</li>
<li>ContIG: Self-supervised Multimodal Contrastive Learning for Medical Imaging with Genetics</li>
<li>Egocentric Scene Understanding via Multimodal Spatial Rectifier</li>
<li>Are Multimodal Transformers Robust to Missing Modality?</li>
<li>A Simple Multi-Modality Transfer Learning Baseline for Sign Language Translation</li>
<li>Multimodal Material Segmentation</li>
<li>Target-aware Dual Adversarial Learning and a Multi-scenario Multi-Modality Benchmark to Fuse Infrared and Visible for Object Detection</li>
<li>Balanced Multimodal Learning via On-the-fly Gradient Modulation</li>
<li>Multimodal Token Fusion for Vision Transformers</li>
<li>Learnable Irrelevant Modality Dropout for Multimodal Action Recognition on Modality-Specific Annotated Videos</li>
<li>XYLayoutLM: Towards Layout-Aware Multimodal Networks For Visually-Rich Document Understanding</li>
<li>STCrowd: A Multimodal Dataset for Pedestrian Perception in Crowded Scenes</li>
<li>3MASSIV: Multilingual, Multimodal and Multi-Aspect dataset of Social Media Short Videos</li>
<li>M5Product: Self-harmonized Contrastive Learning for E-commercial Multi-modal Pretraining. CVPR 22. <a target="_blank" rel="noopener" href="https://xiaodongsuper.github.io/M5Product_dataset/">代码</a>
<ul>
<li>作者创建了一个包括了音频、图像、属性、视频等信息的大规模多模态数据集。虽然从全文来看没有声明是一个MMKG，但是如果从广义的角度看，也是属于KG</li>
</ul></li>
<li>VisualHow: Multimodal Problem Solving</li>
<li>WebQA: Multihop and Multimodal QA</li>
<li>Query and Attention Augmentation for Knowledge-Based Explainable Reasoning</li>
<li>Open-Vocabulary One-Stage Detection with Hierarchical Visual-Language Knowledge Distillation</li>
</ol>
<h3 id="cvpr-21">CVPR 21</h3>
<ol type="1">
<li>Shared Cross-Modal Trajectory Prediction for Autonomous Driving</li>
<li>Robust Multimodal Vehicle Detection in Foggy Weather Using Complementary Lidar and Radar Signals</li>
<li>EvDistill: Asynchronous Events to End-task Learning via Bidirectional Reconstruction-guided Cross-modal Knowledge Distillation</li>
<li>Cross-Modal Contrastive Learning for Text-to-Image Generation</li>
<li>Looking into Your Speech: Learning Cross-modal Affinity for Audio-visual Speech Separation</li>
<li>Deep RGB-D Saliency Detection with Depth-Sensitive Attention and Automatic Multi-Modal Fusion</li>
<li>Farewell to Mutual Information: Variational Distillation for Cross-Modal Person Re-Identification</li>
<li>Multi-Modal Relational Graph for Cross-Modal Video Moment Retrievals</li>
<li>Progressive Modality Reinforcement for Human Multimodal Emotion Recognition from Unaligned Multimodal Sequences</li>
<li>ABMDRNet: Adaptive-weighted Bi-directional Modality Difference Reduction Network for RGB-T Semantic Segmentation</li>
<li>How2Sign: A Large-scale Multimodal Dataset for Continuous American Sign Language</li>
<li>Cross-Modal Center Loss for 3D Cross-Modal Retrieval</li>
<li>Defending Multimodal Fusion Models against Single-Source Adversaries</li>
<li>StEP: Style-based Encoder Pre-training for Multi-modal Image Synthesiss</li>
<li>M3P: Learning Universal Representations via Multitask Multilingual Multimodal Pre-training</li>
<li>UC2 : Universal Cross-lingual Cross-modal Vision-and-Language Pre-training</li>
<li>Discover Cross-Modality Nuances for Visible-Infrared Person Re-Identification</li>
<li>Cross-Modal Collaborative Representation Learning and a Large-Scale RGBT Benchmark for Crowd Counting</li>
<li>Learning Cross-Modal Retrieval with Noisy Labels</li>
<li>Can audio-visual integration strengthen robustness under multimodal attacks?</li>
<li>Single Pair Cross-Modality Super Resolution</li>
<li>Multimodal Contrastive Training for Visual Representation Learning</li>
<li>VX2TEXT: End-to-End Learning of Video-Based Text Generation From Multimodal Inputs</li>
<li>Multi-Modal Fusion Transformer for End-to-End Autonomous Driving</li>
<li>Multimodal Motion Prediction with Stacked Transformers</li>
<li>Cross Modal Focal Loss for RGBD Face Anti-Spoofing</li>
<li>Probabilistic Embeddings for Cross-Modal Retrieval</li>
<li>There is More than Meets the Eye: Self-Supervised Multi-Object Detection and Tracking with Sound by Distilling Multimodal Knowledge</li>
<li>Audio-Visual Instance Discrimination with Cross-Modal Agreement</li>
<li>Learning from the Master: Distilling Cross-modal Advanced Knowledge for Lip Reading</li>
<li>LaPred: Lane-Aware Prediction of Multi-Modal Future Trajectories of Dynamic Agents</li>
<li>Adaptive Cross-Modal Prototypes for Cross-Domain Visual-Language Retrieval</li>
<li>Revamping Cross-Modal Recipe Retrieval with Hierarchical Transformers and Self-supervised Learning</li>
<li>VISUALVOICE: Audio-Visual Speech Separation with Cross-Modal Consistency</li>
<li>Deep Lucas-Kanade Homography for Multimodal Image Alignment</li>
<li>Distilling Audio-Visual Knowledge by Compositional Contrastive Learning</li>
<li>Improving Weakly Supervised Visual Grounding by Contrastive Knowledge Distillation</li>
<li>Amalgamating Knowledge from Heterogeneous Graph Neural Networks</li>
</ol>
<h3 id="cvpr-20">CVPR 20</h3>
<ol type="1">
<li>Multi-Modal Domain Adaptation for Fine-Grained Action Recognition</li>
<li>Creating Something from Nothing: Unsupervised Knowledge Distillation for Cross-Modal Hashing</li>
<li>Multimodal Future Localization and Emergence Prediction for Objects in Egocentric View With a Reachability Prior</li>
<li>Cross-modal Deep Face Normals with Deactivable Skip Connections</li>
<li>Monocular Real-time Hand Shape and Motion Capture using Multi-modal Data</li>
<li>Semantically Multi-modal Image Synthesis</li>
<li>Knowledge as Priors: Cross-Modal Knowledge Generalization for Datasets without Superior Knowledge</li>
<li>Cross-Modal Pattern-Propagation for RGB-T Tracking</li>
<li>Iterative Answer Prediction with Pointer-Augmented Multimodal Transformers for TextVQA</li>
<li>Modality Shifting Attention Network for Multi-modal Video Question Answering</li>
<li>A Local-to-Global Approach to Multi-modal Movie Scene Segmentation</li>
<li>Hi-CMD: Hierarchical Cross-Modality Disentanglement for Visible-Infrared Person Re-Identification</li>
<li>Speech2Action: Cross-modal Supervision for Action Recognition</li>
<li>Solving Mixed-modal Jigsaw Puzzle for Fine-Grained Sketch-Based Image Retrieval</li>
<li>Referring Image Segmentation via Cross-Modal Progressive Comprehension</li>
<li>Cross-Modal Cross-Domain Moment Alignment Network for Person Search</li>
<li>Vision-Dialog Navigation by Exploring Cross-modal Memory</li>
<li>A Real-Time Cross-modality Correlation Filtering Method for Referring Expression Comprehension</li>
<li>Multi-Modality Cross Attention Network for Image and Sentence Matchings</li>
<li>nuScenes: A multimodal dataset for autonomous driving</li>
<li>Seeing Through Fog Without Seeing Fog: Deep Multimodal Sensor Fusion in Unseen Adverse Weather</li>
<li>End-to-End Adversarial-Attention Network for Multi-Modal Clustering</li>
<li>xMUDA: Cross-Modal Unsupervised Domain Adaptation for 3D Semantic Segmentation</li>
<li>IMRAM: Iterative Matching with Recurrent Attention Memory for Cross-Modal Image-Text Retrieval∗</li>
<li>What Makes Training Multi-modal Classification Networks Hard?</li>
<li>Multi-Modal Graph Neural Network for Joint Reasoning on Vision and Scene Texts</li>
<li>Universal Weighting Metric Learning for Cross-Modal Matching</li>
<li>MMTM: Multimodal Transfer Module for CNN Fusion</li>
<li>Cross-Modality Person Re-Identification With Shared-Specific Feature Transfer</li>
<li>Unsupervised Multi-Modal Image Registration via Geometry Preserving Image-to-Image Translation</li>
<li>CoverNet: Multimodal Behavior Prediction using Trajectory Sets</li>
<li>EmotiCon: Context-Aware Multimodal Emotion Recognition using Frege’s Principle</li>
<li>Discriminative Multi-Modality Speech Recognition</li>
<li>MCEN: Bridging Cross-Modal Gap between Cooking Recipes and Dish Images with Latent Variable Model</li>
<li>Hypergraph Attention Networks for Multimodal Learning</li>
<li>Multimodal Categorization of Crisis Events in Social Media</li>
<li>Transform and Tell: Entity-Aware News Image Captioning</li>
</ol>
<h3 id="kdd-22">KDD 22</h3>
<ol type="1">
<li>Graph-Flashback Network for Next Location Recommendation</li>
<li>ERNIE-GeoL: A Geography-and-Language Pre-trained Model and its Applications in Baidu Maps</li>
<li>Graph Neural Networks for Multimodal Single-Cell Data Integration</li>
<li>External Knowledge Infusion for Tabular Pre-training Models with Dual-adapters</li>
</ol>
<h3 id="kdd-21">KDD 21</h3>
<ol type="1">
<li>Web-Scale Generic Object Detection at Microsoft Bing</li>
<li>Cross-Network Learning with Partially Aligned Graph Convolutional Networks</li>
<li>Triplet Attention: Rethinking the similarity in Transformers</li>
</ol>
<h3 id="iccv">ICCV</h3>
<ol type="1">
<li>VLG-Net: Video-Language Graph Matching Network for Video Grounding. ICCV 21</li>
<li>Visual-Textual Attentive Semantic Consistency for Medical Report Generation. ICCV 21</li>
<li>Public Life in Public Space (PLPS): A multi-task, multi-group video dataset for public life research. ICCV 21</li>
<li>Self-Motivated Communication Agent for Real-World Vision-Dialog Navigation. ICCV 21</li>
<li>VrR-VG: Refocusing Visually-Relevant Relationships. ICCV 19</li>
<li>Concept Generalization in Visual Representation Learning. ICCV 21</li>
<li>DocFormer: End-to-End Transformer for Document Understanding. ICCV 21</li>
<li>Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models. ICCV 15</li>
<li>Virtual Multi-Modality Self-Supervised Foreground Matting for Human-Object Interaction. ICCV 21</li>
<li>Smile, Be Happy :) Emoji Embedding for Visual Sentiment Analysis. ICCV Workshop 19</li>
<li>Explain Me the Painting: Multi-Topic Knowledgeable Art Description Generation. ICCV 19</li>
</ol>
<h3 id="eccv">ECCV</h3>
<ol type="1">
<li>VisualCOMET: Reasoning About the Dynamic Context of a Still Image. ECCV 20</li>
<li>Fashionpedia: Ontology, Segmentation, and an Attribute Localization Dataset. ECCV 20</li>
<li>Learning Type-Aware Embeddings for Fashion Compatibility. ECCV 18</li>
<li>A Dataset and Baselines for Visual Question Answering on Art. ECCV 20 Workshop</li>
<li>Learning to Scale Multilingual Representations for Vision-Language Tasks. ECCV 20</li>
<li>MaxViT: Multi-Axis Vision Transformer. ECCV 22</li>
</ol>
<h3 id="cikm">CIKM</h3>
<ol type="1">
<li>SciClops: Detecting and Contextualizing Scientific Claims for Assisting Manual Fact-Checking. CIKM 21</li>
<li>WebKE: Knowledge Extraction from Semi-structured Web with Pre-trained Markup Language Model. CIKM 21</li>
<li>MLM: A Benchmark Dataset for Multitask Learning with Multiple Languages and Modalities. CIKM 20</li>
<li>Multi-modal Dictionary BERT for Cross-modal Video Search in Baidu Advertising. CIKM 20</li>
<li>IMAS++: An Intelligent Medical Analysis System Enhanced with Deep Graph Neural Networks. CIKM 21</li>
<li>Recipe Representation Learning with Networks. CIKM 21</li>
<li>Student Can Also be a Good Teacher: Extracting Knowledge from Vision-and-Language Model for Cross-Modal Retrieval. CIKM 21</li>
<li>Improving Chinese Character Representation with Formation Graph Attention Network. CIKM 21</li>
<li>VidLife: A Dataset for Life Event Extraction from Videos. CIKM 21</li>
<li>Learning Chinese Word Embeddings from Stroke, Structure and Pinyin of Characters. CIKM 19</li>
</ol>
<h3 id="wsdm">WSDM</h3>
<ol type="1">
<li>Representation Interpretation with Spatial Encoding and Multimodal Analytics. WSDM 19</li>
<li>Beyond Statistical Relations: Integrating Knowledge Relations into Style Correlations for Multi-Label Music Style Classification. WSDM 20</li>
<li>Speaker and Time-aware Joint Contextual Learning for Dialogue-act Classification in Counselling Conversations. WSDM 22.</li>
<li>VISIR: Visual and Semantic Image Label Refinement. WSDM 18</li>
<li>Product Knowledge Graph Embedding for E-commerce. WSDM 20</li>
</ol>
<h3 id="icmr">ICMR</h3>
<ol type="1">
<li>Know Yourself and Know Others: Efficient Common Representation Learning for Few-shot Cross-modal Retrieval. ICMR 21</li>
<li>Personal Knowledge Base Construction from Multimodal Data. ICMR 21</li>
<li>Image Emotion Distribution Learning with Graph Convolutional Network. ICMR 19</li>
<li>HSGMP: Heterogeneous Scene Graph Message Passing for Cross-modal Retrieval. ICMR 21</li>
<li>HLVU : A New Challenge to Test Deep Understanding of Movies the Way Humans do. ICMR 20</li>
<li>Context-Aware Embeddings for Automatic Art Analysis. ICMR 19</li>
<li>Semantic Gated Network for Efficient News Representation. ICMR 20</li>
<li>Ten Questions in Lifelog Mining and Information Recall. ICMR 21</li>
<li>Video2Subtitle: Matching Weakly-Synchronized Sequences via Dynamic Temporal Alignment. ICMR 22</li>
<li>Improve Image Captioning by Modeling Dynamic Scene Graph Extension. ICMR 22</li>
<li>SenseMood: Depression Detection on Social Media. ICMR 20</li>
<li>Incorporating Semantic Knowledge for Visual Lifelog Activity Recognition. ICMR 20</li>
</ol>
<h3 id="coling">COLING</h3>
<ol type="1">
<li>MEISD: A Multimodal Multi-Label Emotion, Intensity and Sentiment Dialogue Dataset for Emotion Recognition and Sentiment Analysis in Conversations</li>
<li>Are Visual-Linguistic Models Commonsense Knowledge Bases? COLING 22</li>
<li>Extracting a Knowledge Base of COVID-19 Events from Social Media. COLING 22</li>
<li>Read Extensively, Focus Smartly: A Cross-document Semantic Enhancement Method for Visual Documents NER. COLING 22</li>
<li>Decoupling Mixture-of-Graphs: Unseen Relational Learning for Knowledge Graph Completion by Fusing Ontology and Textual Experts. COLING 22</li>
<li>A Relation Extraction Dataset for Knowledge Extraction from Web Tables. COLING 22</li>
<li>Virtual Knowledge Graph Construction for Zero-Shot Domain-Specific Document Retrieval. COLING 22</li>
<li>KC-ISA: An Implicit Sentiment Analysis Model Combining Knowledge Enhancement and Context Features. COLING 22</li>
<li>Learning from Adjective-Noun Pairs: A Knowledge-enhanced Framework for Target-Oriented Multimodal Sentiment Classification. COLING 22</li>
<li>Extracting a Knowledge Base of COVID-19 Events from Social Media. COLING 22</li>
<li>Mind the Gap! Injecting Commonsense Knowledge for Abstractive Dialogue Summarization. CLOING 22</li>
<li>Multilingual and Multimodal Topic Modelling with Pretrained Embeddings. COLING 22</li>
<li>Knowledge-injected Prompt Tuning for Event Detection. COLING 22</li>
<li>Section-Aware Commonsense Knowledge-Grounded Dialogue Generation with Pre-trained Language Model. COLING 22</li>
<li>Enhancing Clinical BERT Embedding using a Biomedical Knowledge Base. COLING 20]</li>
<li></li>
</ol>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>Liu Xiyang
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://liuxiyang641.github.io/collection/MMML-survey-list/" title="MMML-survey-list">https://liuxiyang641.github.io/collection/MMML-survey-list/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/Collection/" rel="tag"><i class="fa fa-tag"></i> Collection</a>
              <a href="/tags/Reading-list/" rel="tag"><i class="fa fa-tag"></i> Reading-list</a>
              <a href="/tags/MMKG/" rel="tag"><i class="fa fa-tag"></i> MMKG</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/ml/Attention-survey-2021/" rel="prev" title="Attention-survey-2021">
                  <i class="fa fa-chevron-left"></i> Attention-survey-2021
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/collection/TemporalKG-survey-list/" rel="next" title="TemporalKG-survey-list">
                  TemporalKG-survey-list <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments gitalk-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-flag"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Liu Xiyang</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">375k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">5:41</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="//cdn.jsdelivr.net/npm/@next-theme/pjax@0.5.0/pjax.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/hexo-theme-next@8.12.2/source/js/comments.min.js"></script><script src="https://cdn.jsdelivr.net/npm/hexo-theme-next@8.12.2/source/js/utils.min.js"></script><script src="https://cdn.jsdelivr.net/npm/hexo-theme-next@8.12.2/source/js/motion.min.js"></script><script src="https://cdn.jsdelivr.net/npm/hexo-theme-next@8.12.2/source/js/next-boot.min.js"></script><script src="https://cdn.jsdelivr.net/npm/hexo-theme-next@8.12.2/source/js/pjax.min.js"></script>

  
<script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/hexo-theme-next@8.12.2/source/js/third-party/search/local-search.min.js"></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdn.jsdelivr.net/npm/pdfobject@2.2.8/pdfobject.min.js","integrity":"sha256-tu9j5pBilBQrWSDePOOajCUdz6hWsid/lBNzK4KgEPM="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="https://cdn.jsdelivr.net/npm/hexo-theme-next@8.12.2/source/js/third-party/tags/pdf.min.js"></script>


  <script src="https://cdn.jsdelivr.net/npm/hexo-theme-next@8.12.2/source/js/third-party/fancybox.min.js"></script>


  
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"ams","js":{"url":"//cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"}}</script>
<script src="https://cdn.jsdelivr.net/npm/hexo-theme-next@8.12.2/source/js/third-party/math/mathjax.min.js"></script>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous">

<script class="next-config" data-name="gitalk" type="application/json">{"enable":true,"github_id":"liuxiyang641","repo":"liuxiyang_blog_comment","client_id":"b800b344e096846a4608","client_secret":"45ac194feea7e642c29f8e13180184cc98afb3e6","admin_user":"liuxiyang641","distraction_free_mode":true,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token","language":"zh-CN","js":{"url":"https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.js","integrity":"sha256-Pmj85ojLaPOWwRtlMJwmezB/Qg8BzvJp5eTzvXaYAfA="},"path_md5":"91c21ab6ba3b7dfb4f0f239ddbdd3713"}</script>
<script src="https://cdn.jsdelivr.net/npm/hexo-theme-next@8.12.2/source/js/third-party/comments/gitalk.min.js"></script>
<div class="moon-menu">
  <div class="moon-menu-items">
    
    <div id="moon-menu-item-back2bottom" class="moon-menu-item">
      <i class='fas fa-chevron-down'></i>    </div>
    
    <div id="moon-menu-item-back2top" class="moon-menu-item">
      <i class='fas fa-chevron-up'></i>    </div>
    
  </div>
  <div class="moon-menu-button">
    <svg class="moon-menu-bg">
      <circle class="moon-menu-cricle" cx="50%" cy="50%" r="44%"></circle>
      <circle class="moon-menu-border" cx="50%" cy="50%" r="48%"></circle>
    </svg>
    <div class="moon-menu-content">
      <div class="moon-menu-icon"><i class='fas fa-ellipsis-v'></i></div>
      <div class="moon-menu-text"></div>
    </div>
  </div>
</div><script src="/js/injector.js"></script>
</body>
</html>
