<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.0">

<link rel="preconnect" href="https://fonts.googleapis.com" crossorigin>
<link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin>
  <link rel="apple-touch-icon" sizes="180x180" href="/images/lxy-apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/lxy-favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/lxy-favicon-16x16.png">
  <link rel="mask-icon" href="/images/lxy-favicon.ico" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" integrity="sha256-DfWjNxDkM94fVBWx1H5BMMp0Zq7luBlV8QRcSES7s+0=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"liuxiyang641.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.12.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"width":300},"copycode":{"enable":true,"style":"mac"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":5,"unescape":true,"preload":false}}</script><script src="https://cdn.jsdelivr.net/npm/hexo-theme-next@8.12.2/source/js/config.min.js"></script>

    <meta name="description" content="LLM in-context learning LLM上下文学习，相关论文合集1。">
<meta property="og:type" content="blog">
<meta property="og:title" content="LLM-ICL">
<meta property="og:url" content="https://liuxiyang641.github.io/collection/LLM-ICL1/index.html">
<meta property="og:site_name" content="Liu Xiyang">
<meta property="og:description" content="LLM in-context learning LLM上下文学习，相关论文合集1。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230601201707682.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230601202801649.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230601204311989.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230601204823591.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230601223224356.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230601223450763.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230601223612766.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230603150047353.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230603150335100.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230603150505630.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230603150518579.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230603150903068.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230603150941339.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230603151018452.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230603151030519.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230603151912820.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230603152723289.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230614215935769.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230614215904682.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230614220633652.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230614220800480.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230614220826142.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230614221621275.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230614221822454.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230615203143639.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230615203205166.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230615203224510.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230615203308965.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230919105659374.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230919105832312.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230919110027138.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230919165336543.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230919165630284.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230919165745295.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230919165849595.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230919210208784.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230919210604445.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230920161615499.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230920161132344.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230920211244740.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230920211720814.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230920211806895.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230920212034299.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230920212534148.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230920212653543.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230921172022780.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230921172309663.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230921221943481.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230921222500768.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230921222539791.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230921223126629.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230921235901953.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230922000447451.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230922000836121.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230924210415173.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230924211332259.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230924211421422.png">
<meta property="article:published_time" content="2023-06-01T12:07:01.000Z">
<meta property="article:modified_time" content="2023-09-26T09:49:59.253Z">
<meta property="article:author" content="Liu Xiyang">
<meta property="article:tag" content="Collection">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="ICL">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230601201707682.png">


<link rel="canonical" href="https://liuxiyang641.github.io/collection/LLM-ICL1/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://liuxiyang641.github.io/collection/LLM-ICL1/","path":"collection/LLM-ICL1/","title":"LLM-ICL"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>LLM-ICL | Liu Xiyang</title>
  




<link rel="stylesheet" type="text/css" href="/css/injector/main.css" /><link rel="preload" as="style" href="/css/injector/light.css" /><link rel="preload" as="style" href="/css/injector/dark.css" />
  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Liu Xiyang</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">40</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">54</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">146</span></a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#llm-in-context-learning"><span class="nav-number">1.</span> <span class="nav-text">LLM in-context learning</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#ape"><span class="nav-number">1.1.</span> <span class="nav-text">APE</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#sg-icl"><span class="nav-number">1.2.</span> <span class="nav-text">SG-ICL</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#self-instruct"><span class="nav-number">1.3.</span> <span class="nav-text">Self-Instruct</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#discrete-prompt-for-different-slm"><span class="nav-number">1.4.</span> <span class="nav-text">Discrete prompt for different SLM</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#wang-et-al."><span class="nav-number">1.5.</span> <span class="nav-text">Wang et al.</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#kate"><span class="nav-number">1.6.</span> <span class="nav-text">KATE</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#effect-of-ordered-examples"><span class="nav-number">1.7.</span> <span class="nav-text">Effect of ordered examples</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#rethinking-the-role-of-demonstrations"><span class="nav-number">1.8.</span> <span class="nav-text">Rethinking the role of demonstrations</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#self-adaptive-icl"><span class="nav-number">1.9.</span> <span class="nav-text">Self-adaptive ICL</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#udr"><span class="nav-number">1.10.</span> <span class="nav-text">UDR</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#cover-ls"><span class="nav-number">1.11.</span> <span class="nav-text">Cover-LS</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#mutual-information"><span class="nav-number">1.12.</span> <span class="nav-text">Mutual information</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Liu Xiyang"
      src="/images/lxy-avatar.jpg">
  <p class="site-author-name" itemprop="name">Liu Xiyang</p>
  <div class="site-description" itemprop="description">Try your best to be an ordinary man.</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">146</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">54</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">40</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/liuxiyang641" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;liuxiyang641" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:liuxiyang@buaa.edu.cn" title="E-Mail → mailto:liuxiyang@buaa.edu.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license site-overview-item animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdn.jsdelivr.net/npm/@creativecommons/vocabulary@2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>



        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://liuxiyang641.github.io/collection/LLM-ICL1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/lxy-avatar.jpg">
      <meta itemprop="name" content="Liu Xiyang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Liu Xiyang">
      <meta itemprop="description" content="Try your best to be an ordinary man.">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="LLM-ICL | Liu Xiyang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          LLM-ICL<a href="https://github.com/liuxiyang641/liuxiyang641.github.io/edit/hexo/source/_posts/collection/LLM-ICL1.md" class="post-edit-link" title="编辑" rel="noopener" target="_blank"><i class="fa fa-pen-nib"></i></a>
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-06-01 20:07:01" itemprop="dateCreated datePublished" datetime="2023-06-01T20:07:01+08:00">2023-06-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-09-26 17:49:59" itemprop="dateModified" datetime="2023-09-26T17:49:59+08:00">2023-09-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper/" itemprop="url" rel="index"><span itemprop="name">Paper</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper/LLM/" itemprop="url" rel="index"><span itemprop="name">LLM</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper/LLM/ICL/" itemprop="url" rel="index"><span itemprop="name">ICL</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>26k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>24 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="llm-in-context-learning">LLM in-context learning</h1>
<p>LLM上下文学习，相关论文合集1。</p>
<span id="more"></span>
<h2 id="ape">APE</h2>
<p>Large Language Models are Human-Level Prompt Engineers</p>
<p>ICLR 2023，University of Toronto，<a target="_blank" rel="noopener" href="https://github.com/keirp/automatic_prompt_engineer">代码</a>。</p>
<blockquote>
<p>By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, <strong>we propose Automatic Prompt Engineer (APE) for automatic instruction generation and selection.</strong> In our method, we treat the instruction as the “program,” optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. <strong>Extensive experiments show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 24/24 Instruction Induction tasks and 17/21 curated BIG-Bench tasks.</strong> We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that APE-engineered prompts are able to improve few-shot learning performance (by simply prepending them to standard in-context learning prompts), find better zero-shot chain-of-thought prompts, as well as steer models toward truthfulness and/or informativeness.</p>
</blockquote>
<p>作者提出一种从几个样例中自动创建task instruction的training free的方法APE（Automatic Prompt Engineer）。</p>
<p>由于LLM对于prompt的理解和人类不一样，因此简单的语言prompt不一定能够得到理想的结果。因此往往需要大量的人工去设计prompt，去搜索寻找最好的prompt实践。作者将自动寻找最能够激发LLM执行具体task能力的prompt的过程看做是一个black-box optimization problem，称之为natural language program synthesis。</p>
<p>作者提出的APE方案如下：</p>
<figure>
<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230601201707682.png" alt="image-20230601201707682" /><figcaption>image-20230601201707682</figcaption>
</figure>
<p>简单来说就是3步，</p>
<ol type="1">
<li>让LLM从几个样例中产生一系列的task instructions</li>
<li>让LLM使用不同的task instructions，评估不同task instructions的效果。先选一个子集评估所有的instruction，然后对于score比较高的instructions，再选新的子集评估筛选，直至选出一定数量的候选instructions</li>
<li>[可选] 为了防止在某些任务下，LLM一开始没有找到合适的instructions，重新采样。让LLM在当前score最高的instructions附近采样</li>
</ol>
<p>第1步采样初始的task instructions，作者使用了三种方式：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230601202801649.png"   style="zoom:40%;" /></p>
<p>第一种forward mode prompt是最直接的，让LLM续写query，但是instructions只会出现在句子最后。作者提出第2种reverse mode prompt让LLM填空，让instructions可以出现在句子的任意地方。最后是对于一些已经发现了效果比较好的模板，可以加进来（具体在实验中可能作者只是在TruthfulQA数据集下使用了？）。</p>
<p>第2步怎么样评价不同instructions的效果，当然最直接的是根据任务指标来验证。在一般情况下，作者建议可以直接使用0-1 loss来给不同生成的instructions打分。还可以使用log probability。同时，让每个instructions都在所有的训练样例下进行评估是不实际的，因此作者就提出使用训练子集来筛选合适的instructions。如果一个子集筛选过后的instructions还是太多，就继续采样新的不重叠的训练子集，继续筛选instructions。</p>
<p>第3步是针对一些情况下，LLM生成的所有instructions没有找到合适的instructions，让LLM继续在当前效果最好的instructions周围进行寻找。we consider exploring the search space locally around the current best candidates. 这一步是可选的，因为作者发现不断增加更多的迭代步骤没有带来更多的提升，同时这种提升也不是在所有task中都会出现。（在实验中APE默认无迭代，APE-IT是加入了迭代）</p>
<p>zero-shot instruction induction实验结果来看，APE方法在很多任务上已经达到了人工prompting的效果：</p>
<figure>
<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230601204311989.png" alt="image-20230601204311989" /><figcaption>image-20230601204311989</figcaption>
</figure>
<p>生成更多的候选instructions带来了更好的效果，同时iterative search对于那些一开始没有办法找到合适的instructions的task有提升作用，而在另外task上仅仅是生成一次就效果比较好了：</p>
<figure>
<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230601204823591.png" alt="image-20230601204823591" /><figcaption>image-20230601204823591</figcaption>
</figure>
<h2 id="sg-icl">SG-ICL</h2>
<p>Self-Generated In-Context Learning: Leveraging Auto-regressive Language Models as a Demonstration Generator</p>
<p>NAACL 2022 Workshop，首尔大学</p>
<blockquote>
<p>Large-scale pre-trained language models (PLMs) are well-known for being capable of solving a task simply by conditioning a few input-label pairs dubbed demonstrations on a prompt without being explicitly tuned for the desired downstream task. Such a process (i.e., in-context learning), however, naturally leads to high reliance on the demonstrations which are usually selected from external datasets. In this paper, <strong>we propose self-generated in-context learning (SG-ICL), which generates demonstrations for in-context learning from PLM itself to minimize the reliance on the external demonstration.</strong> We conduct experiments on four different text classification tasks and show SG-ICL significantly outperforms zero-shot learning and is generally worth approximately 0.6 gold training samples. Moreover, our generated demonstrations show more consistent performance with low variance compared to randomly selected demonstrations from the training dataset.</p>
</blockquote>
<p>利用LLM自动生成demonstrations来增强zero-shot ICL的能力（作者声称是首个这么做的工作）。</p>
<figure>
<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230601223224356.png" alt="image-20230601223224356" /><figcaption>image-20230601223224356</figcaption>
</figure>
<p>需要注意的一点是，在使用LLM生成demonstrations的时候，是输入了对应的test instance和期望生成的class一起生成的。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230601223450763.png"   style="zoom:40%;" /></p>
<p>作者在实验部分使用LLM位每个test instance生成8个构造的demonstrations，发现效果和在进行5-shot的LLM ICL效果差不多，因此作者认为1个LLM生成的demonstration价值相当于0.6个gold training sample：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230601223612766.png"  style="zoom:40%;" /></p>
<h2 id="self-instruct">Self-Instruct</h2>
<p>ACL 2023，华盛顿大学，<a target="_blank" rel="noopener" href="https://github.com/%20yizhongw/self-instruct">代码</a>。</p>
<a href="/llm/Self-Instruct/" title="[个人详细博客]">[个人详细博客]</a>
<blockquote>
<p>Large “instruction-tuned” language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. <strong>We introduce SELF-INSTRUCT, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations.</strong> Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute improvement over the original model on SUPER-NATURALINSTRUCTIONS, on par with the performance of InstructGPT 001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with SELF-INSTRUCT outperforms using existing public instruction datasets by a large margin, leaving only a 5% absolute gap behind InstructGPT 001 . SELF-INSTRUCT provides an almost annotation-free method for aligning pretrained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning.</p>
</blockquote>
<p>人工生成instructions一方面代价很大，另一方面人工生成的instructions难以保证quantity, diversity, and creativity。</p>
<p>作者提出使用LLM从已有的task instruction出发，自动生成新的task instruction和对应的input-output，然后过滤掉不符合规则的新task instructions，再加入到已有的task instructions集合中。作者在这个自动构造的instruction data上fine-tuning GPT3，发现效果提升了33%，非常接近InstructGPT001的效果。</p>
<p>作者提出的方法：</p>
<figure>
<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230603150047353.png" alt="image-20230603150047353" /><figcaption>image-20230603150047353</figcaption>
</figure>
<p>首先，作者拥有一个task pool，包括175 tasks (1 instruction and 1 instance for each task)。这175个初始的task instructions都是由本文作者自己创建的。</p>
<p>然后，作者从task pool中随机抽取8个task instructions（6 are from the human-written tasks, and 2 are from the model-generated tasks）。下面是产生新task instruction的prompt：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230603150335100.png" style="zoom: 25%;" /></p>
<p>之后，作者使用LLM判断新产生的instruction是否是一个classification task（using 12 classification instructions and 19 non-classification instructions）：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230603150505630.png"   style="zoom:25%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230603150518579.png" alt="image-20230603150518579" style="zoom:25%;" /></p>
<p>随后，对于新产生的task instruction，用LLM生成新的对应的instance。对于生成任务，作者先生成input，再生成output，作者称为Input-first Approach：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230603150903068.png"   style="zoom:25%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230603150941339.png"  style="zoom:25%;" /></p>
<p>对于分类任务，作者发现如果是先生成input，LLM总是会倾向于生成某一个label的输入。因此作者使用LLM先生成output label，再让LLM生成input，作者称为Output-first Approach：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230603151018452.png"   style="zoom:25%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230603151030519.png"   style="zoom:25%;" /></p>
<p>对于LLM生成的task instruction、input和output，需要通过一些规则过滤，比如：</p>
<ul>
<li>只有当和已有的task instruction相似度全部比较低（<span class="math inline">\(\mbox{ROUGE-L}&lt; 0.7\)</span>）的时候，一个新task instruction会被添加到task pool里</li>
<li>We also exclude instructions that contain some specific keywords (e.g., image, picture, graph) that usually can not be processed by LMs.</li>
<li>When generating new instances for each instruction, we filter out instances that are exactly the same or those with the same input but different outputs.</li>
<li>Invalid generations are identified and filtered out based on heuristics (e.g., instruction is too long or too short, instance output is a repetition of the input).</li>
</ul>
<p>作者从原始的175个task出发，最后构造了5万多的task，并且差异性也比较大：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230603151912820.png"   style="zoom:30%;" /></p>
<p>在SuperNI数据集上的实验结果：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230603152723289.png"   style="zoom:30%;" /></p>
<p>SuperNI数据集大多是已有的NLP任务，为了进一步评估模型在实际使用场景下的价值，作者人工创建了一个包括252 task的新数据集。</p>
<h2 id="discrete-prompt-for-different-slm">Discrete prompt for different SLM</h2>
<p>Can discrete information extraction prompts generalize across language models?</p>
<p>ICLR 2023，<a target="_blank" rel="noopener" href="https://github.com/ncarraz/prompt_%20generalization">代码</a>。</p>
<p>作者探究了不同小参数量语言模型对于discrete prompt的泛化性的情况，并且提出了mixed-training autoprompt。也就是在AutoPrompt的方法基础上，用一个LM进行候选prompt生成，另一个LM进行评估。</p>
<blockquote>
<p>We study whether automatically-induced prompts that effectively extract information from a language model can also be used, out-of-the-box, to probe other language models for the same information. After confirming that discrete prompts induced with the AutoPrompt algorithm outperform manual and semi-manual prompts on the slot-filling task, we demonstrate a drop in performance for AutoPrompt prompts learned on a model and tested on another. We introduce a way to induce prompts by mixing language models at training time that results in prompts that generalize well across models. We conduct an extensive analysis of the induced prompts, finding that the more general prompts include a larger proportion of existing English words and have a less order-dependent and more uniform distribution of information across their component tokens. Our work provides preliminary evidence that it’s possible to generate discrete prompts that can be induced once and used with a number of different models, and gives insights on the properties characterizing such prompts.</p>
</blockquote>
<p>作者对比了三种方法产生的prompt：</p>
<ul>
<li>LPAQA：使用预先定义好的几个prompt，通过mining和paraphrasing发现更多的prompt</li>
<li>AutoPrompt：让LM自动生成prompt</li>
<li>OptiPrompt：使用了soft prompt</li>
</ul>
<p>作者在下面一系列的小LM上进行了实验：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230614215935769.png"   style="zoom:50%;" /></p>
<p>在LAMA数据集上的对比效果如下，表格中LAMA是指该数据集本身提供的人工写的prompt：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230614215904682.png"   style="zoom:50%;" /></p>
<p>作者发现使用了soft prompt的OptiPrompt方法效果最好，而AutoPrompt这种自动生成discrete prompt的方法效果次之，但也是好于人工写的prompt。</p>
<p>soft prompt虽然在自动生成prompt和使用prompt的LM是一致的情况下效果最好，但是泛化性很差：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230614220633652.png"   style="zoom:50%;" /></p>
<p>同时soft prompt还要求能够访问LM的内部结构，能够使用它的embeddings。</p>
<p>作者进而测试了在使用AutoPrompt的情况下，这些生成的prompt在source LM和target LM不一致的情况下的效果：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230614220800480.png"   style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230614220826142.png"   style="zoom:50%;" /></p>
<p>可以看出来，当两个LM是一样的情况下，效果最好。如果target LM和source LM不一致的情况下，基本上效果都会下降。中间GPT-2的一系列变种变化不大，作者解释原因是GPT-2本身效果已经比较差了，再差点也没有什么太大变化了。</p>
<p>为了解决这一问题，作者提出了AutoPrompt的一个简单改动：</p>
<blockquote>
<p>Recall that the AutoPrompt algorithm involves two phases: one in which candidate prompts are generated, and one in which the prompts are evaluated. Rather than relying on the same model for the two phases, we now use two different LMs. <strong>The first model, which we call the generator, proposes a set of candidates. Then, the second model, that we call the evaluator, evaluates the candidates and chooses the best one.</strong></p>
</blockquote>
<p>下面是作者的实验：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230614221621275.png"   style="zoom:50%;" /></p>
<p>可以看到，不同架构的LM混合在一起，是可能生成具有更好泛化性的prompt的。但是作者在论文中也指出，这种更好的泛化性的生成和哪两个LM组合到一起有关，不是说只要组合在一起，就能够生成泛化性更好的prompt。</p>
<p>在最后，作者尝试从4个方面对找到的泛化性更好的prompt进行定量分析：</p>
<ol type="1">
<li>Semantic overlap with English: We thus hypothesize that prompts that generalize better will have a larger semantic overlap with manually crafted English prompts.
<ul>
<li>作者的观察：对于作者提出的mixed-training AutoPrompt方法来说，泛化性更强的prompt和人类实际语言之间的相似度有很强的相关性。但是这种相似度，和泛化性更弱的单个模型比如BERT-base产生的prompt对比的话，相似度耕地（说明这个假设还有待验证）。</li>
</ul></li>
<li>Real-word ratio: We thus conjecture that prompts that generalize better will contain a larger proportion of real English words.
<ul>
<li>作者的观察：进行了混合策略的方法总是自动产生了更多real world的单词；但是不一定能够带来更好的泛化性。</li>
</ul></li>
<li>Shuffling: We thus conjecture that a “bag-of-token” prompt sequence that does not require the tokens to be in any special order will be more general than one where order matters and, consequently, generalizing prompts will be more robust to token shuffling.
<ul>
<li>作者的观察：更加泛化的prompt确实对于token的顺序更加不敏感</li>
</ul></li>
<li>Token deletion: We thus conjecture that generalizing prompts will distribute information more evenly across tokens and thus they will be more robust to single-token deletion.
<ul>
<li>作者的观察：随机去掉prompt上不同位置token，测试prompt不同位置上，LM的关注程度是不是不同的。发现总是对于prompt最后一个位置的token有最大的关注。</li>
</ul></li>
</ol>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230614221822454.png"  style="zoom:50%;" /></p>
<h2 id="wang-et-al.">Wang et al.</h2>
<p>Large Language Models Are Implicitly Topic Models: Explaining and Finding Good Demonstrations for In-Context Learning</p>
<p>arXiv 2023, <a target="_blank" rel="noopener" href="https://github.com/WANGXinyiLinda/concept-based-demonstration-selection">代码</a>。</p>
<blockquote>
<p>In recent years, pre-trained large language models have demonstrated remarkable efficiency in achieving an inference-time few-shot learning capability known as incontext learning. However, existing literature has highlighted the sensitivity of this capability to the selection of few-shot demonstrations. The underlying mechanisms by which this capability arises from regular language model pretraining objectives remain poorly understood. In this study, we aim to examine the in-context learning phenomenon through a Bayesian lens, viewing large language models as topic models that implicitly infer task-related information from demonstrations. On this premise, we propose an algorithm for selecting optimal demonstrations from a set of annotated data and demonstrate a significant 12.5% improvement relative to the random selection baseline, averaged over eight GPT2 and GPT3 models on eight different real-world text classification datasets. Our empirical findings support our hypothesis that large language models implicitly infer a latent concept variable.</p>
</blockquote>
<p>作者提出了一种看待ICL中的demonstration的作用的角度，认为LLM可以从demonstrations中学习到隐式的任务相关的信息。并且给出了一些理论上的分析。</p>
<p>基于此假设，作者提出一种新的找demonstrations的方法，整体流程如下：</p>
<figure>
<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230615203143639.png" alt="image-20230615203143639" /><figcaption>image-20230615203143639</figcaption>
</figure>
<p>首先是利用prompt-tuning的思想，固定LM，学习和task相关的soft prompt：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230615203205166.png"   style="zoom:50%;" /></p>
<p>然后，作者认为最佳的demonstrations就是能够在给定样例的情况下，使得输出上一步学习到的soft prompt的概率最大的样例。直观上的理解就是说最能够“体现”任务的demonstrations就是最佳的找到的demonstrations。demonstrations的候选集应该是各种数量、各种排序的组合，但是为了减小搜索空间，作者简化到了假设最佳的单个demonstration的采样是相互独立的，然后再排序：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230615203224510.png"   style="zoom:50%;" /></p>
<p>最后，找到的这种最佳的demonstrations，可以用于其它的LM。</p>
<p>实验结果：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230615203308965.png"  style="zoom:50%;" /></p>
<h2 id="kate">KATE</h2>
<p>What Makes Good In-Context Examples for GPT-3?</p>
<p>DeeLIO 2022，杜克大学与Microsoft，<a href="https:%20//github.com/jiachangliu/KATEGPT3">代码</a>。</p>
<blockquote>
<p>GPT-3 has attracted lots of attention due to its superior performance across a wide range of NLP tasks, especially with its in-context learning abilities. Despite its success, we found that the empirical results of GPT-3 depend heavily on the choice of in-context examples. In this work, we investigate whether there are more effective strategies for judiciously selecting incontext examples (relative to random sampling) that better leverage GPT-3’s in-context learning capabilities. Inspired by the recent success of leveraging a retrieval module to augment neural networks, <strong>we propose to retrieve examples that are semantically-similar to a test query sample to formulate its corresponding prompt.</strong> Intuitively, the examples selected with such a strategy may serve as more informative inputs to unleash GPT-3’s power of text generation. We evaluate the proposed approach on several natural language understanding and generation benchmarks, where the retrieval-based prompt selection approach consistently outperforms the random selection baseline. Moreover, it is observed that the sentence encoders fine-tuned on task-related datasets yield even more helpful retrieval results. Notably, significant gains are observed on tasks such as table-totext generation (44.3% on the ToTTo dataset) and open-domain question answering (45.5% on the NQ dataset).</p>
</blockquote>
<p>在GPT-3中的原始ICL是随机找上下文样例，作者发现上下文样例的选择会极大的影响GPT-3的效果：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230919105659374.png"   style="zoom:35%;" /></p>
<p>因此，寻找适用于GPT的上下文样例很关键。最粗暴的做法是穷举所有样例的可能排列，但这个是不实际的。作者提出，利用kNN方法，从训练集中寻找sentence-level语义相似的样例来辅助ICL：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230919105832312.png"  style="zoom:50%;" /></p>
<p>利用kNN找相似样例首先是需要将样例转化为embedding，作者提出两个考虑：</p>
<ul>
<li>The first category includes generally pre-trained sentence encoders such as the BERT.</li>
<li>The second category includes sentence encoders fine-tuned on specific tasks or datasets.</li>
</ul>
<p>从实验上看，效果提升很明显：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230919110027138.png"   style="zoom:35%;" /></p>
<h2 id="effect-of-ordered-examples">Effect of ordered examples</h2>
<p>Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity</p>
<p>伦敦大学，ACL 2022</p>
<blockquote>
<p>When primed with only a handful of training samples, very large, pretrained language models such as GPT-3 have shown competitive results when compared to fully-supervised, fine-tuned, large, pretrained language models. <strong>We demonstrate that the order in which the samples are provided can make the difference between near state-of-the-art and random guess performance</strong>: essentially some permutations are “fantastic” and some not. We analyse this phenomenon in detail, establishing that: it is present across model sizes (even for the largest current models), it is not related to a specific subset of samples, and that a given good permutation for one model is not transferable to another. While one could use a development set to determine which permutations are performant, this would deviate from the true fewshot setting as it requires additional annotated data. Instead, <strong>we use the generative nature of language models to construct an artificial development set and based on entropy statistics of the candidate permutations on this set</strong>, we identify performant prompts. Our method yields a 13% relative improvement for GPTfamily models across eleven different established text classification tasks.</p>
</blockquote>
<p>之前已经有很多讨论sample structure/formatting的工作，作者声称这篇论文是首个讨论sample顺序对ICL性能影响的paper。</p>
<p>要注意，作者的ICL是corpus-level的prompt，对于所有query，有相同的demonstrations。直接来看作者实验得到的几个结论：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230919165336543.png"   style="zoom:35%;" /></p>
<p>观察：</p>
<ul>
<li>ICL中demonstrations的排列顺序很重要，最坏的情况下是50%准确率，也就是随机猜测；最好的排列是性能接近有监督SOTA方法</li>
<li>LLM对于demonstrations的order很敏感，增大model size虽然似乎会一定程度缓解模型对example order的敏感程度，但是在一些任务下，这种缓解趋势并不存在（如Figure 1下面在Subj数据集的实验结果）</li>
<li>与之相反的，传统的有不同初始化设置的有监督训练的方法的性能variance，可能就是在1%-2%（反正肯定在个数百分比）的变化。</li>
</ul>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230919165630284.png"   style="zoom:40%;" /></p>
<p>观察：</p>
<ul>
<li>Adding training samples does not significantly reduce variance. 增加上下文样例数量并不会缓解对order的敏感性。</li>
</ul>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230919165745295.png"   style="zoom:40%;" /></p>
<p>观察：</p>
<ul>
<li>Performant prompts are not transferable across models. 不同LLM偏好的order是不通用的（图中的pairwise Spearman’s rank correlation系数很小）</li>
</ul>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230919165849595.png"   style="zoom:40%;" /></p>
<p>观察：</p>
<ul>
<li>表现不好的order原因之一可能是，不够平衡的label估计，总是对某一类label过于自信。</li>
</ul>
<p>从上面可以看出，表现不好的prompt可能会导致有不平衡的label输出估计。因此，作者提出了一种无监督的评估方法，选择ICL样例的合适排序。</p>
<p>首先，作者根据随机采样得到的样例集合，使用它们的所有可能排列，如有<span class="math inline">\(n\)</span>个example，那么对应<span class="math inline">\(n!\)</span>种排列permutations，每个permutation，让LLM生成对应的新的data和label，要注意此时生成的label不能保证是正确的，因此会丢弃。所有生成的sequences作为<em>probing set</em>：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230919210208784.png"   style="zoom:40%;" /></p>
<p>接下来，对于每种可能的排列，作者通过在<em>probing set</em>上计算两种无监督基于熵的指标，来选择合适ordered prompt：</p>
<ul>
<li>Global Entropy (GlobalE)：avoid the issue of extremely unbalanced predictions. 用LLM计算在给定某种排列顺序的demonstrations之后，对于整个probing set上所有data预测label的分布，如果让这个总体分布熵越小，表示越倾向输出一致/不平衡的label，更有可能出错。</li>
<li>Local Entropy (LocalE)：if a model is overly confident for all probing inputs, then it is likely that the model is not behaving as desired. 分别计算probing data预测label分布的熵，然后平均。</li>
</ul>
<p>实验结果：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230919210604445.png"   style="zoom:50%;" /></p>
<p>采用GlobalE或LocalE指标能够减小LLM order对于效果的variance。</p>
<h2 id="rethinking-the-role-of-demonstrations">Rethinking the role of demonstrations</h2>
<p>华盛顿大学与Meta，EMNLP 2022，<a href="github.com/Alrope123/rethinking-demonstrations">代码</a>。<a href="/llm/rethinking-role-of-demonstrations/" title="[详细博客]">[详细博客]</a></p>
<blockquote>
<p>Large language models (LMs) are able to incontext learn—perform a new task via inference alone by conditioning on a few input-label pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of how the model learns and which aspects of the demonstrations contribute to end task performance. <strong>In this paper, we show that ground truth demonstrations are in fact not required—randomly replacing labels in the demonstrations barely hurts performance on a range of classification and multi-choice tasks</strong>, consistently over 12 different models including GPT-3. Instead, we find that other aspects of the demonstrations are the key drivers of end task performance, including the fact that they provide a few examples of (1) the label space, (2) the distribution of the input text, and (3) the overall format of the sequence. Together, our analysis provides a new way of understanding how and why in-context learning works, while opening up new questions about how much can be learned from large language models through inference alone.</p>
</blockquote>
<p>作者对于上下文学习中，什么样的signal是对LLM进行task learning有帮助的进行了实验探究。</p>
<p>作者主要针对4个ICL中的demonstrations可能提供的learning signal进行了实验：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230920161615499.png"   style="zoom:40%;" /></p>
<p>作者的第一个重要发现是ICL中demonstrations的input-label是否正确匹配，对模型效果的影响不大。作者用随机的label来替换demonstrations的ground truth label，发现效果下降不是很多：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230920161132344.png"   style="zoom:50%;" /></p>
<h2 id="self-adaptive-icl">Self-adaptive ICL</h2>
<p>Self-Adaptive In-Context Learning: An Information Compression Perspective for In-Context Example Selection and Ordering</p>
<p>Shanghai AI Laboratory和厦门大学，ACL 2023，<a target="_blank" rel="noopener" href="https://github.com/Shark-NLP/self-adaptive-ICL">代码</a>。</p>
<blockquote>
<p>Despite the impressive few-shot performance of in-context learning (ICL), it remains a common practice to randomly select examples to serve as the context. In this paper, we advocate self-adaptive in-context learning, a new principle for ICL, in which <strong>the self-adaption mechanism is introduced to help each input find an in-context example organization</strong> (i.e., selection and permutation) that can derive the correct output, thus maximizing performance. To validate the effectiveness of self-adaptive ICL, we propose a general <strong>select-then-rank</strong> framework and a set of novel selection and ranking algorithms. Upon extensive evaluation on eight different NLP datasets, our self-adaptive ICL method achieves a 40% relative improvement over the common practice setting. Further analysis reveals the great potential of selfadaptive ICL as a promising method to close the gap between ICL and finetuning. Our code will be released to facilitate future research.</p>
</blockquote>
<p>作者提出，针对每个test example进行demonstrations selection，然后进行ranking也就是排列。首先，作者建议有以下的寻找demonstrations的策略：</p>
<ul>
<li>TopK：KATE方法和<em>Making pre-trained language models better few-shot learners</em> 工作中提出，选择test sample的nearest neighbors作为demonstrations。</li>
<li>VoteK：在TopK的策略上，通过惩罚和已选择的样例相似的候选样例，考虑多样性[<em>Selective annotation makes language models better few-shot learners</em>]。</li>
<li>DPP：作者进一步尝试了determinantal point process (DPP)，同样是一种考虑样例多样性的指标[<em>k-dpps: Fixed-size determinantal point processes. ICML 2011</em>]。</li>
</ul>
<p>为了找合适的样例排列顺序，作者从Solomonoff’s general theory of inference (Solomonoff, 1964)和Minimum Description Length (MDL) principle (Grünwald, 2007)出发，倾向于选择能够无损压缩test sample的demonstrations：</p>
<blockquote>
<p>We assume that a good organization of in-context examples is the organization that is good at losslessly compressing testing samples.</p>
</blockquote>
<p>换句话说，无损的压缩test sample，意味着可以从demonstrations中，正确的推导出test sample对应的label：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230920211244740.png"   style="zoom:50%;" /></p>
<p>其中<span class="math inline">\(c\)</span>表示某种demonstrations组织。<span class="math inline">\(L_{\theta}(y | c, x)\)</span>是指在给定样例<span class="math inline">\(c\)</span>和test input <span class="math inline">\(x\)</span>，能够真正压缩与还原test label <span class="math inline">\(y\)</span>的codelength。<span class="math inline">\(L(\theta)\)</span>是指描述model本身所需的codelength，对于所有的候选demonstrations是一样的，因此可以忽略。</p>
<p>上述公式定义最佳的demonstrations应该使无损压缩test sample所需的codelength最短。</p>
<p>使用Shannon-Huffman code来计算进行data transmission的codelength：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230920211720814.png"   style="zoom:50%;" /></p>
<p>但是test label <span class="math inline">\(y\)</span>在实际中是未知的，因此作者使用codelength的期望来代替：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230920211806895.png"  style="zoom:50%;" /></p>
<p>其中<span class="math inline">\(q(y_i | Y)\)</span>是test label <span class="math inline">\(y\)</span>的先验分布，对于每个test sample来说可能是不一样的，因此作者再次使用model进行估计：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230920212034299.png"   style="zoom:50%;" /></p>
<p>现在，从理论上，对于每个排列<span class="math inline">\(c\)</span>，我们都可以计算出一个无监督的metric来进行选择；但是由于作者的demonstrations是instance-level的，潜在的排序数量仍然过多，不可能真正的计算一遍所有的候选排序。因此，作者实际上只是在对应的<span class="math inline">\(K=8\)</span>采样集合中，随机采样<span class="math inline">\(10\)</span>种排列，然后进行ranking。</p>
<p>对于最终的公式5，此时作者是在计算熵，也就是寻找熵最小的情况，寻找让LLM的预测分布非常confident的demonstrations。这和上面<em>Effect of ordered examples</em>工作中提出的GlobalE和LocalE的思想是相反的。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230920212534148.png"   style="zoom:50%;" /></p>
<p>观察：</p>
<ul>
<li>使用TopK选择demonstrations，然后使用MDL进行ranking取得了最好的结果</li>
<li>TopK的instance-level的ICL策略，已经在大多数据集上优于corpus-level的ICL策略</li>
</ul>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230920212653543.png"   style="zoom:50%;" /></p>
<p>观察：</p>
<ul>
<li>不同任务下需要selection偏好不同。在分类任务下，使用TopK策略取得了最好的效果，在QA任务下，使用VoteK等考虑多样性的策略效果更好。</li>
</ul>
<h2 id="udr">UDR</h2>
<p>Unified Demonstration Retriever for In-Context Learning</p>
<p>复旦，ACL 2023，<a target="_blank" rel="noopener" href="https://github.com/KaiLv69/UDR">代码</a>。</p>
<blockquote>
<p>In-context learning is a new learning paradigm where a language model conditions on a few input-output pairs (demonstrations) and a test input, and directly outputs the prediction. It has been shown highly dependent on the provided demonstrations and thus promotes the research of demonstration retrieval: given a test input, relevant examples are retrieved from the training set to serve as informative demonstrations for in-context learning. While previous works focus on training task-specific retrievers for several tasks separately, these methods are often hard to transfer and scale on various tasks, and separately trained retrievers incur a lot of parameter storage and deployment cost. In this paper, <strong>we propose Unified Demonstration Retriever (UDR), a single model to retrieve demonstrations for a wide range of tasks.</strong> To train UDR, we cast various tasks’ training signals into a unified list-wise ranking formulation by language model’s feedback. Then we propose a multi-task list-wise ranking training framework, with an iterative mining strategy to find high-quality candidates, which can help UDR fully incorporate various tasks’ signals. Experiments on 30+ tasks across 13 task families and multiple data domains show that UDR significantly outperforms baselines. Further analyses show the effectiveness of each proposed component and UDR’s strong ability in various scenarios including different LMs (1.3B ∼ 175B), unseen datasets, varying demonstration quantities, etc.</p>
</blockquote>
<p>目前对于ICL进行demonstrations选择的方法有两类：</p>
<ul>
<li>无监督的：比如使用BM25或者SBERT等embedding进行相似度计算</li>
<li>有监督的：针对特定task，在某种ranking的监督信号下，训练一个demonstration retriever</li>
</ul>
<p>作者期望解决的问题是，能够设计一种适用于不同task的Demonstration Retriever。</p>
<p>作者提出的方法：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230921172022780.png"  style="zoom:50%;" /></p>
<p>首先，作者计算query sample和candidate example的方式是基于dense passage retriever (DPR) [<em>Dense passage retrieval for open-domain question answering. EMNLP 20</em>]的思路。也就是使用encoder，将query example <span class="math inline">\(x\)</span>和candidate example <span class="math inline">\(z\)</span>分别进行编码，然后计算相似度：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230921172309663.png"   style="zoom:40%;" /></p>
<p>为了能够辨别来自不同task的example，编码的时候会将task instruction <span class="math inline">\(I\)</span>和example一起进行编码。</p>
<p>实现中，作者使用BERT-base来编码。</p>
<p>然后，作者的ranking signal来自LM本身的conditional probability，继承于之前的的工作EPR的思想：</p>
<blockquote>
<p>This indicates how helpful this candidate is for decoding the target (independent of all other candidates).</p>
</blockquote>
<p>公式：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230921221943481.png"   style="zoom:40%;" /></p>
<p>其中<span class="math inline">\(G\)</span>代表LLM，<span class="math inline">\(z_j\)</span>是某个candidate example，<span class="math inline">\(s_{gen}\)</span>和<span class="math inline">\(s_{cls}\)</span>分别代表生成任务和分类任务下，加入example <span class="math inline">\(z_j\)</span>后LM对query <span class="math inline">\(x\)</span>输出正确<span class="math inline">\(y\)</span>的概率。<span class="math inline">\(r(z_j)\)</span>代表<span class="math inline">\(z_j\)</span>在所有candidate examples中的rank。</p>
<p>作者期望通过相似度计算的retriever对于candidate examples的排序结果，能够靠近LM基于条件概率的排序结果，作者引入了两种loss：</p>
<ul>
<li><p>LambdaRank loss [<em>From RankNet to LambdaRank to LambdaMART: An overview. 2010</em>]：其中<span class="math inline">\(w\)</span>是一个根据不同pair <span class="math inline">\(z_i,z_j\)</span>动态调整的值</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230921222500768.png"   style="zoom:40%;" /></p></li>
<li><p>in-batch negative loss：让rank 1的example <span class="math inline">\(z^{*}\)</span>应该在所有可能的examples中和<span class="math inline">\(x\)</span>是最相似的</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230921222539791.png"   style="zoom:40%;" /></p></li>
</ul>
<p>最后，为了保证效率，在训练阶段，使用迭代的采样策略；最后在推理时，基于FAISS [<em>Billion-scale similarity search with gpus. 2021</em>]使用基于相似度的计算来寻找demonstrations（考虑到如果在推理阶段，使用LM来ranking的巨大代价）。</p>
<p>实验结果（基于GPT-Neo-2.7B作为LM）：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230921223126629.png"   style="zoom:40%;" /></p>
<h2 id="cover-ls">Cover-LS</h2>
<p>Diverse Demonstrations Improve In-context Compositional Generalization</p>
<p>ACL 2023，<a target="_blank" rel="noopener" href="https://github.com/itayle/diverse-demonstrations">代码</a>。</p>
<blockquote>
<p>In-context learning has shown great success in i.i.d semantic parsing splits, where the training and test sets are drawn from the same distribution. In this setup, models are typically prompted with demonstrations that are similar to the input utterance. However, in the setup of compositional generalization, where models are tested on outputs with structures that are absent from the training set, selecting similar demonstrations is insufficient, as often no example will be similar enough to the input. In this work, <strong>we propose a method to select diverse demonstrations that aims to collectively cover all of the structures required in the output program, in order to encourage the model to generalize to new structures from these demonstrations.</strong> We empirically show that combining diverse demonstrations with in-context learning substantially improves performance across three compositional generalization semantic parsing datasets in the pure in-context learning setup and when combined with fine-tuning.</p>
</blockquote>
<p>作者针对的是Compositional generalization问题。在组合泛化中，仅仅寻找和query example相似的单个example是不够的，需要考虑更加多样diverse的examples set来提供足够的信息。</p>
<p>Follow前人的工作[<em>Unobserved local structures make compositional generalization hard. 2022</em>]，作者定义的example的多样性体现在example的local structures。local structures是指Compositional generalization问题的答案program对应的抽象语法树的各个子图：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230921235901953.png"   style="zoom:40%;" /></p>
<p>作者期望找到的demonstrations一方面能够尽可能和query example相似；一方面能够有更多样的local structures去覆盖query example的predicted local structures。作者训练T5-large来针对query example输出local structures，而不是whole program。这样能够减低预测的难度。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230922000447451.png"   style="zoom:40%;" /></p>
<p>在找demonstrations时：</p>
<ul>
<li>作者先对T5-large预测的query example的local structures根据长度进行降序排列；这一思想和前人的工作[<em>Generate-and-Retrieve: Use Your Predictions to Improve Retrieval for Semantic Parsing. COLING 2022</em>]一致，先进行preliminary prediction，然后基于preliminary prediction再进行检索；</li>
<li>然后迭代的选择当前最长的local structure，寻找training set里覆盖了当前local structure中，和query example最相似的example。相似度的计算可以基于BM25或者SBERT；</li>
<li>最后，移除和已经选择的example的program一样的其它候选examples，继续进行选择；直至达到上限。</li>
</ul>
<p>找到的demonstrations不仅可以辅助与LLM的ICL，作者还直接用来fine-tuning一个SLM（同样是基于T5-large）。</p>
<p>实验结果（LLM基于<code>code-davinci-002</code>）：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230922000836121.png"   style="zoom:40%;" /></p>
<p>（相关工作里有一些寻找多样example的paper，只不过并非是为prompt设计的）</p>
<h2 id="mutual-information">Mutual information</h2>
<p>An Information-theoretic Approach to Prompt Engineering Without Ground Truth Labels</p>
<p>ACL 2022，<a href="github.com/BYU-PCCL/information-theoretic-prompts">代码</a>。</p>
<blockquote>
<p>Pre-trained language models derive substantial linguistic and factual knowledge from the massive corpora on which they are trained, and prompt engineering seeks to align these models to specific tasks. Unfortunately, existing prompt engineering methods require significant amounts of labeled data, access to model parameters, or both. We introduce a new method for selecting prompt templates without labeled examples and without direct access to the model. <strong>Specifically, over a set of candidate templates, we choose the template that maximizes the mutual information between the input and the corresponding model output.</strong> Across 8 datasets representing 7 distinct NLP tasks, we show that when a template has high mutual information, it also has high accuracy on the task. On the largest model, selecting prompts with our method gets 90% of the way from the average prompt accuracy to the best prompt accuracy and requires no ground truth labels.</p>
</blockquote>
<p>这篇属于评价哪个prompt template/ICL formatting比较好的工作，适用于任何分类的NLP任务，并且只要各个分类的开头token是独一无二的。</p>
<p>LLM的prompt是指：</p>
<blockquote>
<p>“prompt” refers to any language passed to the model via the context window.</p>
</blockquote>
<p>prompt的template可以理解为待填充的prompt，用来构建prompt：</p>
<blockquote>
<p>a template refers to a natural language scaffolding filled in with raw data, resulting in a prompt.</p>
</blockquote>
<p>prompt template的好坏能够极大的影响LLM的task performance，一种最general的找最好的template的思路是通过在validation set上进行评估，但这要求有提前的labeled set。</p>
<p>作者的方法是可以在不需要ground truth label的情况下，通过计算不同templates和LLM输出结果的mutual information来找合适的template：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230924210415173.png"   style="zoom:40%;" /></p>
<p>步骤：</p>
<ul>
<li>人工设计<span class="math inline">\(K\)</span>个prompt template，然后用几个samples测试下template，排除或者修改那些无法将对应label最开头的token输出在前几位的templates；</li>
<li>针对每个template，采样数据，填充template，创建测试数据；</li>
<li>对于输出结果，将不同label对应的开头token的权重重新归一化。计算mutual information，选择最大互信息的template；</li>
</ul>
<p>什么是互信息：</p>
<blockquote>
<p>Mutual information is a measure of the amount of shared information between two random variables (Cover and Thomas, 2006); in other words, it is the reduction in entropy that is observed in one random variable when the other random variable is known.</p>
</blockquote>
<p>假定templates <span class="math inline">\(f_{\theta}(X)=\{f_{\theta}(x)\}\)</span>是random variable，<span class="math inline">\(Y\)</span>是labels，计算mutual information <span class="math inline">\(I(f_{\theta(X)};Y)=D_{KL}(P_{(f_{\theta(X)},Y)}||P_{f_{\theta}} \otimes P_Y)\)</span>，进一步化简为：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230924211332259.png"   style="zoom:50%;" /></p>
<p>使用下面的采样期望进行估计：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230924211421422.png"   style="zoom:40%;" /></p>
<p><span class="math inline">\(H(\cdot)\)</span>代表熵。作者认为更高的mutual information更好，代表采用该template之后，熵减小的更多。</p>
<p>如何理解一个template有更大的互信息就更好？</p>
<ol type="1">
<li><p>根据上面的公式2，最大化互信息，意味着最大化<span class="math inline">\(H(Y)\)</span>和最小化<span class="math inline">\(H(Y|f_{\theta}(X))\)</span>。也就是一个更好的template，对原始的label有更少的bias（更uniform，不潜在的偏好某种label）；同时该template倾向于输出更confident的label probability</p></li>
<li><p>从data processing inequality [<em>Elements of Information Theory 2nd Edition (Wiley Series in Telecommunications and Signal Processing) 2006</em>]的角度分析，<span class="math inline">\(I(f_{\theta(X)};Y) \leq I(X;Y)\)</span>，也就是说<span class="math inline">\(I(f_{\theta(X)};Y)\)</span>是<span class="math inline">\(I(X;Y)\)</span>的lower bound。更大的<span class="math inline">\(I(f_{\theta(X)};Y)\)</span>意味着更tight的lower bound，因此保留了<span class="math inline">\(X\)</span>和<span class="math inline">\(Y\)</span>之间更多的信息</p></li>
</ol>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>Liu Xiyang
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://liuxiyang641.github.io/collection/LLM-ICL1/" title="LLM-ICL">https://liuxiyang641.github.io/collection/LLM-ICL1/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/Collection/" rel="tag"><i class="fa fa-tag"></i> Collection</a>
              <a href="/tags/LLM/" rel="tag"><i class="fa fa-tag"></i> LLM</a>
              <a href="/tags/ICL/" rel="tag"><i class="fa fa-tag"></i> ICL</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/llm/LLM-ICL-survey-pku/" rel="prev" title="LLM-ICL-survey-pku">
                  <i class="fa fa-chevron-left"></i> LLM-ICL-survey-pku
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/llm/Self-Instruct/" rel="next" title="Self-Instruct">
                  Self-Instruct <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments gitalk-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-flag"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Liu Xiyang</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">647k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">9:48</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="//cdn.jsdelivr.net/npm/@next-theme/pjax@0.5.0/pjax.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/hexo-theme-next@8.12.2/source/js/comments.min.js"></script><script src="https://cdn.jsdelivr.net/npm/hexo-theme-next@8.12.2/source/js/utils.min.js"></script><script src="https://cdn.jsdelivr.net/npm/hexo-theme-next@8.12.2/source/js/motion.min.js"></script><script src="https://cdn.jsdelivr.net/npm/hexo-theme-next@8.12.2/source/js/next-boot.min.js"></script><script src="https://cdn.jsdelivr.net/npm/hexo-theme-next@8.12.2/source/js/pjax.min.js"></script>

  
<script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/hexo-theme-next@8.12.2/source/js/third-party/search/local-search.min.js"></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdn.jsdelivr.net/npm/pdfobject@2.2.8/pdfobject.min.js","integrity":"sha256-tu9j5pBilBQrWSDePOOajCUdz6hWsid/lBNzK4KgEPM="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="https://cdn.jsdelivr.net/npm/hexo-theme-next@8.12.2/source/js/third-party/tags/pdf.min.js"></script>


  <script src="https://cdn.jsdelivr.net/npm/hexo-theme-next@8.12.2/source/js/third-party/fancybox.min.js"></script>


  
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"ams","js":{"url":"//cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"}}</script>
<script src="https://cdn.jsdelivr.net/npm/hexo-theme-next@8.12.2/source/js/third-party/math/mathjax.min.js"></script>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous">

<script class="next-config" data-name="gitalk" type="application/json">{"enable":true,"github_id":"liuxiyang641","repo":"liuxiyang_blog_comment","client_id":"b800b344e096846a4608","client_secret":"45ac194feea7e642c29f8e13180184cc98afb3e6","admin_user":"liuxiyang641","distraction_free_mode":true,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token","language":"zh-CN","js":{"url":"https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.js","integrity":"sha256-Pmj85ojLaPOWwRtlMJwmezB/Qg8BzvJp5eTzvXaYAfA="},"path_md5":"e4e19132b49a6fe985bc998b9b0f8734"}</script>
<script src="https://cdn.jsdelivr.net/npm/hexo-theme-next@8.12.2/source/js/third-party/comments/gitalk.min.js"></script>
<div class="moon-menu">
  <div class="moon-menu-items">
    
    <div id="moon-menu-item-back2bottom" class="moon-menu-item">
      <i class='fas fa-chevron-down'></i>    </div>
    
    <div id="moon-menu-item-back2top" class="moon-menu-item">
      <i class='fas fa-chevron-up'></i>    </div>
    
  </div>
  <div class="moon-menu-button">
    <svg class="moon-menu-bg">
      <circle class="moon-menu-cricle" cx="50%" cy="50%" r="44%"></circle>
      <circle class="moon-menu-border" cx="50%" cy="50%" r="48%"></circle>
    </svg>
    <div class="moon-menu-content">
      <div class="moon-menu-icon"><i class='fas fa-ellipsis-v'></i></div>
      <div class="moon-menu-text"></div>
    </div>
  </div>
</div><script src="/js/injector.js"></script>
</body>
</html>
