<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>MLP-Mixer</title>
    <url>/cv/MLP-Mixer/</url>
    <content><![CDATA[<h1 id="mlp-mixer-an-all-mlp-architecture-for-vision">MLP-Mixer: An all-MLP Architecture for Vision</h1>
<p>2021-5-4</p>
<p>谷歌大脑团队的新作<a href="https://github.com/google-research/vision_transformer">MLP-Mixer</a>，使用纯MLP进行CV任务，没有超越目前业界SOTA，但是效果很不错。最关键的是只使用了MLP，便于部署。同时要注意，该模型训练使用了TPU3.0，外加众多training skills，model size也很大，不是个人可以玩儿转的。</p>
<span id="more"></span>
<blockquote>
<p>Convolutional Neural Networks (CNNs) are the go-to model for computer vision. Recently, attention-based networks, such as the Vision Transformer, have also become popular. In this paper we show that while convolutions and attention are both sufﬁcient for good performance, neither of them are necessary. <strong>We present MLP-Mixer, an architecture based exclusively on multi-layer perceptrons (MLPs).</strong> MLP-Mixer contains two types of layers: one with MLPs applied independently to image patches (i.e. “mixing” the per-location features), and one with MLPs applied across patches (i.e. “mixing” spatial information). When trained on large datasets, or with modern regularization schemes, MLP-Mixer attains competitive scores on image classiﬁcation benchmarks, with pre-training and inference cost comparable to state-of-the-art models. We hope that these results spark further research beyond the realms of well established CNNs and Transformers.1</p>
</blockquote>
<h2 id="introduction">1 Introduction</h2>
<p>CNN是CV领域目前de-facto标准，近期的transformers-like的模型Vision Transformers (ViT)，也得到了SOTA的结果。</p>
<h2 id="mixer-architecture">2 Mixer Architecture</h2>
<p>本文提出了MLP-Mixer，完全使用MLP的架构。看一下整体结构：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210507150754155.png" style="zoom:50%;" /></p>
<p>接收的输入固定size的，将输入划分为一系列的patch，然后所有的patch经过一个投影层，进入核心的Mixer层。</p>
<p>对于Mixer层，由两个不同的MLP组成，加上layer norm和GELU</p>
<ul>
<li><p>token-mixing：不同patch的相同维度，应该如何产生输出</p>
<blockquote>
<p>The token-mixing MLPs allow communication between different spatial locations (tokens); they operate on each channel independently and take individual columns of the table as inputs.</p>
</blockquote></li>
<li><p>channel-mising：相同patch不同channel，应该如何产生输出</p>
<blockquote>
<p>The channel-mixing MLPs allow communication between different channels; they operate on each token independently and take individual rows of the table as inputs.</p>
</blockquote></li>
</ul>
<p>写成公式：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210507145147282.png" style="zoom:50%;" /></p>
<p>两个MLP的操作区别就是对于输入简单的转置操作。</p>
<p>另外，MLP-Mixer保持MLP的输出dim都是固定的。</p>
<ul>
<li>channel-mixing MLPs对于不同patch的处理是一样的，这提供了位置不变性，it provides positional invariance, a prominent feature of convolutions.</li>
<li>token-mixing MLPs是跨patch，对于所有的channel使用相同的kernel，这点是和很多CNN模型有区别的。它带来的一个好处是不会随着channel的增加而增加参数量。另外，token-mixing MLPs是对输入patch的位置有感知的。一个patch的位置发生变化，对于channel-mixing来说，没有区别。但是对于token-mixing来说，patch位置变动在参数不变的情况下，会导致输出会发生变化，最终参数也会更新。所以它隐式的可以学习位置的表示。</li>
</ul>
<h2 id="experiments">3 Experiments</h2>
<p>没有细看，很多细节都不了解，看不懂。</p>
<p>总结一下：</p>
<ol type="1">
<li>至少8层起步的MLP，也就是至少16层全连接；另外，MLP的size都是很大的，256size起步</li>
<li>使用了非常多的训练技巧！！</li>
<li>使用了谷歌的TPU 3.0，1小时8$</li>
<li>先预训练，再fine-tuning，但没看过CV paper，不理解为什么这么做</li>
</ol>
]]></content>
      <categories>
        <category>Paper</category>
        <category>ML</category>
      </categories>
  </entry>
  <entry>
    <title>ResNet</title>
    <url>/cv/ResNet/</url>
    <content><![CDATA[<h1 id="deep-residual-learning-for-image-recognition">Deep Residual Learning for Image Recognition</h1>
<p>深度残差网络，将CNN拓展到152层乃至更深层，同时表现出更好效果的里程碑文章。核心是将residual connection代入到深层CNN中，使得深层的模型效果不比浅层的模型效果差。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220324174625698.png" alt="image-20220324174625698" style="zoom:50%;" /></p>
<span id="more"></span>
<h2 id="introduction">Introduction</h2>
<h3 id="problem">Problem</h3>
<p>作者认为在CNN中，搭建deep的model能够捕获更高level的特征，最后表现出更好的效果。但是如果model越来越深，导致的问题就是可能出现梯度消失或者梯度爆炸的问题。这个问题被初始值正则化和正则化层所解决。</p>
<p>但是没有解决另外的一个问题，那就是深度的model效果比浅层的model效果还要差，出现了degradation问题，如图所示。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220324175027867.png"  style="zoom:50%;" /></p>
<h3 id="solution">Solution</h3>
<p>作者提出，理论上来看，一个更深的模型不应该比一个浅层的模型效果差。因为一个深层的模型，如果所有深的layer的操作都是identity mapping，即什么都不做，那么效果至少应该和浅层的模型效果相近。</p>
<p>但实际不是这样，因为单纯的非线性层，可能比较难训练出这样的identity mapping。虽然理论上多层感知器都能够逼近任意的函数，但是学习的困难程度可能是不一样的，可能需要某种人为的引导降低训练难度。</p>
<p>因此，作者提出对于深层，不再直接学习理想的映射<span class="math inline">\(\mathcal{H}(\mathbf{x})\)</span>，<span class="math inline">\(\mathbf{x}\)</span>表示浅层的输出，而是让深层学习理想的映射<span class="math inline">\(\mathcal{H}(\mathbf{x})\)</span>减去<span class="math inline">\(\mathbf{x}\)</span>之后的残差<span class="math inline">\(\mathcal{F}(\mathbf{x})\)</span>： <span class="math display">\[
\mathcal{F}(\mathbf{x})=\mathcal{H}(\mathbf{x})-\mathbf{x}
\]</span> 这样，就可以反向获得<span class="math inline">\(\mathcal{H}(\mathbf{x})\)</span>，让<span class="math inline">\(\mathcal{H}(\mathbf{x})\)</span>再输入到后续的层级 <span class="math display">\[
\mathcal{H}(\mathbf{x}) = \mathcal{F}(\mathbf{x})+\mathbf{x}
\]</span> 也就是下面的图：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220324174625698.png" alt="image-20220324174625698" style="zoom:50%;" /></p>
<p>这里作者使用了一个shortcut connection，这并不是什么特别的操作，比如之前在Highway network中，也有shortcut connection，但是Highway network中的shortcut connection是gate-based，本身是有自己的参数的。</p>
<p>但是作者定义这个shortcut connection是一个identity mapping：如果channel相同，直接相加；如果channel不同，可以通过1x1卷积或者padding 0或者线性转换，之后再相加。</p>
<h2 id="explanation">Explanation</h2>
<p>为什么ResNet能够训练深层的模型，最后效果更好呢？作者在原论文中没有提出详细的说明和解释，下面是<a href="https://www.bilibili.com/video/BV1P3411y7nn/?spm_id_from=333.788">李沐大神的课程</a>中的解释，主要是因为能够更好的传递梯度：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220324181048718.png" alt="image-20220324181048718" style="zoom:30%;" /></p>
<p>可以看到，上面的式子中，原来的梯度是链式的相乘，后面的梯度额外增加了一个之前浅层<span class="math inline">\(\mathcal{g}{(\mathbf{x})}\)</span>对<span class="math inline">\(\mathbf{x}\)</span>的梯度，这样最后计算得到的梯度更大。因为一般情况下得到的梯度都是在0左右绝对值较小的值。</p>
<h2 id="deep-residual-learning">Deep Residual Learning</h2>
<p>先来看一下ResNet的结构细节：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220324180551759.png" alt="image-20220324180551759" style="zoom:50%;" /></p>
<p>这里需要注意的一点是，ResNet在layer到了50层的时候，每一层使用了bottleneck的设计，先使用1X1的卷积压缩通道数，然后再3X3卷积，最后再1X1卷积回来，目的是为了减小运算复杂度：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220324180751368.png" alt="image-20220324180751368" style="zoom:40%;" /></p>
<p>最后直接看一下模型结构图：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220324180457763.png" alt="image-20220324180457763" style="zoom:50%;" /></p>
]]></content>
      <categories>
        <category>Paper</category>
      </categories>
  </entry>
  <entry>
    <title>10-graph</title>
    <url>/algorithm-note/10-graph/</url>
    <content><![CDATA[<h1 id="第10章-图算法专题">第10章 图算法专题</h1>
<p>《算法笔记》第10章。</p>
<span id="more"></span>
<h2 id="图的基本定义">图的基本定义</h2>
<p>顶点（vertex）、边（edge）、出度、入度等不再赘述。</p>
<h2 id="图的存储">图的存储</h2>
<p>这里从传统算法的角度讨论图的存储，两个基本办法：邻接矩阵和邻接表</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 邻接矩阵</span></span><br><span class="line"><span class="keyword">int</span> maxv = <span class="number">1000</span>;</span><br><span class="line"><span class="keyword">int</span> G[maxv][maxv]; <span class="comment">// 可使用1表示连通，适用于顶点数量较少，一般少于1000顶点的情况</span></span><br><span class="line"><span class="comment">// 邻接表</span></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; G[n]; <span class="comment">// n是顶点数量，每个数组元素是一个vector</span></span><br><span class="line"><span class="comment">// 如果要储存边权</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Node</span> &#123;</span></span><br><span class="line">  <span class="keyword">int</span> v, dis;</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">vector</span>&lt;Node&gt; G[n];</span><br></pre></td></tr></table></figure>
<h2 id="图的遍历">图的遍历</h2>
<p>和前面在tree中讨论了很多次的一样，图的遍历同样是DFS和BFS，最大的区别在于</p>
<ul>
<li>图不一定是连通的，可能存在多个连通分量，因此需要从每个顶点出发尝试遍历，并且不断记录已经访问过的顶点</li>
<li>遍历了所有顶点，不代表已经遍历了所有边。这一点需要特别注意</li>
</ul>
<p>下面写出BFS和DFS的代码，以邻接表为例</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// DFS遍历grpah</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">DFS</span><span class="params">(<span class="keyword">int</span> u, <span class="keyword">int</span> &amp;stat_res)</span> </span>&#123;</span><br><span class="line">  vis[u] = ture; <span class="comment">// 记当前顶点已访问</span></span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i&lt;G[u].size(); ++i) &#123;</span><br><span class="line">    <span class="keyword">if</span>(vis[G[u][i]]==<span class="literal">false</span>) &#123;</span><br><span class="line">      <span class="comment">// 如果新的下一级顶点还未访问，则DFS</span></span><br><span class="line">      DFS(G[u][i], stat_res);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">DFSGraph</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> u = <span class="number">0</span>; u&lt;n; ++u) &#123;</span><br><span class="line">    <span class="keyword">if</span>(vis[u]==<span class="literal">false</span>) &#123;</span><br><span class="line">      <span class="keyword">int</span> stat_res = <span class="number">0</span>; <span class="comment">// 某些可能的统计数据</span></span><br><span class="line">      DFS(u, stat_res);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// BFS遍历graph</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">BFS</span><span class="params">(<span class="keyword">int</span> u)</span> </span>&#123;</span><br><span class="line">  <span class="built_in">queue</span>&lt;<span class="keyword">int</span>&gt; q;</span><br><span class="line">  vis[u] = ture;</span><br><span class="line">  q.push(q);</span><br><span class="line">  <span class="keyword">while</span>(!q.empty()) &#123;</span><br><span class="line">    <span class="keyword">int</span> u = q.top();</span><br><span class="line">    q.pop();</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;G[u].size();++i) &#123;</span><br><span class="line">      <span class="keyword">if</span>(vis[G[u][i]]==<span class="literal">false</span>)</span><br><span class="line">      	q.push(G[u][i]);</span><br><span class="line">      	vis[G[u][i]] = ture;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">BFSGraph</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> u = <span class="number">0</span>; u&lt;n; ++u) &#123;</span><br><span class="line">    <span class="keyword">if</span>(vis[u]==<span class="literal">false</span>) &#123;</span><br><span class="line">      BFS(u);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="最短路径">最短路径</h2>
<h3 id="dijkstra算法">Dijkstra算法</h3>
<p>求最短路径的经典算法，该算法能够从某个起点出发，寻找到其它所有顶点的最短路径。</p>
<p>基本思想：从还没有到达的剩余顶点中，选择一个最短距离的顶点，访问它；然后检查如果从这个新访问的顶点出发，看能否让剩余未到达的顶点的最短距离变小，如果可以就更新剩余顶点的最短距离；持续执行上一步，知道所有顶点都访问完毕。</p>
<p>实现时候的几个核心思路：</p>
<ul>
<li>一个检查是否已经访问过的数组<code>bool vis[maxv]</code>，初始化时<code>false</code></li>
<li>一个存储到不同顶点最短路径的数据<code>int d[maxv]</code>，初始化为<code>INF</code>，一个巨大的数字，可以是<code>e9</code>；结合<code>vis[maxv]</code>和<code>d[maxv]</code>就可以选出所有未到达顶点中具有最短路径的那个顶点</li>
</ul>
<p>邻接矩阵版本的dijkstra算法：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> INF = <span class="number">1000000000</span>;</span><br><span class="line"><span class="comment">// s是开始的起点编号</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Disjkstra</span><span class="params">(<span class="keyword">int</span> s)</span> </span>&#123;</span><br><span class="line">  fill(d, d+n, INF);</span><br><span class="line">  fill(vis, vis+n, <span class="literal">false</span>);</span><br><span class="line">  d[s] = <span class="number">0</span>; <span class="comment">// 开始顶点距离为0</span></span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i&lt;n; ++i) &#123; <span class="comment">// 开始访问n个顶点</span></span><br><span class="line">    <span class="keyword">int</span> u = <span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">int</span> MIN = INF;</span><br><span class="line">    <span class="comment">// 寻找还未访问的顶点中有最短路径的顶点</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> v=<span class="number">0</span>; v&lt;n; ++v) &#123;</span><br><span class="line">      <span class="keyword">if</span>(vis[v]==<span class="literal">false</span> &amp;&amp; d[v]&lt;MIN) &#123;</span><br><span class="line">        u = v;</span><br><span class="line">        MIN = d[v];</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(u==<span class="number">-1</span>) <span class="keyword">return</span>; <span class="comment">// 已经没有可以访问的顶点了，返回</span></span><br><span class="line">    vis[u] = ture; <span class="comment">// 访问节点u</span></span><br><span class="line">    <span class="comment">// 开始检查从u出发，能否让还未访问的顶点最短路径减小</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> v=<span class="number">0</span>; v&lt;n; ++v) &#123;</span><br><span class="line">      <span class="keyword">if</span>(vis[v]==<span class="literal">false</span> &amp;&amp; G[u][v]!=<span class="number">-1</span> &amp;&amp; d[v]&gt;d[u] + G[u][v]) &#123;</span><br><span class="line">        d[v] = d[u] + G[u][v]; <span class="comment">// 更新最短路径</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面的函数执行完毕后，<code>d[maxv]</code>中将保存所有最短路径距离。</p>
<p>接下来讨论，如何输出最短路径？</p>
<p>解决方法是记录每个顶点最短路径的前驱结点即可，开始结点的最短路径是自身</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> pre[maxv]; <span class="comment">// 记录前驱</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Disjkstra</span><span class="params">(<span class="keyword">int</span> s)</span> </span>&#123;</span><br><span class="line">  fill(d, d+n, INF);</span><br><span class="line">  fill(vis, vis+n, <span class="literal">false</span>);</span><br><span class="line">  d[s] = <span class="number">0</span>; <span class="comment">// 开始顶点距离为0</span></span><br><span class="line">  pre[s] = s; <span class="comment">// 开始结点的前驱是自身</span></span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i&lt;n; ++i) &#123; <span class="comment">// 开始访问n个顶点</span></span><br><span class="line">    <span class="keyword">int</span> u = <span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">int</span> MIN = INF;</span><br><span class="line">    <span class="comment">// 寻找还未访问的顶点中有最短路径的顶点</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> v=<span class="number">0</span>; v&lt;n; ++v) &#123;</span><br><span class="line">      <span class="keyword">if</span>(vis[v]==<span class="literal">false</span> &amp;&amp; d[v]&lt;MIN) &#123;</span><br><span class="line">        u = v;</span><br><span class="line">        MIN = d[v];</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(u==<span class="number">-1</span>) <span class="keyword">return</span>; <span class="comment">// 已经没有可以访问的顶点了，返回</span></span><br><span class="line">    vis[u] = ture; <span class="comment">// 访问节点u</span></span><br><span class="line">    <span class="comment">// 开始检查从u出发，能否让还未访问的顶点最短路径减小</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> v=<span class="number">0</span>; v&lt;n; ++v) &#123;</span><br><span class="line">      <span class="keyword">if</span>(vis[v]==<span class="literal">false</span> &amp;&amp; G[u][v]!=<span class="number">-1</span> &amp;&amp; d[v]&gt;d[u] + G[u][v]) &#123;</span><br><span class="line">        d[v] = d[u] + G[u][v]; <span class="comment">// 更新最短路径</span></span><br><span class="line">        pre[v] = u; <span class="comment">// 更新前驱</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>然后通过递归就可以输出最短路径</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">DFSPath</span><span class="params">(<span class="keyword">int</span> s, <span class="keyword">int</span> u)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span>(s==u) &#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;%d &quot;</span>, s);</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  DFSPath(s, pre[u]);</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;%d &quot;</span>, u);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>当然在做题的时候，不会只有这么简单的要求，通常会有更多的要求，比如要求选择在最短路径中花费最少的一条，要求输出最短路径的数量等等。</p>
<p>下面是三种常见的应对策略：</p>
<ul>
<li>给每条边新增边权，然后要求在多个最短路径中选择新增边权最好的情况</li>
</ul>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 新增的边权就类似于graph进行存储，同时用另一个新数组记录第二边权访问各个顶点时的情况</span></span><br><span class="line"><span class="comment">// 下面是核心代码</span></span><br><span class="line"><span class="comment">// 开始检查从u出发，能否让还未访问的顶点最短路径减小</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> v=<span class="number">0</span>; v&lt;n; ++v) &#123;</span><br><span class="line">  <span class="keyword">if</span>(vis[v]==<span class="literal">false</span> &amp;&amp; G[u][v]!=<span class="number">-1</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (d[v]&gt;d[u] + G[u][v]) &#123;</span><br><span class="line">      d[v] = d[u] + G[u][v]; <span class="comment">// 更新最短路径</span></span><br><span class="line">      pre[v] = u;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (d[v] == d[u] + G[u]) &#123;</span><br><span class="line">      <span class="comment">// 第二边权的更新，在最短路径不变的情况下选择最好的第二边权</span></span><br><span class="line">      <span class="comment">// 这里的cost代表路径的花费</span></span><br><span class="line">      <span class="keyword">if</span>(c[v] &gt; cost[u][v] + c[u]) &#123;</span><br><span class="line">				c[v] = cost[u][v] + c[u];</span><br><span class="line">        pre[v] = u;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>每个点新增了点权，要求在最短路径中，寻找点权最优的情况，类似于上面的方法</li>
</ul>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> v=<span class="number">0</span>; v&lt;n; ++v) &#123;</span><br><span class="line">  <span class="keyword">if</span>(vis[v]==<span class="literal">false</span> &amp;&amp; G[u][v]!=<span class="number">-1</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (d[v] &gt; d[u] + G[u][v]) &#123;</span><br><span class="line">      d[v] = d[u] + G[u][v]; <span class="comment">// 更新最短路径</span></span><br><span class="line">      pre[v] = u;</span><br><span class="line">      w[v] = w[u] + weight[v];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (d[v] == d[u] + G[u]) &#123;</span><br><span class="line">      <span class="comment">// 点权的更新，在最短路径不变的情况下选择最好的点权</span></span><br><span class="line">      <span class="comment">// 这里希望点权w[v]越大越好</span></span><br><span class="line">      <span class="keyword">if</span>(w[v] &lt; w[u] + weight[v]) &#123;</span><br><span class="line">				w[v] = w[u] + weight[v];</span><br><span class="line">        pre[v] = u;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>求最短路径的数量，使用一个数组，记录最短路径数量即可</li>
</ul>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> v=<span class="number">0</span>; v&lt;n; ++v) &#123;</span><br><span class="line">  <span class="keyword">if</span>(vis[v]==<span class="literal">false</span> &amp;&amp; G[u][v]!=<span class="number">-1</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (d[v] &gt; d[u] + G[u][v]) &#123;</span><br><span class="line">      d[v] = d[u] + G[u][v]; <span class="comment">// 更新最短路径</span></span><br><span class="line">      pre[v] = u;</span><br><span class="line">      nums[v] = nums[u] <span class="comment">// 到达顶点v的最短路径数量与达到顶点u一样</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (d[v] == d[u] + G[u]) &#123;</span><br><span class="line">      <span class="comment">// 说明此时从顶点u出发也可以最短路径的到达顶点v</span></span><br><span class="line">      nums[v] += nums[u];</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在上面的方法中，总是只保留最优的最短路径，这种情况不一定适用于所有的情形。下面介绍一种方法，总是先保留所有的最短路径，然后再从所有的最短路径中进行选择。</p>
<p>核心方法是，不再只保留一个前驱结点，而是保留所有的最短路径的前驱结点</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; pre[maxv];</span><br></pre></td></tr></table></figure>
<p>新的方法</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Disjkstra</span><span class="params">(<span class="keyword">int</span> s)</span> </span>&#123;</span><br><span class="line">  fill(d, d+n, INF);</span><br><span class="line">  fill(vis, vis+n, <span class="literal">false</span>);</span><br><span class="line">  d[s] = <span class="number">0</span>; <span class="comment">// 开始顶点距离为0</span></span><br><span class="line">  pre[s].clear();</span><br><span class="line">  pre[s].push_back(s); <span class="comment">// 开始顶点的前驱是自身</span></span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i&lt;n; ++i) &#123; <span class="comment">// 开始访问n个顶点</span></span><br><span class="line">    <span class="keyword">int</span> u = <span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">int</span> MIN = INF;</span><br><span class="line">    <span class="comment">// 寻找还未访问的顶点中有最短路径的顶点</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> v=<span class="number">0</span>; v&lt;n; ++v) &#123;</span><br><span class="line">      <span class="keyword">if</span>(vis[v]==<span class="literal">false</span> &amp;&amp; d[v]&lt;MIN) &#123;</span><br><span class="line">        u = v;</span><br><span class="line">        MIN = d[v];</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(u==<span class="number">-1</span>) <span class="keyword">return</span>; <span class="comment">// 已经没有可以访问的顶点了，返回</span></span><br><span class="line">    vis[u] = ture; <span class="comment">// 访问节点u</span></span><br><span class="line">    <span class="comment">// 开始检查从u出发，能否让还未访问的顶点最短路径减小</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> v=<span class="number">0</span>; v&lt;n; ++v) &#123;</span><br><span class="line">      <span class="keyword">if</span>(vis[v]==<span class="literal">false</span> &amp;&amp; G[u][v]!=<span class="number">-1</span>) &#123;</span><br><span class="line">        <span class="keyword">if</span>(d[v]&gt;d[u] + G[u][v]) &#123;</span><br><span class="line">          d[v] = d[u] + G[u][v]; <span class="comment">// 更新最短路径</span></span><br><span class="line">          <span class="comment">// 更新前驱</span></span><br><span class="line">          pre[v].clear();</span><br><span class="line">          pre[v].push_back(u);</span><br><span class="line">        &#125; <span class="keyword">else</span> <span class="keyword">if</span>(d[v]&gt;d[u] + G[u][v]) &#123;</span><br><span class="line">          pre[v].push_back(u); <span class="comment">// 记录新的可能的最短路径的前驱</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>遍历所有的最短路径：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">DFSPath</span><span class="params">(<span class="keyword">int</span> s, <span class="keyword">int</span> u, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;tmpPath, <span class="keyword">int</span> &amp;optValue, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; &amp;optPath)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span>(u==s) &#123;</span><br><span class="line">    tmpPath.push_back(u);</span><br><span class="line">    <span class="keyword">int</span> value;</span><br><span class="line">    <span class="keyword">if</span>(当前最短路径的value优于optvalue) &#123;</span><br><span class="line">      optValue = value;</span><br><span class="line">      optPath = tmpPath;</span><br><span class="line">    &#125;</span><br><span class="line">    tmpPath.pop_back();</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  tmpPath.push_back(u); <span class="comment">// 当前路径加入新的结点</span></span><br><span class="line">  <span class="comment">// 开始遍历所有的前驱结点</span></span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i =<span class="number">0</span>;i&lt;pre[u].size();++i) &#123;</span><br><span class="line">    DFSPath(s, pre[i], tmpPath, optValue, optPath);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 退出当前结点，返回上一级</span></span><br><span class="line">  tmpPath.pop_back();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="bellman-ford算法和spfa算法">Bellman-Ford算法和SPFA算法</h3>
<p>在dijkstra算法中，如果遇到图有负权边，由于该算法会直接选择该负边，而忽略了其它可能的路径，可能造成某些通过非负权边可以访问到的顶点没有被访问到。它无法较好的处理负权边。</p>
<p>对于以上问题，同样是针对单源最短路径问题，有bellman-ford算法，以及其改进版本SPFA算法可以解决。</p>
<p>Bellman-ford算法的基本思想：</p>
<ul>
<li>对图中的每个边进行<code>V-1</code>轮的检查。在每一轮的检查中，如果发现通过边<code>[u][v]</code>，可以让顶点<code>v</code>的最短路径缩短，就进行替换，这一点类似于Dijkstra算法，区别在于Bellman算法是遍历每条边，保证所有的边都会参与判定过程。进行<code>V-1</code>轮检查的原因是，某个结点到开始顶点的最短路径长度不会超过<code>V</code>（包括开始顶点），如果不考虑都有的开始顶点，只需要最多<code>V-1</code>步就可以到达任意连通的结点。</li>
<li>之后，再进行一轮检查，如果发现还有某个边，可以更新当前的最短路径，可以判定图中存在源点可达的负环（也就是循环一轮后，发现总的边权减少了）。请注意，这样无法判定图中是否有源点不可达的负环。</li>
</ul>
<p>以邻接表为例的代码：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 返回ture表示无源点s可达的负环</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">bellman</span><span class="params">(<span class="keyword">int</span> s)</span> </span>&#123;</span><br><span class="line">  d[s] = <span class="number">0</span>;</span><br><span class="line">  <span class="comment">// 开始n-1轮检查</span></span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n<span class="number">-1</span>; ++i) &#123;</span><br><span class="line">    <span class="comment">// 开始遍历所有边</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> u = <span class="number">0</span>; u &lt; n; ++u) &#123;</span><br><span class="line">      <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; Adj[u].size(); ++j) &#123;</span><br><span class="line">        <span class="keyword">int</span> v = Adj[u][j].v;</span><br><span class="line">        <span class="keyword">int</span> dis = Adj[u][j].dis;</span><br><span class="line">        <span class="keyword">if</span>(d[v] &gt; d[u] + dis) &#123;</span><br><span class="line">          d[v] = d[u] + dis;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 开始判断是否有源点可达的负环</span></span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> u = <span class="number">0</span>; u &lt; n; ++u) &#123;</span><br><span class="line">      <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; Adj[u].size(); ++j) &#123;</span><br><span class="line">        <span class="keyword">int</span> v = Adj[u][j].v;</span><br><span class="line">        <span class="keyword">int</span> dis = Adj[u][j].dis;</span><br><span class="line">        <span class="keyword">if</span>(d[v] &gt; d[u] + dis) &#123;</span><br><span class="line">          <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">   &#125;</span><br><span class="line">	<span class="keyword">return</span> ture;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上述算法每次要遍历所有的边，实际上只有最短路径<code>d</code>发生变化的顶点出发的边才需要进行判断，因此可以使用一个队列存储所有最短路径发生变化的顶点，出队后，再把发生最短路径变化且不再队列中的顶点入队。如果发现队空了，可以判断没有可达的负环；如果有某个顶点入队次数超过了<code>V</code>（也就是最短路径发生变化超过了<code>V</code>次），可以判断存在可达的负环。</p>
<p>经过上述改进后的算法就叫做SPFA算法（Shortest Path Faster Algorithm），该算法在大多数的图中都非常高效。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 返回ture表示无源点s可达的负环</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">SPFA</span><span class="params">(<span class="keyword">int</span> s)</span> </span>&#123;</span><br><span class="line">  <span class="built_in">queue</span>&lt;<span class="keyword">int</span>&gt; q;</span><br><span class="line">  q.push(s);</span><br><span class="line">  <span class="keyword">bool</span> inqueue[n]=&#123;<span class="literal">false</span>&#125;;</span><br><span class="line">  <span class="keyword">int</span> inqueueNum[n]=&#123;<span class="number">0</span>&#125;;</span><br><span class="line">  inqueue[s] = <span class="literal">true</span>;</span><br><span class="line">  d[s] = <span class="number">0</span>;</span><br><span class="line">  inqueueNum[s] = <span class="number">1</span>;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">while</span>(!q.empty()) &#123;</span><br><span class="line">    <span class="keyword">int</span> u = q.top();</span><br><span class="line">    q.pop();</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; Adj[u].size(); ++j) &#123;</span><br><span class="line">        <span class="keyword">int</span> v = Adj[u][j].v;</span><br><span class="line">        <span class="keyword">int</span> dis = Adj[u][j].dis;</span><br><span class="line">        <span class="keyword">if</span>(d[v] &gt; d[u] + dis) &#123;</span><br><span class="line">          d[v] = d[u] + dis;</span><br><span class="line">          <span class="comment">// 顶点v的最短路径发生变化</span></span><br><span class="line">          <span class="keyword">if</span>(inqueue[v]==<span class="literal">false</span>) &#123;</span><br><span class="line">            q.push(v);</span><br><span class="line">            inqueue[v] = <span class="literal">true</span>;</span><br><span class="line">            inqueueNum[v]++;</span><br><span class="line">            <span class="keyword">if</span>(inqueueNum[v]&gt;=n) <span class="comment">// 入队次数超过或者达到了n</span></span><br><span class="line">              <span class="keyword">return</span> <span class="literal">false</span>; </span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> ture;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="floyd算法">Floyd算法</h3>
<p>Floyd可以解决全源最短路径的问题，也就是说询问任意两个点之间的最短距离，该问题就限制了问题可以查询的顶点数量在200以内，所以总是可以使用邻接矩阵的方法解决。核心思想是，如果顶点<code>k</code>为中介时，可以使得顶点<code>i</code>到顶点<code>j</code>的距离缩短，就使用顶点<code>k</code>为中介。</p>
<p>代码：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Floyd</span><span class="params">(<span class="keyword">int</span> s)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> k =<span class="number">0</span>; k&lt;n; ++k) &#123;</span><br><span class="line">    <span class="comment">// 开始遍历所有顶点组合</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i =<span class="number">0</span>; i&lt;n; ++i) &#123;</span><br><span class="line">      <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>; j&lt;n; ++j) &#123;</span><br><span class="line">        <span class="keyword">if</span>(dis[i][k]!=INF &amp;&amp; dis[k][j]!=INF &amp;&amp; dis[i][k]+dis[k][j]&lt;dis[i][j]) &#123;</span><br><span class="line">          dis[i][j] = dis[i][k]+dis[k][j];</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="最小生成树">最小生成树</h2>
<p>最小生成树是从一个无向图当中，获取一课树，这棵树满足</p>
<ul>
<li>包括了所有的图顶点</li>
<li>所有的边都是图中原有的边</li>
<li>该树的边权和最小</li>
</ul>
<p>由于是一棵树，所以最小生成树一定有<code>V-1</code>条边。最小生成树的根结点可以是任意的结点（试想下一棵树，如果没有特殊的性质，我们当然可以把任意一个数结点当做是根结点，然后重新排列成树的层级形状）。当然在题目中，一般会指定要从哪个结点出发生成最小生成树。</p>
<h3 id="prime算法">Prime算法</h3>
<p>prime算法和dijkstra算法很相似，区别在于prime选择下一步访问的图顶点时不是考虑到起源结点最短距离，而是到整个已访问结点集合的最短距离（具体的说，访问新顶点，检查下新访问顶点到未访问顶点的距离，看能否让距离减小，不需要考虑之前新访问顶点的最短距离）。</p>
<p>下面写一下邻接表版本的prime算法：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> INF = <span class="number">1000000000</span>;</span><br><span class="line"><span class="keyword">bool</span> vis[maxn];</span><br><span class="line"><span class="keyword">int</span> dis[maxn];</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">prime</span><span class="params">(<span class="keyword">int</span> s)</span> </span>&#123;</span><br><span class="line">  fill(vis, vis+n, <span class="literal">false</span>);</span><br><span class="line">  fill(dis, dis+n, INF);</span><br><span class="line">  dis[s] = <span class="number">0</span>; <span class="comment">// 起源顶点的最短距离设置为1</span></span><br><span class="line">  <span class="keyword">int</span> ans = <span class="number">0</span>; <span class="comment">// 记录生成树的边权和</span></span><br><span class="line">  <span class="comment">// 开始访问所有顶点，总共访问n次</span></span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; ++i) &#123;</span><br><span class="line">    <span class="keyword">int</span> MINDIS = INF;</span><br><span class="line">    <span class="keyword">int</span> u = <span class="number">-1</span>;</span><br><span class="line">    <span class="comment">// 下面这段代码可以使用小顶堆或者优先队列维护，就无须总是遍历所有的顶点了</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> v = <span class="number">0</span>; v&lt;n; ++v) &#123;</span><br><span class="line">			<span class="keyword">if</span>(vis[v]==<span class="literal">false</span> &amp;&amp; dis[v]&lt;MINDIS) &#123;</span><br><span class="line">        <span class="comment">// 寻找当前最短距离最小的顶点</span></span><br><span class="line">        MINDIS = dis[v];</span><br><span class="line">        u = v;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(u==<span class="number">-1</span>) <span class="keyword">return</span> ans;</span><br><span class="line">    vis[u] = <span class="literal">true</span>; <span class="comment">// 访问顶点u</span></span><br><span class="line">    ans += dis[u]; <span class="comment">// 累积边权和</span></span><br><span class="line">    <span class="comment">// 开始更新未访问顶点的最短距离</span></span><br><span class="line">    <span class="comment">// 检查顶点u的相连顶点</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j&lt;Adj[u].size(); ++j) &#123;</span><br><span class="line">      <span class="keyword">int</span> v = Adj[u][j].v;</span><br><span class="line">      <span class="keyword">int</span> dis_uv = Adj[u][j].dis;</span><br><span class="line">      <span class="comment">// 如果顶点v未访问，并且距离顶点u的边权更小</span></span><br><span class="line">      <span class="comment">// 这一行是prime区别于dijkstra的核心，不考虑之前顶点u的最短距离</span></span><br><span class="line">			<span class="keyword">if</span>(vis[v] == <span class="literal">false</span> &amp;&amp; dis[v] &gt; dis_uv) &#123;</span><br><span class="line">        dis[v] = dis_uv;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> ans;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="kruskal算法">kruskal算法</h3>
<p>Kruskal算法（克鲁斯卡尔算法）的思想很简单，使用边贪心的思路：</p>
<ul>
<li>按照边权从小到大排序所有边</li>
<li>认为所有图顶点一开始是独立不连通的块（并查集的初始状态）</li>
<li>遍历所有排好序的边，如果一条边的两个顶点不在同一个连通块（并查集寻找集合根结点），就加入这个边，连通两个连通块（并查集合并）；如果两个顶点已经处于同一个连通块，就略过该边</li>
<li>重复上一步直至所有边遍历完毕或者已经选择了<code>V-1</code>个边</li>
</ul>
<p>代码示意：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">edge</span> &#123;</span></span><br><span class="line">  <span class="keyword">int</span> u, v;</span><br><span class="line">  <span class="keyword">int</span> dis;</span><br><span class="line">&#125; E [maxe];</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">cmp</span><span class="params">(edge e1, edge e2)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> e1.dis &lt; e2.dis;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> father[maxn]; <span class="comment">// 记录顶点所属的连通块/集合</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">findFather</span><span class="params">(<span class="keyword">int</span> i)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> tmp_i = i;</span><br><span class="line">  <span class="keyword">while</span>(father[i]!=i) &#123;</span><br><span class="line">    i = father[i];</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 压缩并查集路径</span></span><br><span class="line">  <span class="keyword">while</span>(i!=father[tmp_i]) &#123;</span><br><span class="line">    <span class="keyword">int</span> tmp_z = tmp_i;</span><br><span class="line">    tmp_i=father[tmp_i];</span><br><span class="line">    father[tmp_z] = i; <span class="comment">// 直接指向集合的根结点</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> i;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">kruskal</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> ans = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i&lt;n; ++i) &#123;</span><br><span class="line">		father[i] = i; <span class="comment">// n个不连通块</span></span><br><span class="line">  &#125;</span><br><span class="line">  sort(E, E+edge_num, cmp);</span><br><span class="line">  <span class="keyword">int</span> u, v;</span><br><span class="line">  <span class="keyword">int</span> tree_count = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;edge_num; ++i) &#123;</span><br><span class="line">    u = E[i].u;</span><br><span class="line">    fatherU = findFather(u);</span><br><span class="line">    v = E[i].v;</span><br><span class="line">    fatherV = findFather(v);</span><br><span class="line">    dis = E[i].dis;</span><br><span class="line">	  <span class="keyword">if</span>(fatherU!=fatherV) &#123;</span><br><span class="line">      father[fatherU] = father[fatherV]; <span class="comment">// 合并两个并查集，根结点合并</span></span><br><span class="line">      ans += dis; <span class="comment">// 边加入生成树</span></span><br><span class="line">      tree_count++;</span><br><span class="line">      <span class="keyword">if</span>(tree_count==n<span class="number">-1</span>) <span class="keyword">break</span>; <span class="comment">// 如果已经找到足够的生成树边</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span>(tree_count!=n<span class="number">-1</span>) <span class="keyword">return</span> <span class="number">-1</span>; <span class="comment">// 有顶点无法连通</span></span><br><span class="line">  <span class="keyword">return</span> ans;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="拓扑排序">拓扑排序</h2>
<p>一个有向图的任意顶点都不可能通过一些有向边返回，这样的有向图叫做有向无环图。</p>
<p>检查一个有向无环图的办法可以通过检查图的拓扑排序能否包括所有的图顶点。拓扑排序是指如果在图中存在<code>u-&gt;v</code>，则<code>u</code>在拓扑排序中一定在<code>v</code>前，<code>u</code>是<code>v</code>的先导元素。</p>
<p>解决思路是，使用一个队列存储所有入度为0的顶点，出队队首元素，访问该顶点，然后删除所有以该顶点为起点的边，如果有顶点入度变为了0，就入队。重复上述过程直到队列为空。检查此时访问的元素，如果存在部分顶点为访问，则说明有向图中存在环。</p>
<p>邻接表版本的代码：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> inDegree[maxn]; <span class="comment">// 记录所有顶点的入度</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">topologicalSort</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="built_in">queue</span>&lt;<span class="keyword">int</span>&gt; q;</span><br><span class="line">  <span class="comment">// 所有入度是0的顶点入队</span></span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;n;i++) &#123;</span><br><span class="line">    <span class="keyword">if</span>(inDegree[i]==<span class="number">0</span>) &#123;</span><br><span class="line">      q.push(i);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">int</span> zeroDegreeCount = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">while</span>(!q.empty()) &#123;</span><br><span class="line">    <span class="keyword">int</span> u = q.front();</span><br><span class="line">    q.pop();</span><br><span class="line">    zeroDegreeCount++;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;%d &quot;</span>, u); <span class="comment">// 访问队顶元素，输出拓扑排序</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; Adj[u].size(); ++j) &#123;</span><br><span class="line">      <span class="keyword">int</span> v = Adj[u][j].v;</span><br><span class="line">      inDegree[v]--;</span><br><span class="line">      <span class="keyword">if</span>(inDegree[v]==<span class="number">0</span>) &#123;</span><br><span class="line">        q.push(v);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span>(zeroDegreeCount==n) <span class="keyword">return</span> <span class="literal">true</span>; <span class="comment">// 返回true，没有环</span></span><br><span class="line">  <span class="keyword">return</span> <span class="literal">false</span>; <span class="comment">// 有环</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="关键路径">关键路径</h2>
<p>AOV（Activity On Vertex）网络是顶点表示活动，顶点可以有点权，边没有边权，代表优先级。上一节的拓扑排序就是用来寻找AOV网络上的一种活动排序。</p>
<p>AOE（Activity On Edge）网络是边表示活动，顶点表示事件，顶点没有点权，边有边权。AOE通常用在工程场景中，边表示从一个事件到另一事件需要的时间/代价等。</p>
<p>AOV和AOE网络的边都表示某种优先级，因此不会存在环，都是有向无环图。</p>
<p>AOV网络总是可以转换为AOE网络，试想下只需要把AOV中的顶点拆分为两个顶点作为事件开始与结束，这两个顶点中的有向边边权就是原顶点的点权，剩下的AOV原有边边权设置为0。</p>
<p>AOE网络总是可以通过添加额外的起点和汇点，形成只有一个起点，一个汇点的图。</p>
<p>关键路径：对于AOE网络中的最长路径，叫做关键路径；关键路径上的所有活动叫做关键活动；关键路径表示要完成AOE网络中的所有活动所需要的最少时间，关键活动是无法拖延完成的活动。</p>
<p>关键路径的寻找方法，核心在于寻找顶点<code>i</code>（事件）的最早开始时间和最晚开始时间，最早开始时间是从起点开始就马不停蹄的完成所有事件，只要顶点<code>i</code>的所有先导顶点完成了，就立刻开始完成顶点<code>i</code>。顶点<code>i</code>的最晚开始时间，是从终点开始反向计算，只要不延误后续顶点的最晚开始时间就可以。</p>
<p>实现的时候，对于每个顶点，维护数组<code>ve[maxn]</code>保存顶点的最早开始时间；数组<code>vl[maxn]</code>保存顶点的最迟开始时间。计算好这两个数组之后，遍历所有的边<code>u-&gt;v</code>，计算<code>u-&gt;v</code>的最早开始时间<code>ve[u]</code>和最晚开始时间<code>vl[v]-dis[u-&gt;v]</code>。</p>
<p>按照拓扑排序，可以计算出各个顶点的最早开始时间<code>max</code>（所有先导顶点的最早时间+先导边时间）；然后按照拓扑排序的反向顺序，计算各个顶点的最晚开始时间<code>min</code>（所有后续顶点的最晚开始时间-后续边时间）。</p>
<p>代码实现：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="built_in">stack</span>&lt;<span class="keyword">int</span>&gt; topoloStack; <span class="comment">// 保存拓扑排序序列</span></span><br><span class="line"><span class="keyword">int</span> ve[maxn];</span><br><span class="line"><span class="keyword">int</span> vl[maxn];</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> inDegree[maxn];</span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">topologicalSort</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  fill(ve, ve+n, <span class="number">0</span>);</span><br><span class="line">  <span class="built_in">queue</span>&lt;<span class="keyword">int</span>&gt; q;</span><br><span class="line">  <span class="comment">// 所有入度是0的顶点入队</span></span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;n;i++) &#123;</span><br><span class="line">    <span class="keyword">if</span>(inDegree[i]==<span class="number">0</span>) &#123;</span><br><span class="line">      q.push(i);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">int</span> zeroDegreeCount = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">while</span>(!q.empty()) &#123;</span><br><span class="line">    <span class="keyword">int</span> u = q.front();</span><br><span class="line">    q.pop();</span><br><span class="line">    zeroDegreeCount++;</span><br><span class="line">    topoloStack.push(u);  <span class="comment">// 加入拓扑排序</span></span><br><span class="line">    <span class="comment">// 对于顶点u的所有后续结点，顶点u都是先导顶点</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; Adj[u].size(); ++j) &#123;</span><br><span class="line">      <span class="keyword">int</span> v = Adj[u][j].v;</span><br><span class="line">      <span class="keyword">int</span> dis = Adj[u][j].dis;</span><br><span class="line">      inDegree[v]--;</span><br><span class="line">      <span class="keyword">if</span>(inDegree[v]==<span class="number">0</span>) &#123;</span><br><span class="line">        q.push(v);</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 如果以顶点u为先导，完成活动后到达顶点v的最早开始时间更长</span></span><br><span class="line">      <span class="keyword">if</span>(ve[v] &lt; ve[u] + dis) &#123;</span><br><span class="line">        ve[v] = ve[u] + dis;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span>(zeroDegreeCount==n) <span class="keyword">return</span> <span class="literal">true</span>; <span class="comment">// 返回true，没有环</span></span><br><span class="line">  <span class="keyword">return</span> <span class="literal">false</span>; <span class="comment">// 有环</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">criticalPath</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span>(!topologicalSort()) <span class="keyword">return</span> <span class="number">-1</span>; <span class="comment">// 有环</span></span><br><span class="line">  <span class="comment">// 开始计算vl[n]</span></span><br><span class="line">  fill(vl, vl+n, ve[n<span class="number">-1</span>]); <span class="comment">// 初始化vl值为汇点的最晚开始时间（等于汇点的最早开始时间），也就是关键路径长度</span></span><br><span class="line">  <span class="keyword">while</span>(!topoloStack.empty()) &#123;</span><br><span class="line">    <span class="keyword">int</span> u = topoloStack.top();</span><br><span class="line">    topoloStack.pop();</span><br><span class="line">    <span class="comment">// 对于顶点u的所有后续结点</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; Adj[u].size(); ++j) &#123;</span><br><span class="line">      <span class="keyword">int</span> v = Adj[u][j].v;</span><br><span class="line">      <span class="keyword">int</span> dis = Adj[u][j].dis;</span><br><span class="line">      <span class="comment">// 如果到达顶点v的最晚开始时间更小</span></span><br><span class="line">      <span class="keyword">if</span>(vl[u] &gt; vl[v] - dis) &#123;</span><br><span class="line">        vl[u] = vl[v] - dis;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 开始遍历所有边，寻找关键活动，也就是不能延误开始的边</span></span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> u = <span class="number">0</span>; u &lt; n; ++u) &#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; Adj[u].size(); ++j) &#123;</span><br><span class="line">      <span class="keyword">int</span> v = Adj[u][j].v;</span><br><span class="line">      <span class="keyword">int</span> dis = Adj[u][j].dis;</span><br><span class="line">      <span class="keyword">int</span> e_edge = ve[u]; <span class="comment">// 边的最早开始时间</span></span><br><span class="line">      <span class="keyword">int</span> l_edge = vl[v] - dis; <span class="comment">// 边的最迟开始时间</span></span><br><span class="line">      <span class="keyword">if</span>(e_dege == l_edge) &#123;</span><br><span class="line">				<span class="built_in">printf</span>(<span class="string">&quot;%d-&gt;%d\n&quot;</span>, u, v);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> ve[n<span class="number">-1</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>book</tag>
      </tags>
  </entry>
  <entry>
    <title>12-string</title>
    <url>/algorithm-note/12-string/</url>
    <content><![CDATA[<h1 id="字符串专题">字符串专题</h1>
<span id="more"></span>
<h2 id="字符串hash进阶">字符串hash进阶</h2>
<p>在前面第4章讨论过对只有大小写字母的字符串的hash办法，简单说就是把字母看出26进制，然后换算成10进制 <span class="math display">\[
H[i] = H[i-1]\times26 + index(str[i])
\]</span> 为了避免结果太大，取模 <span class="math display">\[
H[i] = \left(H[i-1]\times26 + index(str[i])\right)\ \%\ mod
\]</span> 当然这可能出现hash碰撞。幸运的是，在实践中发现，如果选择合适的进制和取模除数，可以很大程度避免这个问题，一般来讲，设置进制<code>p</code>为一个<span class="math inline">\(10^7\)</span>的素数（如10000019），<code>mod</code>为<span class="math inline">\(10^9\)</span>的素数（例如1000000007），冲突概率就非常小了。 <span class="math display">\[
H[i] = \left(H[i-1]\times p + index(str[i])\right)\ \%\ mod
\]</span> 字符串的子串hash问题：如何表示<code>H[i...j]</code>？</p>
<p>思路：我们当然可以对子串同样进行相同的hash操作，但是这样会造成大量的冗余计算。我们可以利用上面计算的结果来简化操作，<code>H[j]</code>可以由<code>H[i-1]</code>一路推导下来： <span class="math display">\[
H[j] = H[i-1]\times p^{j-i+1} + H[i...j] \\
H[i...j] = H[j] - H[i-1]\times p^{j-i+1}
\]</span> 如果加入取模操作，注意<code>H[j]</code>可能会小于<code>H[i-1]xp</code>，直接取模可能得到负值，因此为了得到非负结果，结果取模后再度加一次<code>mod</code>，然后再取模，保证能够得到正值。 <span class="math display">\[
H[i...j] = ((H[j] - H[i-1]\times p^{j-i+1})\%\ mod + mod)\%\ mod
\]</span> 因此，我们可以在计算获得<code>H[i]</code>之后，按照上面的式子方便的获得所有子串的hash值，用于之后的计算。</p>
<p>可以用于计算两个字符串的最大公共子串（注意不是子序列），只需要比较两个子字符串的所有子串hash是否相同，并取最大值即可；</p>
<p>也可以用于计算字符串的最大回文子串，把原来的字符串反转，然后比较最大的hash相同的公共子串。</p>
<p>最后，如果出现了hash冲突，只需要改变下<code>p</code>和<code>mod</code>即可。甚至还可以采用双hash，也就是计算两个不同的hash值一起表示字符串的办法。</p>
<h2 id="kmp算法">KMP算法</h2>
<p>接下来讨论字符串匹配问题，KMP算法是三个发明作者的首字母。</p>
<p>思想：核心是设计一个next数组，<code>next[i]=k</code>表示字符串<code>s[0..i]</code>的最长相等前缀<code>s[0...k]</code>和后缀<code>s[i-k...i]</code>。</p>
<p>获取next数组的代码：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">getNext</span><span class="params">(<span class="keyword">char</span> s[], <span class="keyword">int</span> len)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> j = <span class="number">-1</span>;</span><br><span class="line">  next[<span class="number">0</span>] = <span class="number">-1</span>; <span class="comment">// -1表示没有匹配的前后缀</span></span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>; i&lt;len; ++i) &#123;</span><br><span class="line">    <span class="comment">// 后缀最后一位一定是s[i]，因此前缀最后一位必须匹配s[i]</span></span><br><span class="line">    <span class="keyword">while</span>(j!=<span class="number">-1</span> &amp;&amp; s[i]!=s[j+<span class="number">1</span>]) &#123;</span><br><span class="line">      j = next[j];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 如果成功匹配</span></span><br><span class="line">    <span class="keyword">if</span>(s[i]==s[j+<span class="number">1</span>]) &#123;</span><br><span class="line">      j++;</span><br><span class="line">    &#125;</span><br><span class="line">    next[i] = j; <span class="comment">// 设置最大匹配前后缀</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>KMP算法就是利用前面的next数组来实现，对于字符串<code>text</code>和模式串<code>pattern</code>。计算<code>pattern</code>的next数组，然后模仿next的计算过程不断去匹配<code>text</code>。实际上，<code>next</code>数组的求解过程，就是自身对自身的匹配。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 返回匹配的字符串个数</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">KMP</span><span class="params">(<span class="keyword">char</span> text[], <span class="keyword">char</span> pattern[])</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> n = <span class="built_in">strlen</span>(text), m = <span class="built_in">strlen</span>(pattern);</span><br><span class="line">  getNext(pattern, m);</span><br><span class="line">  <span class="keyword">int</span> j = <span class="number">-1</span>; <span class="comment">// 此时j表示在pattern上的位置</span></span><br><span class="line">  <span class="keyword">int</span> matchCount = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;n; ++i) &#123;</span><br><span class="line">    <span class="comment">// 没有办法继续匹配，就让pattern已匹配的前缀后退</span></span><br><span class="line">    <span class="keyword">while</span>(j!=<span class="number">-1</span> &amp;&amp; text[i]!=pattern[j+<span class="number">1</span>]) &#123;</span><br><span class="line">      j = next[j];</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(text[i] == pattern[j+<span class="number">1</span>]) &#123;</span><br><span class="line">      j++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 如果已经全部匹配</span></span><br><span class="line">    <span class="keyword">if</span>(j==m<span class="number">-1</span>) &#123;</span><br><span class="line">      matchCount++;</span><br><span class="line">      j = next[j]; <span class="comment">// 回退，然后继续尝试匹配</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> matchCount;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>book</tag>
      </tags>
  </entry>
  <entry>
    <title>5-math</title>
    <url>/algorithm-note/5-math/</url>
    <content><![CDATA[<h1 id="数学问题">数学问题</h1>
<p>《算法笔记》第五章 数学问题</p>
<span id="more"></span>
<h2 id="最大公约数和最小公倍数">最大公约数和最小公倍数</h2>
<p>求解最大公约数的方法一般为欧几里得法，即辗转相除法。0和任意整数的最大公约数都是整数本身。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">gcd</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y)</span> </span>&#123;</span><br><span class="line">	<span class="keyword">if</span>(y==<span class="number">0</span>) <span class="keyword">return</span> x; <span class="comment">// 保证除数不是0</span></span><br><span class="line">  <span class="keyword">return</span> gcd(y, x % y);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>最小公倍数的求解方法是在求解最大公约数的基础上进行的，最小公倍数的求解方法未<code>a*b/d</code>，<code>d</code>就是最大公约数。为了避免超出类型范围，一般为<code>a / d * b</code>。</p>
<h2 id="素数">素数</h2>
<p>素数，也叫做质数是指除了1和本身之外，不能被任意的整数整除的数字。</p>
<p>合数是指可以被除了1和本身之外的数字整除的数字。</p>
<p>特殊的是1，<em>1既不是素数，也不是合数</em>。</p>
<h3 id="判断素数">判断素数</h3>
<p>假设数字n可以被k整除：<code>n % k == 0</code>，我们当然可以遍历从2开始到n-1的所有值检查是否可以整除。</p>
<p>实际上，可以缩小判断范围，考虑平方根，<code>sqrt(n)*sqrt(n) == n</code>，所有可以整除n的第一个数字，一定是<code>&lt;= sqrt(n)</code>，因此我们只需要遍历从2开始到sqrt(n)。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">isPrime</span><span class="params">(<span class="keyword">int</span> x)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> sqr = <span class="built_in">sqrt</span>(x);</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">2</span>; i &lt;= sqr; ++i) &#123;</span><br><span class="line">    <span class="keyword">if</span>(n % i == <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="获取素数表">获取素数表</h3>
<p>我们当然可以判断每一个数字是否是素数获得素数表，但是这样效率较低。</p>
<p>可以考虑使用埃式筛选法：</p>
<p>如果一个数字<code>i</code>是素数，就把它所有的后续整倍数筛选出去。如果一个数没有被筛选出去，那它就是素数。初始化默认2是素数。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> prime[maxn];</span><br><span class="line"><span class="keyword">int</span> pNum = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">bool</span> isPrime[maxn] = &#123;<span class="literal">true</span>&#125;;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">findPrime</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">2</span>; i &lt; maxn; ++i) &#123;</span><br><span class="line">		<span class="keyword">if</span>(isPrime[i]) &#123;</span><br><span class="line">      prime[pNum++] = i;</span><br><span class="line">      <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">2</span>; i * j &lt; maxn; ++j) &#123;</span><br><span class="line">				isPrime[i * j] = <span class="literal">false</span>;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="质因子分解">质因子分解</h2>
<p>把一个数字<code>n</code>分解为多个质数的乘积形式。</p>
<p>首先通过获取质数表，我们可以获得所有的候选质数，然后判断各个质数是否是数字<code>n</code>的因子。</p>
<p>同样的，类似于判断质数的方法，数字<code>n</code>的<code>&gt;=sqrt(n)</code>的质因子至多有一个，<code>&lt;sqrt(n)</code>的质因子可以有多个，因此，我们可以先寻找所有<code>&lt;sqrt(n)</code>的质因子，之后如果发现乘积不为数字<code>n</code>，则继续计算。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">factor</span> &#123;</span></span><br><span class="line">  <span class="keyword">int</span> x, cnt; <span class="comment">// 记录质因子和个数</span></span><br><span class="line">&#125; fac[<span class="number">10</span>]; <span class="comment">// 10个最多了</span></span><br><span class="line">findPrime();</span><br><span class="line"><span class="keyword">int</span> sqr = <span class="built_in">sqrt</span>(n);</span><br><span class="line"><span class="keyword">int</span> num; <span class="comment">// 记录所有质因子的数量</span></span><br><span class="line"><span class="comment">// 遍历素数表</span></span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; pNum &amp;&amp; primes[i] &lt;= sqr; ++i) &#123;</span><br><span class="line">  <span class="keyword">if</span>(n % prime[i] == <span class="number">0</span>) &#123;</span><br><span class="line">    fac[num].x = prime[i];</span><br><span class="line">    fac[num].cnt = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span>(n % prime[i] ==<span class="number">0</span>) &#123;</span><br><span class="line">      n = n / prime[i];</span><br><span class="line">      fac[num].cnt++;</span><br><span class="line">    &#125;</span><br><span class="line">    num++;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span>(n==<span class="number">1</span>) <span class="keyword">break</span>; <span class="comment">// 已经找到所有的质因子</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(n != <span class="number">1</span>) &#123;</span><br><span class="line">  <span class="comment">// 如果还存在&gt;=sqrt(n)的质因子</span></span><br><span class="line">  fac[num].x = n;</span><br><span class="line">  fac[num].cnt = <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="大整数运算">大整数运算</h2>
<h3 id="大整数的存储">大整数的存储</h3>
<p>对于过于大的整数，比如1000位的整数，不能再使用基本类型存储，因此考虑使用结构体进行存储。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">bign</span> &#123;</span></span><br><span class="line">  <span class="keyword">int</span> d[<span class="number">1000</span>]; <span class="comment">// 保存整数的各位数字，[0]是最低位</span></span><br><span class="line">  <span class="keyword">int</span> len; <span class="comment">// 保存位数</span></span><br><span class="line">  bign() &#123;</span><br><span class="line">    <span class="built_in">memset</span>(d, <span class="number">0</span>, <span class="keyword">sizeof</span>(d)); <span class="comment">// 使用0初始化所有位，方便四则运算</span></span><br><span class="line">    len = <span class="number">0</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>需要记住我们使用从低位到高位的存储办法。因此，在读入大整数的时候，对于读入的大整数可以先考虑使用<code>char[]</code>保存，然后逆位赋值给<code>bign</code>。同理，<code>bign</code>和<code>bign</code>的比较，同样是从<code>len-1</code>开始比较。</p>
<h3 id="大整数的四则运算">大整数的四则运算</h3>
<p>大整数的加法，从最低位开始相加，如果大于10就进位（除以10），余数在当前位。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function">bign <span class="title">add</span><span class="params">(bign a, bign b)</span> </span>&#123;</span><br><span class="line">  bign c;</span><br><span class="line">  <span class="keyword">int</span> carry = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">int</span> tmp;</span><br><span class="line">  <span class="comment">// 从低位开始加起</span></span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; a.len || i &lt; b.len; ++i) &#123;</span><br><span class="line">    tmp = a.d[i] + b.d[i] + carry;</span><br><span class="line">    c.d[i] = tmp % <span class="number">10</span>;</span><br><span class="line">    c.len++;</span><br><span class="line">    carry = tmp / <span class="number">10</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span>(carry != <span class="number">0</span>) &#123; <span class="comment">// 加法的最终进位最多1位</span></span><br><span class="line">    c.d[len++] = carry;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> c;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>大整数的减法，与加法的一个很大区别是，需要首先判断两个大整数的大小，总是使用大整数减去小整数，对于结果是负数的情况额外输出负号即可。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function">bign <span class="title">sub</span><span class="params">(bign a, bign b)</span> </span>&#123;</span><br><span class="line">  bign c;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i =<span class="number">0</span>; i &lt; a.len || i &lt; b.len; ++i) &#123;</span><br><span class="line">    <span class="keyword">if</span>(a.d[i] - b.d[i] &lt; <span class="number">0</span>) &#123;</span><br><span class="line">      a.d[i + <span class="number">1</span>]--;</span><br><span class="line">      a.d[i] += <span class="number">10</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    c.d[len++] = a.d[i] - b.d[i];</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 减去高位可能存在的多个0，但是至少保留1位，例如3333332-3333331=1</span></span><br><span class="line">  <span class="keyword">while</span>(c.len &gt; <span class="number">1</span> &amp;&amp; c.d[len - <span class="number">1</span>] == <span class="number">0</span>) &#123;</span><br><span class="line">    c.len--;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> c;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>大整数与int的乘法，类似于大整数的加法，从低位到高位，将int与大整数的某一位相乘，结果加上进位，然后个位保留作为该位结果，更高位作为进位。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function">bign <span class="title">mult</span><span class="params">(bign a, <span class="keyword">int</span> b)</span> </span>&#123;</span><br><span class="line">  bign c;</span><br><span class="line">  <span class="keyword">int</span> tmp, carry = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; a.len; ++i) &#123;</span><br><span class="line">    tmp = a.d[i] * b + carry;</span><br><span class="line">    c.d[len++] = tmp % <span class="number">10</span>;</span><br><span class="line">    carry = tmp / <span class="number">10</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">while</span>(carry != <span class="number">0</span>) &#123;</span><br><span class="line">    c.d[len++] = carry % <span class="number">10</span>;</span><br><span class="line">    carry /= <span class="number">10</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> c;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>大整数与int的除法，每一步都是上一步的余数乘以10，加上当前位与除数相除，结果作为当前位，余数留到下一位。除法需要从高位开始操作。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function">bign <span class="title">divide</span><span class="params">(bign a, <span class="keyword">int</span> b)</span> </span>&#123;</span><br><span class="line">  bign c;</span><br><span class="line">  c.len = a.len; <span class="comment">// 余数的位数最多和被除数一样</span></span><br><span class="line">  <span class="keyword">int</span> tmp, carry = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i = a.len - <span class="number">1</span>; i &gt;= <span class="number">0</span>; --i) &#123;</span><br><span class="line">		tmp = carry * <span class="number">10</span> + a.d[i];</span><br><span class="line">    a.d[i] = tmp % b;</span><br><span class="line">    carry = tmp / b;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 去除高位的0</span></span><br><span class="line">  <span class="keyword">while</span>(c.len &gt; <span class="number">1</span> &amp;&amp; c.d[len - <span class="number">1</span>] == <span class="number">0</span>) &#123;</span><br><span class="line">    c.len--;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> c;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="组合数">组合数</h2>
<p>关于问题，求<code>n!</code>有几个质因子<code>p</code>？</p>
<p>可以记住一个式子，<code>n!</code>中有<span class="math inline">\(\frac{n}{p}+\frac{n}{p^2}+\frac{n}{p^3}+\dots\)</span>的质因子<code>p</code>。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">cal</span><span class="params">(<span class="keyword">int</span> n, <span class="keyword">int</span> p)</span> </span>&#123;</span><br><span class="line">	<span class="keyword">int</span> ans = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">while</span>(n) &#123;</span><br><span class="line">    ans += n / p;</span><br><span class="line">    n /= p;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> ans;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面答案的一个变形是求解<code>n!</code>会有末尾0的个数，本质上等于求有多少个2和5的组合的个数，又因为质因子2的数量多于5，因此直接求解<code>n!</code>有多少个质因子5即可。</p>
<p>求解组合数<span class="math inline">\(C_{n}^{m}\)</span>，如果直接使用公式<span class="math inline">\(\frac{n!}{m!(n-m)!}\)</span>可能超界限，可以考虑使用公式<span class="math inline">\(C_{n}^{m}=C_{n-1}^{m-1}+C_{n-1}^{m}\)</span>。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">long</span> <span class="keyword">long</span> res[<span class="number">67</span>][<span class="number">67</span>] = &#123;<span class="number">0</span>&#125;;</span><br><span class="line"><span class="function"><span class="keyword">long</span> <span class="keyword">long</span> <span class="title">C</span><span class="params">(<span class="keyword">long</span> <span class="keyword">long</span> n, <span class="keyword">long</span> <span class="keyword">long</span> m)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span>(m==<span class="number">0</span> || n==m) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span>(res[n][m] != <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> res[n][m];</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> res[n][m] = C(n<span class="number">-1</span>, m) + C(n<span class="number">-1</span>, m<span class="number">-1</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>book</tag>
      </tags>
  </entry>
  <entry>
    <title>2-basic</title>
    <url>/algorithm-note/2-basic/</url>
    <content><![CDATA[<h1 id="c语言基础要点">C语言基础要点</h1>
<p>《算法笔记》第二章 C/C++快速入门。这里记录些要点。</p>
<p>C语言常包括头文件<code>&lt;stdio.h&gt;</code>，<code>stdio</code>是标准输入输出的意思。实际上，在c++标准中，推荐使用<code>&lt;cstdio&gt;</code>，<code>cmath</code>和<code>cstring</code>等头文件，和<code>.h</code>结尾的头文件是等价的。</p>
<span id="more"></span>
<h2 id="基本数据类型">基本数据类型</h2>
<p>int的取值范围在<span class="math inline">\(10^9\)</span>范围内，long long的范围在<span class="math inline">\(10^{18}\)</span>。如果要给int定义一个表示无穷大的数，推荐取值为<span class="math inline">\(2^{30}-1\)</span>，这样能够避免相加后超出int取值范围，一般定义是<code>int INF=(1&lt;&lt;30)-1</code>。</p>
<p>浮点数float的精度是6-7位，double是15-16位，推荐一般使用double。</p>
<p>单个char赋值的时候，字符常量只有单个字符，要使用<code>''</code>单引号。字符编码记住小写字母&gt;大写字母&gt;数字，大写字母+32就是对应的小写字母ASCII码。</p>
<p>字符串可以使用char数组，<code>char str[24]</code>，输入和输出可以直接使用<code>%s</code>。字符串一定不能赋值给char。</p>
<p>条件运算符是c语言中的唯一三母运算符，<code>int c = a&gt;b ? a : b;</code>。</p>
<h2 id="scanf与printf">scanf与printf</h2>
<p>scanf和printf比c++中的cin和cout要快，在某些要求时间约束的题目中好用。</p>
<p>输入和输出long long，都使用<code>%lld</code>。字符是<code>%c</code>，输入float是<code>%f</code>，输入double是<code>%lf</code>，输入字符串是<code>%s</code>。记住<code>%c</code>可以直接获得空格和换行符，<code>%s</code>遇到空格和换行符就会停止输入。</p>
<p>scanf的格式：<code>scanf(&quot;格式控制&quot;,  变量地址)</code>。注意除了<code>%s</code>由于是数组首元素地址，其它变量都需要使用<code>&amp;</code>获得变量地址。</p>
<p>printf的格式：<code>printf(&quot;格式控制&quot;，变量名)</code>。printf输出变量时，除了double和float一样都使用<code>%f</code>即可，其它都和scanf一样。几个特殊的控制输出的格式：</p>
<ul>
<li><code>%md</code>：<code>m</code>是控制输出整型以m位右对齐，如果整型变量本身超出m位，就保持原样。</li>
<li><code>%0md</code>：对于左边不够m位的，使用<code>0</code>补齐</li>
<li><code>%.mf</code>：浮点数保留m位，保留的规则不是简单的四舍五入，而是四舍六入五成双。</li>
<li><code>%%</code>和<code>//</code>：输出<code>%</code>和<code>/</code>。</li>
</ul>
<p><code>getchar()</code>和<code>putchar(char)</code>可以方便的获取单个字符。</p>
<h2 id="常用math函数">常用math函数</h2>
<p>下面提到的math函数，输入都是<code>double</code>。</p>
<ul>
<li><code>fabs(double x)</code>：double取绝对值</li>
<li><code>floor(double x)</code>和<code>ceil(double x)</code>：向下和向上取整，注意负数的向下取整是取更负的值。</li>
<li><code>pow(double r, double p)</code>：计算<span class="math inline">\(r^p\)</span>。</li>
<li><code>sqrt(double x)</code>：返回算术平方根</li>
<li><code>log(double x)</code>：返回<span class="math inline">\(e\)</span>为底的对数值，如果要计算非<span class="math inline">\(e\)</span>为底的对数，利用换底公式<span class="math inline">\(log_ab=log_eb/log_ea\)</span>。</li>
<li><code>sin(double x), cos(), tan()</code>：输入的<code>x</code>是弧度，不是角度，注意<code>弧度=角度 x pi/180</code>。</li>
<li><code>asin(double x), acos(), atan()</code>：反三角函数，返回弧度值</li>
<li><code>round(double x)</code>：四舍五入</li>
</ul>
<h2 id="数组">数组</h2>
<h3 id="一般数组">一般数组</h3>
<p>数组的一般定义形式</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">数据类型 数组名[数量]</span><br></pre></td></tr></table></figure>
<p><em>注意数组传入函数时，如果是二维数组，需要指定第二维的长度，如果是一维数组不要求指定；在函数中修改数组元素，会直接修改原数组的值</em>。</p>
<p>定义的时候初始化：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 一维数组</span></span><br><span class="line"><span class="keyword">int</span> a[<span class="number">45</span>] = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>&#125;;</span><br><span class="line"><span class="comment">// 二维数组</span></span><br><span class="line"><span class="keyword">int</span> b[<span class="number">2</span>][<span class="number">3</span>] = &#123;&#123;<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>&#125;, &#123;&#125;, &#123;<span class="number">2</span>, <span class="number">4</span>&#125;&#125;;</span><br><span class="line"><span class="comment">// 字符数组</span></span><br><span class="line"><span class="keyword">char</span> c[<span class="number">2</span>] = &#123;<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;c&#x27;</span>&#125;;</span><br><span class="line"><span class="keyword">char</span> c[<span class="number">2</span>] = <span class="string">&quot;ac&quot;</span>; <span class="comment">// 注意只有定义的时候可以这样用&quot;&quot;定义</span></span><br></pre></td></tr></table></figure>
<p>注意，如果定义的数组较大（元素数量&gt;<span class="math inline">\(10^6\)</span>），由于函数内部的局部变量来自系统栈，允许的空间较小，需要移到函数外，函数外的全局变量来自静态存储区，允许申请的空间比较大。</p>
<p>利用函数初始化数组，有两种函数：</p>
<ul>
<li><code>memset(数组名, 值, sizeof(数组名)</code>：按照<em>字节</em>依据所给的值赋值给数组元素，因此为了避免错误，一般是使用<code>0</code>或者<code>-1</code>初始化。该函数执行速度较快。需要<code>#include&lt;string&gt;</code></li>
<li><code>fill(数组开始地址, 数组结束地址, 值)</code>：需要包括<code>#include&lt;algorithm&gt;</code>，便于利用其他值初始化数组，不会出错，执行速度更慢。</li>
</ul>
<h3 id="字符数组">字符数组</h3>
<p>字符数组的输入与输出：</p>
<ol type="1">
<li>使用<code>scanf</code>和<code>printf</code></li>
</ol>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="built_in">scanf</span>(<span class="string">&quot;%s&quot;</span>, str); <span class="comment">// 遇到空格或者换行停止</span></span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;%s&quot;</span>, str);</span><br></pre></td></tr></table></figure>
<p>注意，使用scanf输入时，编译器会自动在字符串末尾添加<code>\0</code>，因此字符串数组大小至少要比规定的大小大1。使用printf输出时，识别<code>\0</code>作为中断输出，如果没有该字符，会输出乱码。</p>
<ol start="2" type="1">
<li>使用<code>getchar</code>和<code>putchar</code></li>
</ol>
<p>输入输出单个字符，注意该方法无法自动识别字符串尾端，需要人工增加<code>\0</code>避免出错。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">char</span> c = getchar();</span><br><span class="line"><span class="built_in">putchar</span>(c);</span><br></pre></td></tr></table></figure>
<ol start="3" type="1">
<li>使用<code>gets</code>和<code>puts</code></li>
</ol>
<p>用来直接输入和输出字符串，记住<code>gets(str)</code>是以<code>\n</code>作为输入结束标志，因此<code>gets()</code>可以用来输入有空格的字符串，它也会自动添加<code>\0</code>。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">char</span> str1[<span class="number">100</span>];</span><br><span class="line">gets(str1);</span><br><span class="line"><span class="built_in">puts</span>(str1);</span><br></pre></td></tr></table></figure>
<p><code>string.h</code>头文件包含了很多有用的处理字符串的函数：</p>
<ul>
<li><code>strlen(str)</code> ：返回字符个数，<code>int len=strlen(str);</code>。</li>
<li><p><code>strcmp(str1, str2)</code>：按照字典序比较字符串，<code>str1&lt;str2</code>返回负数，<code>str1==str2</code>返回<code>0</code>，<code>str1&gt;str2</code>返回正数。</p></li>
<li><code>strcpy(str1, str2)</code>：拷贝str1给str2。</li>
<li><p><code>strcat(str1, str2)</code>：把str2连接到str1后面，<code>strcat(str1, str2);</code>。</p></li>
</ul>
<p>介绍两个在<code>strio.h</code>中就包含的函数，<code>sscanf</code>和<code>sprintf</code>。<code>sscanf</code>用来把一个字符串按照要求的格式赋值给其它变量，<code>sprintf</code>用来把其它变量按照要求的格式赋值给字符串。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">char</span> str[<span class="number">50</span>]=<span class="string">&quot;12345&quot;</span>;</span><br><span class="line"><span class="keyword">int</span> n;</span><br><span class="line"><span class="comment">// sscanf用法</span></span><br><span class="line"><span class="built_in">sscanf</span>(str, <span class="string">&quot;%d&quot;</span>, &amp;n);</span><br><span class="line"><span class="keyword">char</span> str1[<span class="number">50</span>]=<span class="string">&quot;2048:3.14,hello&quot;</span>;</span><br><span class="line"><span class="keyword">double</span> b;</span><br><span class="line"><span class="keyword">char</span> s[<span class="number">10</span>];</span><br><span class="line"><span class="built_in">sscanf</span>(str1, <span class="string">&quot;%d:%lf,%s&quot;</span>, &amp;n, &amp;b, s);</span><br><span class="line"><span class="comment">// sprintf用法</span></span><br><span class="line"><span class="built_in">sprintf</span>(str, <span class="string">&quot;%d:%.1lf&quot;</span>, n, b); <span class="comment">// 此时str由12345变为2018:3.1</span></span><br></pre></td></tr></table></figure>
<h2 id="指针">指针</h2>
<p>在c语言中，指针是指向变量首个字节地址的变量，由于不同类型的变量有不同的字节，因此还需要确定变量的类型。<code>int *p = &amp;a;</code>。指针本身都是一个unsigned的int。</p>
<p>如果定义多个指针，都需要包括<code>*</code>，<code>int *p1, *p2, *p3;</code>。</p>
<p>指针变量支持自增和自减操作。</p>
<p>c语言中的数组名就代表首个数组元素的地址，也可以直接看做是个指针。因此</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> a[<span class="number">10</span>];</span><br><span class="line"><span class="keyword">int</span> *p = a;</span><br><span class="line"><span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, a); <span class="comment">// 会直接赋值给a[0]</span></span><br><span class="line"><span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, a+<span class="number">1</span>); <span class="comment">// 会直接赋值给a[1]</span></span><br></pre></td></tr></table></figure>
<p>指针作为函数参数传入时，指针参数本身是值传递，但是通过指针操作被指向的变量可以直接修改。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">change</span><span class="params">(<span class="keyword">int</span> *p)</span> </span>&#123;</span><br><span class="line">  *p = <span class="number">1</span>; <span class="comment">// 会修改被指向变量的值</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>c++提供了引用的语法，用在函数的形式参数中，用来表示该形式参数只是传入变量的一个别名，不会拷贝和复制，这样不需要指针也能够直接修改传入的原参数。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">swap</span><span class="params">(<span class="keyword">int</span> &amp;a, <span class="keyword">int</span> &amp;b)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> c = a;</span><br><span class="line">  a = b;</span><br><span class="line">  b = c;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">int</span> x = <span class="number">0</span>, y = <span class="number">1</span>;</span><br><span class="line">swap(x, y); <span class="comment">// 注意，x和y不能再加&amp;</span></span><br></pre></td></tr></table></figure>
<h2 id="结构体的使用">结构体的使用</h2>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">studentInfo</span> &#123;</span></span><br><span class="line">  <span class="comment">// 基本数据类型</span></span><br><span class="line">  <span class="keyword">int</span> age;</span><br><span class="line">  <span class="keyword">char</span> name[<span class="number">10</span>];</span><br><span class="line">  studentInfo * nextStu;</span><br><span class="line">&#125; stu, *stuP, students[<span class="number">20</span>];</span><br></pre></td></tr></table></figure>
<p>注意，定义的时候，结构体内不能有自身类型，但是可以有指向自身的指针。</p>
<p>访问结构体元素：</p>
<ul>
<li><p>非指针：<code>stu.age; stu.name</code></p></li>
<li><p>指针：<code>stuP-&gt;age; stuP-&gt;name</code></p></li>
</ul>
<p>构造函数，为了方便直接利用已有的基本数据类型变量生成一个结构体实例，c语言提供了结构体的构造函数，没有返回类型，构造函数命名就是本身。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">studentInfo</span> &#123;</span></span><br><span class="line">  <span class="comment">// 基本数据类型</span></span><br><span class="line">  <span class="keyword">int</span> age;</span><br><span class="line">  <span class="keyword">char</span> name[<span class="number">10</span>];</span><br><span class="line">  studentInfo * nextStu;</span><br><span class="line">  <span class="comment">// 构造函数</span></span><br><span class="line">  <span class="comment">// 默认构造函数，可以设计多个应用与不同场景，只要形参不同即可</span></span><br><span class="line">  studentInfo() &#123;&#125; <span class="comment">// 便于不初始化也能定义结构体变量</span></span><br><span class="line">  studentInfo(<span class="keyword">int</span> _age, <span class="keyword">char</span> _name[]) &#123;</span><br><span class="line">    age = _age;</span><br><span class="line">    <span class="built_in">strcpy</span>(name, _name);</span><br><span class="line">  &#125;</span><br><span class="line">  studentInfo(<span class="keyword">int</span> _age) &#123;</span><br><span class="line">    age = _age;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br><span class="line">studentInfo stu; <span class="comment">// 如果没有默认构造函数就无法这样定义</span></span><br><span class="line">studentInfo stu2 = studentInfo(<span class="number">18</span>, <span class="string">&quot;alexpp&quot;</span>);</span><br></pre></td></tr></table></figure>
<h2 id="补充">补充</h2>
<p>cin读入字符串数组的时候，直接<code>cin&gt;&gt;str</code>即可，但是如果希望读入一整行的话，使用<code>getline</code>，比如<code>getline(cin, str)</code>，这个方法对于STL中的string同样适用。</p>
<p>cout的输出，如果需要指定浮点数的精度，需要包括<code>&lt;iomanip&gt;</code>，</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cout</span>&lt;&lt;setiosflags(ios::fixed)&lt;&lt;setprecision(<span class="number">2</span>)&lt;&lt;<span class="number">123.456</span>&lt;&lt;<span class="built_in">endl</span>;</span><br></pre></td></tr></table></figure>
<p>浮点数的比较，在进行了可能影响小数点的精度的运算之后，本来两个从原理上应该相等的浮点数，结果由于舍弃了部分小数位，导致不相等，这时候需要考虑误差允许范围内的比较操作。</p>
<p>定义一个小数，<code>const double eps = 1e-8</code>，<code>1e</code>代表的是<code>10</code>。</p>
<p>重新定义<code>==, !=,&lt;,&gt;,&gt;=,&lt;=</code>。</p>
<p>举例：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">equ</span><span class="params">(<span class="keyword">double</span> a, <span class="keyword">double</span> b)</span> </span>&#123;</span><br><span class="line">	<span class="keyword">return</span> <span class="built_in">fabs</span>(a - b) &lt; eps;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">large</span><span class="params">(<span class="keyword">double</span> a, <span class="keyword">double</span> b)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> (a - b) &gt; eps;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">less</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> (b - a) &gt; eps;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">largeEqu</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> a &gt; (b - eps);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">lessEqu</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> a &lt; (b + eps);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>计算<span class="math inline">\(\pi\)</span>，使用<code>const double pi = acos(-1.0);</code>即可。</p>
<h2 id="黑盒测试">黑盒测试</h2>
<p>单点测试就是OJ对于每组输入都重新启动文件，程序运行一次只需要处理一组测试数据，即程序只需要保证单次运行成功即可。</p>
<p>多点测试就是要求程序必须一次运行所有组测试数据，根据题目不同有不同写法。</p>
<p>由于OJ是把所有测试数据放在一个文件中，因此只要判断测试文件是否已经输入完毕即可。<code>scanf</code>函数会返回成功读入的参数的个数，如果读到文件末尾就会返回<code>-1</code>，在c语言中使用<code>EOF</code>代表-1。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span>(<span class="built_in">scanf</span>(<span class="string">&quot;d&quot;</span>, &amp;n)!=EOF) &#123;</span><br><span class="line">  <span class="comment">// pass</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 如果是在读入字符串</span></span><br><span class="line"><span class="keyword">while</span>(<span class="built_in">scanf</span>(<span class="string">&quot;s&quot;</span>, str)!=EOF) &#123;</span><br><span class="line">  <span class="comment">// pass</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">while</span>(gets(str)!=<span class="literal">NULL</span>) &#123;</span><br><span class="line">    <span class="comment">// pass</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>当然根据题目，可能还有其它合适的循环读入的方法。</p>
]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>book</tag>
      </tags>
  </entry>
  <entry>
    <title>6-stl</title>
    <url>/algorithm-note/6-stl/</url>
    <content><![CDATA[<h1 id="c标准模板库">C++标准模板库</h1>
<p>本文来自于《算法笔记》第六章内容</p>
<p>STL：standard template library</p>
<span id="more"></span>
<h2 id="vector常见用法">vector常见用法</h2>
<p>可变长数组，使用前提</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br></pre></td></tr></table></figure>
<p>定义方法：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="built_in">vector</span> &lt;<span class="keyword">typename</span>&gt; name;</span><br><span class="line"><span class="comment">// 比如：</span></span><br><span class="line"><span class="built_in">vector</span> &lt;<span class="keyword">char</span>&gt; vec;</span><br><span class="line"><span class="built_in">vector</span> &lt;<span class="built_in">vector</span> &lt;<span class="keyword">int</span>&gt; &gt; twod_array <span class="comment">// 注意这里的&gt; &gt;之间的空格，防止部分编译器编译错误</span></span><br></pre></td></tr></table></figure>
<p>访问vector中的元素有两种方法，下标访问以及通过迭代器访问。</p>
<p>下标访问，和数组的正常访问一样，只要不越界即可：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="built_in">vector</span> &lt;<span class="keyword">int</span>&gt; vec;</span><br><span class="line"><span class="comment">// pass</span></span><br><span class="line">a = vec[<span class="number">1</span>];</span><br></pre></td></tr></table></figure>
<p>迭代器访问，迭代器是一种类似于指针的东西，定义方法：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="built_in">vector</span> &lt;<span class="keyword">typename</span>&gt;::iterator it;</span><br><span class="line"><span class="comment">// 让it指向vector中的某个元素地址，然后直接使用*it就能取值，例如：</span></span><br><span class="line">it = vec.begin();</span><br><span class="line">val = *it;</span><br><span class="line">val = *(it+<span class="number">3</span>);</span><br><span class="line"><span class="keyword">int</span> i = <span class="number">8</span>;</span><br><span class="line">val = *(it+i); <span class="comment">// 注意这种迭代器+整数的写法，只有在vector和string中有实现，其它stl容器无提供相关内容</span></span><br></pre></td></tr></table></figure>
<p>迭代器实现了自增和自减操作：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 可用于循环取vector中的元素</span></span><br><span class="line">it++;</span><br><span class="line">++it;</span><br><span class="line">--it;</span><br><span class="line">it--;</span><br></pre></td></tr></table></figure>
<p>可以用于遍历元素：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 注意这里&lt;vec.end()的写法，只有在vector和string容器里可用</span></span><br><span class="line"><span class="keyword">for</span>(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;::iterator it = vec.begin(); it &lt; vec.end(); ++it) &#123;</span><br><span class="line">  <span class="comment">// pass</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>vector常用函数举例：</p>
<p><code>begin()</code>和<code>end()</code>，需要注意的是<code>end()</code>不指向实际的元素，这是因为在c语言中，习惯用左闭右开的区间写法。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 返回vector的首地址以及尾地址的后一位</span></span><br><span class="line">vec.begin();</span><br><span class="line">vec.end();</span><br><span class="line"><span class="comment">// 这两个函数可以和iterator结合起来进行元素遍历，也可以用在sort函数中</span></span><br><span class="line">sort(vec.begin(), vec.end());</span><br></pre></td></tr></table></figure>
<p><code>size()</code>返回元素个数，常用于遍历</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> total_num = vec.size();</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i &lt; total_num; ++i) &#123;</span><br><span class="line">  <span class="comment">// pass</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>push_back()</code>末尾增加元素</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>;i&lt;n;++i) &#123;</span><br><span class="line">	<span class="built_in">scanf</span>(<span class="string">&quot;%d&quot;</span>, &amp;temp);</span><br><span class="line">	vec.push_back(temp);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>pop_back()</code>删除末尾元素</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">vec.pop_back();</span><br></pre></td></tr></table></figure>
<p><code>clear()</code>清空所有元素</p>
<p><code>insert(it, x)</code>在特定位置插入新元素</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">vec.insert(vec.begin()+<span class="number">2</span>, <span class="number">-1</span>); <span class="comment">// 在第3个元素插入-1</span></span><br></pre></td></tr></table></figure>
<p><code>earse()</code>删除特定元素或者一个区间的元素（左闭右开）</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 删除特定迭代器位置的元素</span></span><br><span class="line">vec.earse(vec.begin()+<span class="number">3</span>);</span><br><span class="line"><span class="comment">// 删除一个区间的元素</span></span><br><span class="line">vec.earse(vec.begin()+<span class="number">1</span>, vec.begin()+<span class="number">3</span>);</span><br></pre></td></tr></table></figure>
<h2 id="set常见用法">set常见用法</h2>
<p>在c语言中的<code>set</code>是一个<strong>自动排序且无重复元素</strong>的容器。</p>
<p>使用前提</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;set&gt;</span></span></span><br></pre></td></tr></table></figure>
<p>定义方法，与vector一样</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="built_in">set</span>&lt;<span class="keyword">typename</span>&gt; name;</span><br><span class="line"><span class="built_in">set</span>&lt;<span class="keyword">int</span>&gt; st;</span><br></pre></td></tr></table></figure>
<p>另外，set的排序是可以更改的，默认的是使用<code>less&lt;typename&gt;</code>的排序标准，可以改为从大到小的排序</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="built_in">set</span>&lt;<span class="keyword">int</span>, greater&lt;<span class="keyword">int</span>&gt;&gt; numbers;</span><br></pre></td></tr></table></figure>
<p>访问元素，只能使用迭代器，不能使用下标访问。遍历set中的元素：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 注意这里，不能使用it&lt;st.end()来判断停止条件</span></span><br><span class="line"><span class="keyword">for</span>(<span class="built_in">set</span>&lt;<span class="keyword">int</span>&gt;::iterator it = st.begin(); it!=st.end(); ++it) &#123;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;%d &quot;</span>, *it);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>set</code>的常用函数：</p>
<p><code>insert()</code>插入新元素，会去重并排序，记得没有<code>push_back()</code>。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">st.insert(<span class="number">1</span>);</span><br><span class="line">st.insert(<span class="number">2</span>);</span><br><span class="line">st.insert(<span class="number">3</span>);</span><br></pre></td></tr></table></figure>
<p><code>size()</code>返回元素个数；</p>
<p><code>find()</code>返回对应值的元素的迭代器：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">it = st.find(<span class="number">2</span>);</span><br><span class="line"><span class="comment">// 如果没有找到对应元素</span></span><br><span class="line"><span class="keyword">if</span> (st.find(<span class="number">-1000</span>) == st.end()) &#123;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;not found\n&quot;</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>earse()</code>删除元素：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 删除单个元素</span></span><br><span class="line">st.earse(<span class="number">2</span>); <span class="comment">// 直接删除对应值的元素</span></span><br><span class="line">st.earse(st.find(<span class="number">2</span>)); <span class="comment">// 删除对应迭代器位置的元素</span></span><br><span class="line"><span class="comment">// 删除一组元素，[)</span></span><br><span class="line">st.earse(it1, it2);</span><br></pre></td></tr></table></figure>
<p><code>clear()</code>清空元素。</p>
<p>还有其它类似的<code>set</code>容器，比如<code>multiset</code>和<code>unordered_set</code>。</p>
<p><code>multiset</code>不会去重，但是会排序，使用方法与set类似，同样是默认从小到大的排序。</p>
<p><code>unordered_set</code>不会排序但是会去重，速度更快，可以用来很方便的去重元素，使用方法与set类似。</p>
<h2 id="string常见用法">string常见用法</h2>
<p>c++在stl中提供了string类型用于实现对字符串的处理。</p>
<p>使用前提</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 注意&lt;string.h&gt;是完全不同的头文件</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string&gt;</span></span></span><br></pre></td></tr></table></figure>
<p>定义：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="built_in">string</span> str = <span class="string">&quot;abcde&quot;</span>;</span><br></pre></td></tr></table></figure>
<p>输入与输出：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 只能使用cin和cout</span></span><br><span class="line"><span class="built_in">cin</span>&gt;&gt;str;</span><br><span class="line"><span class="built_in">cout</span>&lt;&lt;str;</span><br><span class="line"><span class="comment">// 输出也可以使用printf</span></span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;%s&quot;</span>, str.c_str());</span><br></pre></td></tr></table></figure>
<p>元素访问：</p>
<p>单元素访问，类似于vector可以使用下标：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> = <span class="number">2</span>;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;%c&quot;</span>, str[i]);</span><br></pre></td></tr></table></figure>
<p>通过迭代器访问：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="built_in">string</span>::iterator it = str.begin();</span><br><span class="line">it++;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;%c&quot;</span>, *it);</span><br><span class="line">it+=<span class="number">3</span>; <span class="comment">// 类似于vector，可以让迭代器加一个常量长度</span></span><br></pre></td></tr></table></figure>
<p>string常用函数：</p>
<p>操作符<code>+=</code>，直接拼接两个string，<code>str1+=str2</code>。</p>
<p>比较符<code>==,!=,&lt;,&lt;=,&gt;,&gt;=</code>，比较顺序是字典序。</p>
<p>元素个数查询<code>length()/size()</code>。</p>
<p>插入新字符<code>insert()</code>，举两个实例：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 在特定位置pos插入字符串str</span></span><br><span class="line">str.insert(<span class="number">3</span>, <span class="string">&quot;opt&quot;</span>);</span><br><span class="line"><span class="comment">// 在迭代器it，插入另一个字符串的[it1, it2)子串</span></span><br><span class="line">str.insert(it, str2.begin(), str2.end());</span><br></pre></td></tr></table></figure>
<p>删除元素<code>erase()</code>，举例：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 删除迭代器位置it的单个元素</span></span><br><span class="line">str.erase(it);</span><br><span class="line"><span class="comment">// 删除一个区间的元素，两个迭代器中间的元素</span></span><br><span class="line">str.erase(first, last);</span><br><span class="line"><span class="comment">// 删除一个位置开始的一定长度的元素</span></span><br><span class="line">str.erase(pos, length);</span><br></pre></td></tr></table></figure>
<p>清空元素<code>clear()</code>。</p>
<p>截取子串，<code>substr(pos, len)</code>。</p>
<p>寻找子串<code>find()</code>，举例：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 寻找一个子串，返回位置pos</span></span><br><span class="line"><span class="keyword">int</span> pos = str.find(<span class="string">&quot;ab&quot;</span>);</span><br><span class="line"><span class="comment">// 如果没有找到，返回没有找到的代码</span></span><br><span class="line"><span class="keyword">if</span> (pos == <span class="built_in">string</span>::npos) &#123;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;not found\n&quot;</span>);</span><br><span class="line">&#125; <span class="comment">// 记住，string::npos是一个常数，作为find函数匹配失败的返回值</span></span><br><span class="line"><span class="comment">// 还可以从某个位置开始匹配</span></span><br><span class="line">pos = str.find(<span class="string">&quot;ab&quot;</span>, <span class="number">3</span>);</span><br></pre></td></tr></table></figure>
<p>替换子串<code>replace()</code>，举例：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">str.replace(<span class="number">2</span>, <span class="number">4</span>, <span class="string">&quot;gggg&quot;</span>); <span class="comment">//把str中[2, 4)的子串替换为&quot;gggg&quot;</span></span><br><span class="line">str.replace(it1, it2, <span class="string">&quot;gggg&quot;</span>); <span class="comment">//把str中[it1, it2)的子串替换为&quot;gggg&quot;</span></span><br></pre></td></tr></table></figure>
<h2 id="map常见用法">map常见用法</h2>
<p>map可以将任何基本类型（包括容器）映射到任何基本类型（包括容器）。</p>
<p>使用map</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;map&gt;</span></span></span><br></pre></td></tr></table></figure>
<p>map的定义</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="built_in">map</span>&lt;keytype, valtype&gt; mp;</span><br><span class="line"><span class="comment">// 如果使用字符串作为key，只能使用string</span></span><br><span class="line"><span class="built_in">map</span>&lt;<span class="built_in">string</span>, <span class="keyword">int</span>&gt; mp;</span><br></pre></td></tr></table></figure>
<p>map元素的访问，通过下标key进行访问</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">mp[&#x27;aa&#x27;] = 2;</span><br><span class="line">mp[&#x27;aa&#x27;] = 20;</span><br></pre></td></tr></table></figure>
<p>通过迭代器访问</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="built_in">map</span>&lt;<span class="built_in">string</span>, <span class="keyword">int</span>&gt;::iterator it;</span><br><span class="line">it = mp.begin();</span><br><span class="line"><span class="built_in">cout</span>&lt;&lt;it-&gt;first; <span class="comment">// 通过it-&gt;first访问键, it-&gt;second访问值</span></span><br><span class="line"><span class="built_in">cout</span>&lt;&lt;it-&gt;second;</span><br></pre></td></tr></table></figure>
<p>map内部是使用红黑树实现的，因此会按照从小到大的顺序自动排列键。</p>
<p>map常用函数：</p>
<p>查找某个key是否存在<code>find()</code>，返回迭代器，<code>mp.find(&quot;aa&quot;)</code>；</p>
<p>插入元素<code>insert()</code></p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">mp.insert(<span class="built_in">pair</span>&lt;<span class="built_in">string</span>, <span class="keyword">int</span>&gt;(<span class="string">&quot;bbb&quot;</span>, <span class="number">7</span>)); <span class="comment">// 注意map都是以pair为操作对象，插入需要是一个pair对象</span></span><br></pre></td></tr></table></figure>
<p>删除元素<code>erase()</code>，</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">mp.erase(it); <span class="comment">// 删除迭代器位置的元素</span></span><br><span class="line">mp.erase(<span class="string">&quot;aa&quot;</span>); <span class="comment">// 删除key键的元素</span></span><br><span class="line">mp.erase(first_it, last_it); <span class="comment">// 删除[first_it, last_it)的元素</span></span><br></pre></td></tr></table></figure>
<p>元素个数<code>size()</code>；</p>
<p>清空<code>clear()</code>；</p>
<p>map中的键和值是唯一的，如果希望一个键对应多个值，可以使用<code>multimap</code>。</p>
<p>由于map会默认按照<code>less&lt;typename&gt;</code>进行排序，所以类似于set，c++11中提供了<code>unordered_map</code>。</p>
<p>map和set实际具有几乎完全相同的接口和函数名，set可以看做是一种特殊的map，即key=value。</p>
<h2 id="queue常见用法">queue常见用法</h2>
<p>queue是先进先出的限制性数据结构</p>
<p>使用queue</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;queue&gt;</span></span></span><br></pre></td></tr></table></figure>
<p>定义queue</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="built_in">queue</span>&lt;<span class="keyword">typename</span>&gt; que;</span><br></pre></td></tr></table></figure>
<p>访问元素，queue只能访问队首或者队尾，不能像前面的stl一样通过下标任意访问</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">que.front(); <span class="comment">// 访问队首，在使用前记住要先判断que.empty()，避免队空而出错</span></span><br><span class="line">que.back(); <span class="comment">// 访问队尾</span></span><br></pre></td></tr></table></figure>
<p>queue常用函数</p>
<p>增加新元素<code>push()</code>。</p>
<p>队首元素出列<code>pop()</code>。</p>
<p>检测queue是否为空<code>empty()</code>，如果是空返回<code>true</code>。</p>
<p>元素个数<code>size()</code>。</p>
<h2 id="priority_queue常见用法">priority_queue常见用法</h2>
<p>priority_queue是优先队列，和queue的区别是它保证队列中优先级最高的总是在<em>队首</em>，queue不会自动排序。</p>
<p>priority_queue的定义</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;queue&gt;</span></span></span><br><span class="line"><span class="built_in">priority_queue</span>&lt;<span class="keyword">typename</span>&gt; pq;</span><br></pre></td></tr></table></figure>
<p>常用函数：</p>
<p>增加新元素<code>push()</code>。</p>
<p>查看队首元素<code>top()</code>，注意没有queue中的<code>front()</code>和<code>back()</code>。</p>
<p>弹出队首元素<code>pop()</code>，使用前记得使用<code>empty()</code>判断是否为空，防止报错。</p>
<p>检测空<code>empty()</code>。</p>
<p>元素个数<code>size()</code>。</p>
<p>如何设置priority_queue的优先级？</p>
<p>一般的，按照基本类型的从大到小排序，字符串按照字典序，int按照数值大小。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="built_in">priority_queue</span>&lt;<span class="keyword">int</span>, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;, less&lt;<span class="keyword">int</span>&gt; &gt; pq; <span class="comment">// 优先级大的int元素在队首</span></span><br></pre></td></tr></table></figure>
<p>其中的<code>vector&lt;int&gt;</code>是priority_queue的底层数据结构堆的容器，类型需要和前面的元素type保持一致。<code>less&lt;int&gt;</code>是数值大的在队首，这一点和前面的set是相反的，<code>greater&lt;int&gt;</code>表示数值小的会在队首。</p>
<p>另外，对于结构体，可以通过下面重载比较符<code>&lt;</code>的方法定义优先级，注意只能重载<code>&lt;</code>不能重载<code>&gt;</code>，因为只要定义好了<code>&lt;</code>，<code>&gt;</code>和<code>==</code>也就都定义好了。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">fruit</span> &#123;</span></span><br><span class="line">  <span class="built_in">string</span> name;</span><br><span class="line">  <span class="keyword">int</span> price;</span><br><span class="line">  <span class="keyword">friend</span> <span class="keyword">bool</span> <span class="keyword">operator</span> &lt; (fruit f1, fruit f2) &#123;</span><br><span class="line">    <span class="keyword">return</span> f1.price &lt; f2.price; <span class="comment">// 让价格高的在队首，优先队列会选择f1和f2中比较出来大的对象放到前面</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="built_in">priority_queue</span>&lt;fruit&gt; fruits;</span><br></pre></td></tr></table></figure>
<p>这种定义方式有些类似于<code>sort()</code>函数中可以自定义的比较函数，但是如果上面的比较方法放在<code>sort</code>中，会让价格低的在前面，与priority_queue刚好相反。</p>
<h2 id="stack常见用法">stack常见用法</h2>
<p>stack是后进先出的限制性容器，定义</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stack&gt;</span></span></span><br><span class="line"><span class="built_in">stack</span>&lt;<span class="keyword">typename</span>&gt; st;</span><br></pre></td></tr></table></figure>
<p>访问元素类似于priority_queue，只能通过<code>top()</code>访问栈顶元素。</p>
<p>常用函数：</p>
<p><code>push()</code>增加新元素</p>
<p><code>top()</code>获得栈顶元素</p>
<p><code>pop()</code>退栈</p>
<p><code>empty()</code>检测是否为空</p>
<p><code>size()</code>元素个数</p>
<h2 id="pair常见用法">pair常见用法</h2>
<p>pair可以将两个元素合并为一个元素，可以看做是一个包含两个元素的struct。</p>
<p>pair的定义</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;utility&gt;</span></span></span><br><span class="line"><span class="comment">// #include&lt;map&gt; // 由于map的内部使用了pair，所以map头文件中会自动添加utility，所以可以偷懒使用map头文件</span></span><br><span class="line"><span class="built_in">pair</span>&lt;typename1, typename2&gt; p;</span><br></pre></td></tr></table></figure>
<p>pair的初始化</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">pair&lt;string, int&gt; p (&quot;aaa&quot;, 9);</span><br></pre></td></tr></table></figure>
<p>pair临时构造</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="built_in">pair</span>&lt;<span class="built_in">string</span>, <span class="keyword">int</span>&gt;(<span class="string">&quot;cccc&quot;</span>, <span class="number">11</span>);</span><br><span class="line"><span class="comment">// 或者使用make_pair函数</span></span><br><span class="line"><span class="built_in">make_pair</span>(<span class="string">&quot;cccc&quot;</span>, <span class="number">11</span>);</span><br></pre></td></tr></table></figure>
<p>pair元素访问，只有两个元素，分别是first和second</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">p.first;</span><br><span class="line">p.second;</span><br></pre></td></tr></table></figure>
<p>pair常用函数</p>
<p>支持比较操作符，<code>==</code>,<code>&lt;</code>,<code>&gt;</code>等，规则是先比较first，只有first相等之后才会比较second。</p>
<h2 id="algorithm头文件下的常用函数">algorithm头文件下的常用函数</h2>
<p>使用</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;algorithm&gt;</span></span></span><br></pre></td></tr></table></figure>
<p><code>max</code>,<code>min</code>和<code>abs</code>分别返回最大值、最小值以及<em>整数</em>的绝对值</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> x = <span class="number">1</span>, y = <span class="number">-1</span>, z = <span class="number">3</span>;</span><br><span class="line">max(x, y);</span><br><span class="line">min(x, y);</span><br><span class="line"><span class="built_in">abs</span>(y);</span><br><span class="line"><span class="comment">// 如果希望是浮点数的绝对值，使用&lt;math&gt;头文件下的fabs</span></span><br><span class="line"><span class="built_in">fabs</span>(<span class="number">-0.19</span>);</span><br></pre></td></tr></table></figure>
<p><code>swap()</code>交换x和y的值</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">swap(x, y);</span><br></pre></td></tr></table></figure>
<p><code>reverse()</code>反转一段数组或者一部分容器的元素</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> a [<span class="number">3</span>] = &#123;<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>&#125;;</span><br><span class="line">reverse(a, a+<span class="number">2</span>);</span><br><span class="line"><span class="built_in">string</span> b = <span class="string">&quot;abcdefg&quot;</span>;</span><br><span class="line">reverse(b.begin(), b.begin()+<span class="number">3</span>);</span><br></pre></td></tr></table></figure>
<p><code>next_permutation()</code>返回下一个排列</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">do</span> &#123;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;%d %d %d\n&quot;</span>, a[<span class="number">0</span>], a[<span class="number">1</span>], a[<span class="number">2</span>]);</span><br><span class="line">&#125; <span class="keyword">while</span> (next_permutation(a, a+<span class="number">3</span>));</span><br></pre></td></tr></table></figure>
<p><code>fill(it1, it2, val)</code>将数组或者容器的一部分连续元素赋值为相同值</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line">fill(a, a+<span class="number">2</span>, <span class="number">4</span>);</span><br></pre></td></tr></table></figure>
<p><code>sort(it1, it2, compare_func)</code>排序数组或容器，结构体等。可能是最常用的方法。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 默认从小到大排序</span></span><br><span class="line">sort(a, a+<span class="number">3</span>);</span><br><span class="line"><span class="comment">// 从大到小排序数组</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">cmp</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> x &gt; y;</span><br><span class="line">&#125;</span><br><span class="line">sort(a, a+<span class="number">3</span>, cmp);</span><br><span class="line"><span class="comment">// 排序容器，只有vector，string，deque可用</span></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; vec;</span><br><span class="line">vec.push_back(<span class="number">2</span>);</span><br><span class="line">vec.push_back(<span class="number">4</span>);</span><br><span class="line">vec.push_back(<span class="number">-1</span>);</span><br><span class="line">sort(vec.begin(), vec.end(), cmp);</span><br><span class="line"><span class="comment">// 排序结构体</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">node</span> &#123;</span></span><br><span class="line">  <span class="keyword">int</span> x, y;</span><br><span class="line">&#125; ssd[<span class="number">10</span>];</span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">cmp_stru</span><span class="params">(node n1, node n2)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> n1.x &gt; n2.x;</span><br><span class="line">&#125;</span><br><span class="line">sort(ssd, ssd+<span class="number">4</span>, cmp_stru);</span><br></pre></td></tr></table></figure>
<p><code>lower_bound()</code>返回第一个<em>等于或者大于</em>目标元素的指针（数组）或者迭代器，如果没找到返回适合插入该元素的位置；</p>
<p><code>upper_bound()</code>返回第一个<em>大于</em>目标元素的指针（数组）或者迭代器，如果没找到返回适合插入该元素的位置；</p>
<p>因此，如果没有找到对应元素，两个函数会返回相同的值</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> * low_pos = lower_bound(a, a+<span class="number">2</span>, <span class="number">3</span>);</span><br><span class="line"><span class="keyword">int</span> * up_pos = upper_bound(a, a+<span class="number">2</span>, <span class="number">3</span>);</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;lower_bound: %d\n&quot;</span>, low_pos - a); <span class="comment">// 返回下标</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>book</tag>
      </tags>
  </entry>
  <entry>
    <title>11-dynamic-programming</title>
    <url>/algorithm-note/11-dynamic-programming/</url>
    <content><![CDATA[<h1 id="动态规划">动态规划</h1>
<p>《算法笔记》第11章，动态规划</p>
<span id="more"></span>
<h2 id="动态规划基本介绍">动态规划基本介绍</h2>
<p>动态规划（Dynamic Programming, DP）是很精妙的算法，没有固定的写法。基本思想是把一个复杂问题拆分为几个不同的子问题，通过综合子问题的最优解来获得复杂问题的最优解。理解动态规划有以下几点需要注意：</p>
<ul>
<li>应用动态规划的前提是复杂问题的最优解可以通过其子问题的最优解来获得，如果一个问题满足这样的性质，就称该问题具有最优子结构（Optimal Substructure）。</li>
<li>动态规划与分治法的区别是：分治法划分子问题没有重叠，一个子问题不会再重复的出现；动态规划划分出的子问题会重复的出现，因此<em>动态规划通常会记录子问题的解</em>，当再次遇到子问题时直接给出解。另外，分治法不一定是在解决最优问题，动态规划则总是在找最优解。</li>
<li>动态规划与贪心法的区别是：贪心算法会依赖于当前状态，在多个子问题中直接选择当前最优的子问题，而不再考虑其它子问题，因此贪心算法能否真的找到最优解还依赖于分析证明；<em>动态规划会考虑所有子问题</em>，在后续的步骤中可能会重新考虑之前的子问题，不断综合子问题的最优解，保证最后能够找到最优结果。</li>
</ul>
<p>接下来考虑几个经典的动态规划算法</p>
<h2 id="最大连续子序列和">最大连续子序列和</h2>
<p>给定一个数字序列<code>A</code>，求最大的连续数字序列的和。</p>
<p>思路：对于<code>i</code>位的数字，规定以<code>i</code>位数字结尾的最大连续序列和为<code>dp[i]</code>，那么<code>dp[i]=max(A[i], dp[i-1]+A[i])</code>，最大的子序列和就是所有<code>dp[]</code>的最大值。核心就是这个状态转移方程，可以看到当前状态仅仅依赖于已有的状态，一旦当前状态确定后，不会在后续的步骤中改变。动态规划的核心难点就是这一点。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> dp[n];</span><br><span class="line">  dp[<span class="number">0</span>] = A[<span class="number">0</span>]; <span class="comment">// 初始化首位</span></span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; n; ++i) &#123;</span><br><span class="line">		dp[i] = max(dp[i<span class="number">-1</span>]+A[i], A[i]);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 检查最大值</span></span><br><span class="line">  <span class="keyword">int</span> MAX = dp[<span class="number">0</span>];</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; n; ++i) &#123;</span><br><span class="line">    <span class="keyword">if</span>(dp[i]&gt;MAX) &#123;</span><br><span class="line">      MAX = dp[i];</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;%d\n&quot;</span>, MAX);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="最长不下降子序列lis">最长不下降子序列（LIS）</h2>
<p>给定一个序列，求最长的可不连续、非递降子序列（Longest Increasing Sequence, LIS）。</p>
<p>思路：对于<code>A[i]</code>，遍历所有的<code>A[0]-A[i-1]</code>，如果存在一个元素<code>A[k]</code>比<code>A[i]</code>小或者相等，那么计算下<code>dp[k]+1</code>，选择最大值保存为<code>dp[i]</code>。同样，无论<code>dp[i]</code>之后出现什么新元素，不会改变<code>dp[i]</code>，<code>dp[i]</code>也只依赖于之前已有的结果。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> dp[n];</span><br><span class="line">  dp[<span class="number">0</span>] = <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; n; ++i) &#123;</span><br><span class="line">    dp[i] = <span class="number">1</span>;</span><br><span class="line">    <span class="comment">// 遍历以前所有的元素，保存LIS值</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; i; ++j) &#123;</span><br><span class="line">      <span class="keyword">if</span>(A[j]&lt;=A[i] &amp;&amp; dp[j] + <span class="number">1</span> &gt; dp[i]) &#123;</span><br><span class="line">        dp[i] = dp[j] + <span class="number">1</span>;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 检查最大不下降序列</span></span><br><span class="line">  <span class="keyword">int</span> MAX = dp[<span class="number">0</span>];</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; ++i) &#123;</span><br><span class="line">		<span class="keyword">if</span>(MAX&lt;dp[i]) MAX = dp[i];</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;%d\n&quot;</span>, MAX);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="最长公共子序列lcs">最长公共子序列（LCS）</h2>
<p>给定两个序列<code>A</code>和<code>B</code>，求最长的公共可不连续子序列（Longest Common Seqence, LCS）</p>
<p>思路：对于<code>A[i]</code>和<code>B[j]</code>，如果<code>A[i]==B[j]</code>，那么<code>LCS+1</code>；如果<code>A[i]!=B[j]</code>，那么<code>LCS=max(dp[i][j-1], dp[i-1][j])</code>。初始化的边界是<code>dp[0][j]=0</code>和<code>dp[i][0]=0</code>。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> m,n; <span class="comment">// m是序列A的长度，n是序列B的长度</span></span><br><span class="line">  <span class="keyword">int</span> dp[m+<span class="number">1</span>][n+<span class="number">1</span>];</span><br><span class="line">  <span class="comment">// 初始化</span></span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;m;++i) &#123;</span><br><span class="line">    dp[i][<span class="number">0</span>] = <span class="number">0</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;n;++j) &#123;</span><br><span class="line">    dp[<span class="number">0</span>][j] = <span class="number">0</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 从下标1开始，避免i-1小于0</span></span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>;i&lt;=m;i++) &#123;</span><br><span class="line">		<span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">1</span>;j&lt;=n;j++) &#123;</span><br><span class="line">      <span class="keyword">if</span>(A[i]==B[j]) &#123;</span><br><span class="line">        dp[i][j] = dp[i<span class="number">-1</span>][j<span class="number">-1</span>] + <span class="number">1</span>;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">else</span> &#123;</span><br><span class="line">        dp[i][j] = max(dp[i<span class="number">-1</span>][j], dp[i][j<span class="number">-1</span>]);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;%d\n&quot;</span>, dp[m][n]); <span class="comment">// 此时，dp[m][n]就是最大公共子序列</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="最长回文子串">最长回文子串</h2>
<p>回文是指从左向右和从右到左读的结果都一样的序列。检验回文只需要两个指针从两端分别检查是否元素相等，直至相遇即可。</p>
<p>给定序列<code>A</code>，要求输出最长的回文串。</p>
<p>思路：如果<code>dp[i][j]</code>中，如果<code>A[i]==A[j]</code>，并且<code>dp[i+1][j-1]</code>是回文串，那么<code>dp[i][j]</code>也是回文串；如果<code>A[i]!=A[j]</code>，那么<code>dp[i][j]</code>不是回文串。这里在实现的时候，问题在于不能直接按照<code>i</code>，<code>j</code>的循环遍历，因为这样会造成<code>dp[i+1]</code>在<code>dp[i]</code>之后出现。因此需要想办法先计算好<code>dp[i+1][j-1]</code>，考虑到长度问题，使用长度来作为循环。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> dp[n][n]; <span class="comment">// n是序列A长度，dp[i][j]=1表示序列i-&gt;j是回文子串</span></span><br><span class="line">  <span class="keyword">int</span> ans = <span class="number">1</span>; <span class="comment">// 初始化的最大回文子串长度</span></span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i&lt;n; ++i) &#123;</span><br><span class="line">    dp[i][i] = <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 子问题的长度从2-n</span></span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> L=<span class="number">2</span>; L&lt;=n; L++) &#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>; i + L - <span class="number">1</span>&lt;n; i++) &#123;</span><br><span class="line">      <span class="keyword">int</span> j = i + L - <span class="number">1</span>;</span><br><span class="line">      <span class="keyword">if</span>(A[i] == A[j] &amp;&amp; dp[i+<span class="number">1</span>][j<span class="number">-1</span>] == <span class="number">1</span>) &#123;</span><br><span class="line">        dp[i][j] = <span class="number">1</span>;</span><br><span class="line">        ans = L;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;%d\n&quot;</span>, ans);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="dag最长路径">DAG最长路径</h2>
<p>这里使用DP的思想解决有向无环图DAG的最长/最短路径问题，思想实际比dijkstra等算法更加简单易理解。</p>
<p>求一个DAG中的最长路径，不规定起点和终点</p>
<p>思想：令<code>dp[i]</code>表示从顶点<code>i</code>出发能够到达的最长路径，那么<code>dp[i]=max&#123;dp[j]+G[i][j]&#125;</code>，顶点<code>j</code>是顶点<code>i</code>的后继顶点。很明显这是一个逆拓扑排序的顺序，使用递归的方法可以简单实现。最后只需要在所有的<code>dp[i]</code>中搜索最大值即可。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> choice[maxn]; <span class="comment">// 记录选择的后继结点，初始化为-1</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">DP</span><span class="params">(<span class="keyword">int</span> i)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// dp初始化全为0</span></span><br><span class="line">  <span class="keyword">if</span>(dp[i]&gt;<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>;j&lt;n;++j) &#123;</span><br><span class="line">		<span class="keyword">if</span>(G[i][j]!=INF &amp;&amp; dp[i]&lt;DP(j)+G[i][j]) &#123;</span><br><span class="line">      dp[i] = dp[j]+G[i][j]; <span class="comment">// 更新</span></span><br><span class="line">      choice[i] = j;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> dp[i];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">printPath</span><span class="params">(<span class="keyword">int</span> i)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">while</span>(choice[i]!=<span class="number">-1</span>) &#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;%d &quot;</span>, i);</span><br><span class="line">    i = choice[i];</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>求一个DAG中，固定重点T的最长路径</p>
<p>思想：令<code>dp[i]</code>表示从顶点<code>i</code>出发能够到达终点<code>T</code>的最长路径，那么<code>dp[i]=max&#123;dp[j]+G[i][j]&#125;</code>，顶点<code>j</code>是顶点<code>i</code>的后继顶点。和上面的问题最大区别是如何体现能够到达终点<code>T</code>？前面使用<code>dp[i]==0</code>代表已经到了终点，如果还是使用0表示到达终点<code>T</code>，这会导致无法区分出不能到达<code>T</code>的顶点，那些无法到达终点<code>T</code>的路径也被考虑，最后输出错误结果。因此，考虑让<code>dp[i]</code>初始化为<code>-INF</code>，这样不能到达<code>T</code>的路径都会取到很小的值，从而被排除。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">DP</span><span class="params">(<span class="keyword">int</span> i)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// dp初始化全为-INF</span></span><br><span class="line">  <span class="keyword">if</span>(vis[i]==<span class="literal">true</span>)</span><br><span class="line">    <span class="keyword">return</span> dp[i];</span><br><span class="line">  vis[i] = <span class="literal">true</span>;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> j=<span class="number">0</span>; j&lt;n; ++j) &#123;</span><br><span class="line">		<span class="keyword">if</span>(G[i][j]!=INF &amp;&amp; dp[i]&lt;DP(j)+G[i][j]) &#123;</span><br><span class="line">      dp[i] = dp[j]+G[i][j]; <span class="comment">// 更新</span></span><br><span class="line">      choice[i] = j;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> dp[i];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="背包问题">背包问题</h2>
<h3 id="背包">01背包</h3>
<p>n种物品，价值和重量分别为<code>c[i]</code>和<code>w[i]</code>，每种物品只有1件，要求总背包承重是<code>V</code>的情况下，总价值最大。</p>
<p>思路：第<code>i</code>种物品是否放入背包容积<code>v</code>的最大值由前<code>i-1</code>种物品完全决定。<code>dp[i][v]=max(dp[i-1][v], dp[i-1][v-w[i]])</code>。为了减小二维数组的开销，可以使用滚动数组的方法，只使用一个动态改变值的一维数组来实现。注意到计算<code>i</code>的时候，<code>i-1</code>是固定值，完全可以把<code>i-1</code>看做是上一轮数组的结果，同样还是两层循环，<code>i:1-&gt;n, v:V-&gt;w[i]</code>。</p>
<p>代码：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> dp[V+<span class="number">1</span>];</span><br><span class="line">  <span class="comment">// 初始化</span></span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> v=<span class="number">0</span>;v&lt;V+<span class="number">1</span>;++v) &#123;</span><br><span class="line">    dp[v] = <span class="number">0</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>; i&lt;=n; ++i) &#123;</span><br><span class="line">    <span class="comment">// 这里必须是逆序排列，第i轮计算要用到第i-1轮的新结果</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> v=V; v&gt;=w[i]; --v) &#123;</span><br><span class="line">      dp[v] = max(dp[v], dp[v-w[i]] + c[i]);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 寻找最大值</span></span><br><span class="line">  <span class="keyword">int</span> max = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> v=<span class="number">0</span>;v&lt;V+<span class="number">1</span>;++v) &#123;</span><br><span class="line">    <span class="comment">// pass</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="完全背包">完全背包</h3>
<p>n种物品，价值和重量分别为<code>c[i]</code>和<code>w[i]</code>，每种物品有任意件，要求总背包承重是<code>V</code>的情况下，总价值最大。</p>
<p>思路：第<code>i</code>种物品是否放入背包容积<code>v</code>的最大值由前<code>i-1</code>种物品完全决定。<code>dp[i][v]=max(dp[i-1][v], dp[i][v-w[i]])</code>。和01背包的区别就是放入<code>i</code>种物品后，还可以继续考虑放入第<code>i</code>种物品。</p>
<p>代码：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> dp[V+<span class="number">1</span>];</span><br><span class="line">  <span class="comment">// 初始化</span></span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> v=<span class="number">0</span>;v&lt;V+<span class="number">1</span>;++v) &#123;</span><br><span class="line">    dp[v] = <span class="number">0</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">1</span>; i&lt;=n; ++i) &#123;</span><br><span class="line">    <span class="comment">// 这里必须是正序排列，第i轮计算要用到第i轮的新结果</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> v=w[i]; v&lt;=V; --v) &#123;</span><br><span class="line">      dp[v] = max(dp[v], dp[v-w[i]] + c[i]);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>book</tag>
      </tags>
  </entry>
  <entry>
    <title>7-data-structure</title>
    <url>/algorithm-note/7-data-structure/</url>
    <content><![CDATA[<h1 id="第七章-数据结构专题">第七章 数据结构专题</h1>
<p>《算法笔记》笔记。</p>
<span id="more"></span>
<h2 id="栈的应用">栈的应用</h2>
<p>这里记录下使用数组实现栈的思路，核心是使用一个栈顶指针记录位置。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> s[<span class="number">100000</span>]; <span class="comment">// 栈的定义</span></span><br><span class="line"><span class="keyword">int</span> TOP = <span class="number">-1</span>; <span class="comment">// 栈顶指针</span></span><br><span class="line"><span class="comment">// 自定义的常用函数，可以看到实现非常简单，主要是直接对TOP指针的操作</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">clear</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  TOP = <span class="number">-1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">size</span><span class="params">()</span> </span>&#123;</span><br><span class="line">	<span class="keyword">return</span> TOP + <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">push</span><span class="params">(<span class="keyword">int</span> x)</span> </span>&#123;</span><br><span class="line">  s[TOP++] = x;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">pop</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  TOP--;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">top</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> s[TOP];</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">empty</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (TOP == <span class="number">-1</span>) <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">  <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="队列的应用">队列的应用</h2>
<p>这里同样使用数组实现队列，和实现栈不一样的是，这里维护两个指针<code>front</code>和<code>back</code>。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> q[<span class="number">100000</span>];</span><br><span class="line"><span class="keyword">int</span> front = <span class="number">-1</span>, back = <span class="number">-1</span>; <span class="comment">// 队首与队尾指针，让队首始终指向首个元素的前一位，这样方便判断只有一个元素时，队列是否为空</span></span><br><span class="line"><span class="comment">// 常用函数</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">clear</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  front = back = <span class="number">-1</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">size</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> back - front;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">empty</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (front == back) <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">  <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">push</span><span class="params">(<span class="keyword">int</span> x)</span> </span>&#123;</span><br><span class="line">  q[++back] = x;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">pop</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  front++;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">get_front</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> q[front+<span class="number">1</span>];</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">get_back</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> q[back];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="链表的处理">链表的处理</h2>
<p>这里讨论的链表，类似于vector，只不过是自己实现。</p>
<h3 id="动态链表">动态链表</h3>
<p>先讨论动态链表，即动态生成、删除、释放链表节点。</p>
<p>首先需要定义节点用来存储数据，</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Node</span> &#123;</span></span><br><span class="line">  <span class="keyword">typename</span> data;</span><br><span class="line">  Node * next;</span><br><span class="line">&#125;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Node</span> &#123;</span></span><br><span class="line">  <span class="keyword">int</span> data;</span><br><span class="line">  Node * next;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>接下来讨论如何动态生成新节点，在c语言和c++中都有不同的实现办法，c语言中使用<code>malloc()</code>函数，c++中使用<code>new</code>操作符。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 使用malloc函数，malloc函数会返回(void *)的指针，使用(typename*)进行类型转化</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="keyword">typename</span> * p = (<span class="keyword">typename</span> *)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(<span class="keyword">typename</span>));</span><br><span class="line"><span class="comment">// 举例</span></span><br><span class="line">Node * p = (Node *)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(Node));</span><br><span class="line"><span class="comment">// 在c++中直接使用new</span></span><br><span class="line"><span class="keyword">typename</span> * p = <span class="keyword">new</span> <span class="keyword">typename</span>;</span><br><span class="line"><span class="comment">// 举例</span></span><br><span class="line">Node *p = <span class="keyword">new</span> Node;</span><br></pre></td></tr></table></figure>
<p>如何释放新节点？</p>
<p>为什么要释放新节点？</p>
<p><em>因为c语言的设计者认为程序员完全有能力自己控制内存的分配与释放</em>。</p>
<p>c语言中使用<code>free()</code>函数，c++中使用<code>delete()</code>。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// c语言</span></span><br><span class="line"><span class="built_in">free</span>(p);</span><br><span class="line"><span class="comment">// c++</span></span><br><span class="line"><span class="keyword">delete</span>(p);</span><br></pre></td></tr></table></figure>
<p>链表的创建，首先第一个问题是是否拥有头结点（dummy node）？这里按照《算法笔记》上的内容，默认拥有头结点，头结点不是第一个节点，头结点的<code>next</code>指向第一个保存数据的结点，头结点本身不储存任何数据。这样的好处之一是无论链表是否有数据，总会有一个固定的头结点，方便判断链表是否为空，以及插入新的结点等。</p>
<p>尾结点的<code>next</code>应该设置为<code>NULL</code>。</p>
<p>对于动态链表的各项操作，这里不写出来。</p>
<h3 id="静态链表">静态链表</h3>
<p>静态链表和动态链表的区别是，静态链表是通过定义数组的方式提前开辟好的连续内存，<code>next</code>就是下一个结点在数组中的下标。</p>
<p>静态链表的适用范围是在需要的地址比较小的时候，比如<code>&lt;10^5</code>。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 静态链表的定义</span></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Node</span> &#123;</span></span><br><span class="line">  <span class="keyword">int</span> data;</span><br><span class="line">  <span class="keyword">int</span> next; <span class="comment">// next == -1时表示到了链表尾端</span></span><br><span class="line">  <span class="keyword">bool</span> is_node; <span class="comment">// 这里是用来记录当前的数组元素是否属于链表的一个节点，默认应该设置为false</span></span><br><span class="line">&#125;</span><br><span class="line">Node p[<span class="number">100010</span>];</span><br><span class="line"><span class="comment">// 访问下个结点</span></span><br><span class="line"><span class="keyword">int</span> begin_address = <span class="number">2210</span>;</span><br><span class="line"><span class="keyword">int</span> next_address = p[begin_address].next;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;%d&quot;</span>, p[next_address].data);</span><br><span class="line"><span class="comment">// 静态链表的好处是，便于直接适用sort函数实现排序</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">cmp</span> <span class="params">(Node n1, Node n2)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span>(!n1.is_node || !n2.is_node) &#123;</span><br><span class="line">    <span class="keyword">return</span> n1.is_node &gt; n2.is_node; <span class="comment">// 让是结点元素的在数组靠前的位置</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> n1.data &lt; n2.data; <span class="comment">// 让data小的在前</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>book</tag>
      </tags>
  </entry>
  <entry>
    <title>8-search</title>
    <url>/algorithm-note/8-search/</url>
    <content><![CDATA[<h1 id="搜索专题">搜索专题</h1>
<p>《算法笔记》第8章搜索专题，包括BFS和DFS。</p>
<span id="more"></span>
<h2 id="深度优先搜索">深度优先搜索</h2>
<p>Depth first search（DFS），以深度作为首要因素，每次都遍历一条完整的路径，然后返回上一层的岔路口，继续进行深度的遍历。</p>
<p>假设是在一个迷宫当中寻找出口，那么DFS的意思是每次找到一个岔路口，做个标记，然后找第一条岔路，往下走，找不到出口就回来，继续下一条岔路。对于岔路口的标记属于越往后出现的岔路，越是会被首先回顾，这个是属于栈的思想。</p>
<p>在实现的时候，我们可以使用递归的方式。对于递归的子函数，新生成的子函数会被首先返回，递归的实现本身就依赖于系统栈。</p>
<p>DFS的伪代码：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">DFS</span><span class="params">(<span class="keyword">int</span> depth, <span class="keyword">int</span> args)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 对当前的depth进行某些题目需要的操作，判断是否满足条件，比如是否越界，是否找到迷宫的出口等等</span></span><br><span class="line">  <span class="keyword">if</span> (depth==MAX_DEPTH) <span class="keyword">return</span>;</span><br><span class="line">  <span class="comment">// 开始遍历岔路，不同的岔路深度是一样的，区别在于其它和题目相关的条件</span></span><br><span class="line">  DFS(depth+<span class="number">1</span>, args1);</span><br><span class="line">  DFS(depth+<span class="number">1</span>, args2);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="广度优先搜索">广度优先搜索</h2>
<p>Breadth first search（BFS），以广度作为第一关键词，它访问当前岔路口的所有岔路，而不是顺着某一条岔路继续走下去。</p>
<p>因此，在实现的时候，就不能再采用DFS以递归为途径的遍历手段。还是以迷宫为背景，假设我们希望找到最短的路径是什么。对于当前到达的岔路口S，依次访问岔路A、岔路B、岔路C....，如果都不满足条件，再顺着岔路A的下一个岔路继续走。可以看出来，这个实际是队列的思想，把当前路口的所有岔路入队，然后依次出队，对于出队的岔路，把它的子岔路再入队等待后续的访问。</p>
<p>BFS的伪代码：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">BFS</span><span class="params">(<span class="keyword">int</span> data, <span class="keyword">int</span> args)</span> </span>&#123;</span><br><span class="line">  <span class="built_in">queue</span>&lt;node&gt; q; <span class="comment">// BFS需要用到的队列</span></span><br><span class="line">  <span class="comment">// 创建队首元素</span></span><br><span class="line">  node * n = <span class="keyword">new</span> node;</span><br><span class="line">  n.data = data;</span><br><span class="line">  q.push(n);</span><br><span class="line">  <span class="keyword">while</span>(!q.empty()) &#123;</span><br><span class="line">    <span class="comment">// 当前的队首元素出队</span></span><br><span class="line">    node n_tmp = q.front();</span><br><span class="line">    q.pop();</span><br><span class="line">    <span class="comment">// 进行某些判断，例如求和，是否超出边界等</span></span><br><span class="line">    <span class="comment">// n_tmp的所有子结点入队</span></span><br><span class="line">    <span class="keyword">while</span>(n_tmp的所有子结点) &#123;</span><br><span class="line">      q.push()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>book</tag>
      </tags>
  </entry>
  <entry>
    <title>9-data-structure2</title>
    <url>/algorithm-note/9-data-structure2/</url>
    <content><![CDATA[<h1 id="数据结构专题2">数据结构专题2</h1>
<p>《算法笔记》第九章，数据结构专题2，主要涉及树、并查集、堆等。</p>
<span id="more"></span>
<h2 id="树和二叉树">树和二叉树</h2>
<p>树的基本定义：在数据结构中的树只有一个根结点，若干个节点和边，并且所有的节点是连通的且无闭环。</p>
<p>几个比较实用的性质：</p>
<ul>
<li>树可以是空的，此时连根结点都没有，叫做空树。</li>
<li>树的层次layer，是从根结点作为第一层开始计算的，依次递增。</li>
<li>结点的子树数量叫做度degree，注意不计算空树。结点的最大度数看做是树的度数，叫做宽度。</li>
<li>树中的边只能连接两个结点，因此，对于有<span class="math inline">\(n\)</span>个结点的树，一定有<span class="math inline">\(n-1\)</span>个边。如果有<span class="math inline">\(n\)</span>个相互连通的结点，并且有<span class="math inline">\(n-1\)</span>个边，那么一定是一棵树。</li>
<li>叶子结点是度为0的结点。</li>
<li>节点的深度是从根结点深度为1开始算起到该节点的层数；节点的高度是从最底层的叶子结点到该节点的层数。类似于树的宽度定义，树的深度是结点的最大深度，树的高度是根结点的高度。</li>
<li>多棵树组合在一起叫做森林forest。</li>
</ul>
<p>二叉树的递归定义：</p>
<ol type="1">
<li>二叉树可以是空树（没有根结点）；</li>
<li>二叉树由根结点、左子树、右子树组成。左右子树都是二叉树；</li>
</ol>
<p>二叉树一定是度为2的树，但是度为2的树不一定是二叉树，因为二叉树的左右子树是严格区分的，不能互换。</p>
<p>两种特殊的二叉树：</p>
<ol type="1">
<li>满二叉树：除去叶子结点外，所有的节点的左右子树都是满的（即树的每一层都是满的，包括最底层）</li>
<li>完全二叉树：除去最底层外，所有层都是满结点的；并且最底层的叶子结点从左到右是连续的。</li>
</ol>
<p>二叉树的存储，一般的二叉树使用链表来定义（对于完全二叉树可以使用数组来定义）：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Node</span> &#123;</span></span><br><span class="line">	<span class="keyword">typename</span> data;</span><br><span class="line">  Node * lchild; <span class="comment">// lchild == NULL表示左子树为空</span></span><br><span class="line">  Node * rchild; <span class="comment">// rchild == NULL表示右子树为空</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 定义一棵空树</span></span><br><span class="line">Node * root = <span class="literal">NULL</span>;</span><br></pre></td></tr></table></figure>
<p>新建一个结点，但是还未加入到树中，具体要增加到树的哪个地方依赖于之后的具体二叉树的性质。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function">Node * <span class="title">newNode</span> <span class="params">(<span class="keyword">int</span> data)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// c语言和c++两种创建方式</span></span><br><span class="line">  Node * node = <span class="keyword">new</span> Node;</span><br><span class="line">  Node * node = (Node*)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(Node));</span><br><span class="line">  node-&gt;data = data;</span><br><span class="line">  node-&gt;lchild = node-&gt;rchild = <span class="literal">NULL</span>;</span><br><span class="line">  <span class="keyword">return</span> node;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>二叉树的查找与修改：查找到所有满足查询条件的结点，并且可以修改数据</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 下面的代码是从左开始的DFS</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">search</span><span class="params">(Node* current_root, <span class="keyword">int</span> x, <span class="keyword">int</span> new_data)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (current_root==<span class="literal">NULL</span>) <span class="keyword">return</span>; <span class="comment">// 空树返回</span></span><br><span class="line">  <span class="keyword">if</span> (current_root-&gt;data == x)</span><br><span class="line">    current_root-&gt;data = new_data;</span><br><span class="line">  <span class="comment">// 继续查询左子树</span></span><br><span class="line">  search(current_root-&gt;lchild, x, new_data);</span><br><span class="line">  <span class="comment">// 查询右子树</span></span><br><span class="line">  search(current_root-&gt;rchild, x, new_data);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>二叉树的插入依赖于具体二叉树的性质，但是一般来说，二叉树的插入就是在不满足某种条件的位置，并且这个位置一般是确定唯一的，否则就会有多个位置适合插入了。</p>
<p>下面是一个二叉树插入新结点的伪代码：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 下面的函数需要特殊注意，current_root是引用形式，这样方便直接修改传入的root值</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">insert</span><span class="params">(Node* &amp;current_root, <span class="keyword">int</span> new_data)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span>(current_root==<span class="literal">NULL</span>) &#123;</span><br><span class="line">		root = newNode(new_data); <span class="comment">// 没有找到合适位置，就是目前的空树插入新结点</span></span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">if</span> (根据new_data和目前节点的数据判断发现应该插入到左子树) &#123;</span><br><span class="line">    insert(current_root-&gt;lchild, new_data);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">else</span> &#123; <span class="comment">// 根据new_data和目前节点的数据判断发现应该插入到右子树</span></span><br><span class="line">    insert(current_root-&gt;rchild, new_data);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>二叉树的创建，就是重复上面的插入函数：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function">Node* <span class="title">create</span><span class="params">(<span class="keyword">int</span> data[], <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">  Node* root = <span class="literal">NULL</span>; <span class="comment">// 注意不能new Node，这样就不是空树了</span></span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n ; i++) &#123;</span><br><span class="line">    insert(root, data[i]);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> root;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面是一般的二叉树存储，对于完全二叉树，在元素数量不多的情况下，可以直接使用数组存储。</p>
<p>将根结点放在数据的位置<span class="math inline">\(1\)</span>，不能从<span class="math inline">\(0\)</span>开始存放，否则会丧失下面的规律：</p>
<p>对于结点<span class="math inline">\(i\)</span>，它的左子树位置一定是<span class="math inline">\(2i\)</span>，右子树位置是<span class="math inline">\(2i+1\)</span>。</p>
<p>判断是否是叶子结点，该结点的左子树位置超过了树的结点数量<span class="math inline">\(n\)</span>。</p>
<h2 id="二叉树的遍历">二叉树的遍历</h2>
<p>这里讨论二叉树的四种遍历方法：</p>
<ul>
<li>先序遍历：根结点、左子树、右子树</li>
<li>中序遍历：左子树、根结点、右子树</li>
<li>后序遍历：左子树、右子树、根结点</li>
<li>层序遍历：BFS，访问同一层的结点结束后，再访问下一层</li>
</ul>
<p>在上面的遍历过程中，可以发现，总是先访问左子树，再访问右子树。</p>
<p>下面是先序遍历的函数</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">preSearch</span><span class="params">(Node* root)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span>(root==<span class="literal">NULL</span>) <span class="keyword">return</span>;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">&quot;%d &quot;</span>, root-&gt;data);</span><br><span class="line">  preSearch(root-&gt;lchild);</span><br><span class="line">  preSearch(root-&gt;rchild);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 中序与后序遍历就是上面代码的顺序改变下即可</span></span><br></pre></td></tr></table></figure>
<p>层序遍历，利用到之前学的使用队列实现BFS</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">layerSearch</span><span class="params">(Node* root)</span> </span>&#123;</span><br><span class="line">  <span class="built_in">queue</span>&lt;Node*&gt; q; <span class="comment">// 注意是结点指针的队列</span></span><br><span class="line">  q.push(root);</span><br><span class="line">  <span class="keyword">while</span>(!q.empty()) &#123;</span><br><span class="line">    Node* tmp_node = q.front();</span><br><span class="line">    q.pop();</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;%d &quot;</span>, tmp_node-&gt;data);</span><br><span class="line">    <span class="comment">// 左右子树入队</span></span><br><span class="line">    q.push(tmp_node-&gt;lchild);</span><br><span class="line">    q.push(tmp_node-&gt;rchild);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>先序、中序以及后序遍历的作用在于，可以根据先序+中序或者后序+中序的输出，来唯一的重建树。先序或者后序都可以确定当前序列的根结点（第一位或者最后一位），然后结合中序就可以找到左右子树。</p>
<p>利用先序+中序重建树，如果两者对应的序列数组是<span class="math inline">\([preL, preR]\)</span>和<span class="math inline">\([inL, inR]\)</span>，那么根结点是<span class="math inline">\(preL\)</span>位置的元素，根据根结点寻找根结点在中序中的位置假设为<span class="math inline">\(k\)</span>，那么对应的左子树的先序和中序序列是<span class="math inline">\([preL+1, preL+k-inL]\)</span>，<span class="math inline">\([inL, k-1]\)</span>；右子树的先序序列是<span class="math inline">\([preL+k-inL+1, preR]\)</span>，中序序列是<span class="math inline">\([k+1, inR]\)</span>。</p>
<p>使用递归的方式就可以实现重建树。</p>
<p>二叉树也可以使用数组的方式来实现，指针变为数组的下标，左右子树都指向数组的下标，同时，如果是空树就设为-1，这里不赘述具体实现。</p>
<h2 id="树的遍历">树的遍历</h2>
<p>一般的树有多个子结点，并且不限制左右树顺序。</p>
<p>对于多个子树，当然可以使用链表来保存，但是有些麻烦，这里按照书上的说明统一使用静态写法，</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Node</span> &#123;</span></span><br><span class="line">  <span class="keyword">typename</span> data;</span><br><span class="line">  <span class="built_in">vector</span> child;</span><br><span class="line">&#125; node[maxn];</span><br></pre></td></tr></table></figure>
<p>创建新结点的办法，类似与前面的静态二叉树：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> index = <span class="number">0</span>; <span class="comment">// 记录当前的新元素在tree数组中的位置，index下没有数据</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">newNode</span><span class="params">(<span class="keyword">int</span> x)</span> </span>&#123;</span><br><span class="line">	node[index].data = x;</span><br><span class="line">  node[index].child.clear();</span><br><span class="line">  <span class="keyword">return</span> index++;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="二叉查找树bst">二叉查找树（BST）</h2>
<p>二叉查找树就是满足左子树中的元素都<span class="math inline">\(&lt;=\)</span>根结点，右子树都<span class="math inline">\(&gt;\)</span>根结点的二叉树。当然，<span class="math inline">\(==\)</span>根结点的元素到底在哪颗子树存放可以看题目的具体要求。</p>
<p>bst的查找：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function">Node* <span class="title">search</span><span class="params">(Node* root, <span class="keyword">int</span> x)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span>(root==<span class="literal">NULL</span>) <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">  <span class="keyword">if</span>(x==root-&gt;data) &#123;</span><br><span class="line">    <span class="keyword">return</span> root;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (x&lt;root-&gt;data) <span class="keyword">return</span> search(root-&gt;lchild, x);</span><br><span class="line">  <span class="keyword">else</span> <span class="keyword">return</span> search(root-&gt;rchild, x);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>bst的插入：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">insert</span><span class="params">(Node* &amp;root, <span class="keyword">int</span> x)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span>(root==<span class="literal">NULL</span>) &#123;</span><br><span class="line">    node = <span class="keyword">new</span> Node;</span><br><span class="line">    node-&gt;data = x;</span><br><span class="line">    node-&gt;lchild = node-&gt;rchild = <span class="literal">NULL</span>;</span><br><span class="line">    root = node;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (x&lt;=root-&gt;data) insert(root-&gt;lchild, x);</span><br><span class="line">  <span class="keyword">else</span> insert(root-&gt;rchild, x);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>bst的删除比较特殊，主要是涉及如何让结点被删除后，以该结点为根结点的整棵树还能够保证原来二叉查找树的性质。</p>
<p>有两种办法：用该结点的前驱或者后继来替换该结点。</p>
<ul>
<li>前驱：某个节点左子树中的最大结点，在树上表现为左子树的最右结点（可能有自己的左子树）</li>
<li>后继：某个结点右子树中的最小结点，在树上表现为右子树的最左结点（可能有自己的右子树）</li>
</ul>
<p>寻找前驱和后继：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 寻找某棵树的最大结点</span></span><br><span class="line"><span class="function">Node * <span class="title">findMax</span><span class="params">(Node* root)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">while</span>(root!=<span class="literal">NULL</span>) &#123;</span><br><span class="line">    root = root-&gt;rchild;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">Node* preMaxNode = findMax(root-&gt;lchild);</span><br><span class="line"><span class="comment">// 寻找某棵树的最小结点</span></span><br><span class="line"><span class="function">Node * <span class="title">findMin</span><span class="params">(Node* root)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">while</span>(root!=<span class="literal">NULL</span>) &#123;</span><br><span class="line">    root = root-&gt;lchild;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">Node* postMinNode = findMin(root-&gt;rchild);</span><br></pre></td></tr></table></figure>
<p>bst的删除操作：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">deleteNode</span><span class="params">(Node* &amp;root, <span class="keyword">int</span> x)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span>(root==<span class="literal">NULL</span>) <span class="keyword">return</span>;</span><br><span class="line">  <span class="keyword">if</span>(root-&gt;data == x) &#123;</span><br><span class="line">    <span class="comment">// 如果要删除的结点是叶子结点</span></span><br><span class="line">    <span class="keyword">if</span>(root-&gt;lchild == <span class="literal">NULL</span> &amp;&amp; root-&gt;rchild==<span class="literal">NULL</span>) &#123;</span><br><span class="line">      <span class="keyword">delete</span>(root);</span><br><span class="line">      root=<span class="literal">NULL</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 寻找前驱</span></span><br><span class="line">    <span class="keyword">if</span>(root-&gt;lchild != <span class="literal">NULL</span>) &#123;</span><br><span class="line">      Node* preMaxNode = findMax(root-&gt;lchild);</span><br><span class="line">      root-&gt;data = preMaxNode-&gt;data; <span class="comment">// 替换为前驱</span></span><br><span class="line">      deleteNode(preMaxNode, preMaxNode-&gt;data); <span class="comment">// 删除前驱</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">      Node* postMinNode = findMin(root-&gt;rchild);</span><br><span class="line">      root-&gt;data = postMinNode-&gt;data; <span class="comment">// 替换为后继</span></span><br><span class="line">      deleteNode(postMinNode, postMinNode-&gt;data); <span class="comment">// 删除后继</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">else</span> <span class="keyword">if</span>(root-&gt;data &lt; x) &#123;</span><br><span class="line">    deleteNode(root-&gt;rchild, x);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">else</span> &#123;</span><br><span class="line">    deleteNode(root-&gt;lchild, x);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="平衡二叉树avl">平衡二叉树（AVL）</h2>
<p>平衡二叉树是对于前面二叉查找树的改进，“平衡“的意思是保证左右子树的高度差不超过1，这样总能够保证找寻所需元素时的搜索效率在<span class="math inline">\(O(log(n))\)</span>内。</p>
<p>平衡二叉树是两位前苏联的数学家提出来的，取他们名的大写字母命名为AVL树。在AVL树中的每个结点会额外记录一下当前结点的高度，同时利用左右子树的结点高度差<span class="math inline">\(左子树高度-右子树高度\)</span>，可以可以计算当前结点的<em>平衡因子</em>。</p>
<p>AVL树的难点在于插入与删除，一个结点的定义</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Node</span> &#123;</span></span><br><span class="line">  <span class="keyword">int</span> data;</span><br><span class="line">  <span class="keyword">int</span> height; <span class="comment">// 默认为1</span></span><br><span class="line">  Node* lchild, rchild;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>新建结点</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function">Node* <span class="title">newNode</span><span class="params">(<span class="keyword">int</span> x)</span> </span>&#123;</span><br><span class="line">  Node* node = <span class="keyword">new</span> Node;</span><br><span class="line">  node-&gt;data = x;</span><br><span class="line">  node-&gt;height = <span class="number">1</span>; <span class="comment">// 新增的定义</span></span><br><span class="line">  node-&gt;lchild = node-&gt;rchild = <span class="literal">NULL</span>;</span><br><span class="line">  <span class="keyword">return</span> node;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>查询当前结点的高度与计算平衡因子</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">getHight</span><span class="params">(Node* node)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> node-&gt;height;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">getBalanceFactor</span><span class="params">(Node * node)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> getHeight(node-&gt;lchild) - getHeight(node-&gt;rchild);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>更新节点高度</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">updateHeight</span><span class="params">(Node* node)</span> </span>&#123;</span><br><span class="line">  node-&gt;height = max(getHeight(node-&gt;lchild), getHeight(node-&gt;rchild)) + <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>AVL树的查询操作，与一般的BST树一样，这里不重复叙述。</p>
<p>AVL树的插入操作，比较复杂，需要考虑在插入新结点后，如何保持”平衡“？这里有两种办法调整当前树的左右子树高度，<em>左旋</em>和<em>右旋</em>：</p>
<ul>
<li>左旋：通过旋转，让当前根结点变为原来右子树根结点的左结点，原来右子树根结点成为新根结点</li>
<li>右旋：通过旋转，让当前根结点变为原来左子树根结点的右结点，原来左子树根结点成为新根结点</li>
</ul>
<p>详细的旋转过程可以参考《算法笔记》上的图示，这里提供代码，核心思想在于：</p>
<ul>
<li>左旋：当前根结点变为新的左结点，当前根结点的右子树链接到右结点（新根结点）的左子树</li>
<li>右旋：当前根结点变为新的右结点，当前根结点的左子树链接到左结点（新根结点）的右子树</li>
</ul>
<p>代码：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 左旋</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">L</span><span class="params">(Node* &amp;root)</span> </span>&#123;</span><br><span class="line">  Node * tmp = root-&gt;rchild; <span class="comment">// 新根结点是原右子树</span></span><br><span class="line">  root-&gt;rchild = tmp-&gt;lchild; <span class="comment">// 当前根结点右子树变为原右子树的左子树</span></span><br><span class="line">  tmp-&gt;lchild = root; <span class="comment">// 新根结点的左子树是原根结点</span></span><br><span class="line">  updateHeight(root); <span class="comment">// 先更新新树的子树</span></span><br><span class="line">  updateHeight(tmp); <span class="comment">// 然后更新根结点</span></span><br><span class="line">  root = tmp; <span class="comment">// 将root改为新的根结点</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 右旋</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">R</span><span class="params">(Node* &amp;root)</span> </span>&#123;</span><br><span class="line">  Node * tmp = root-&gt;lchild; <span class="comment">// 新根结点是原左子树</span></span><br><span class="line">  root-&gt;lchild = tmp-&gt;rchild; <span class="comment">// 当前根结点左子树变为原左子树的右子树</span></span><br><span class="line">  tmp-&gt;rchild = root; <span class="comment">// 新根结点的右子树是原根结点</span></span><br><span class="line">  updateHeight(root); <span class="comment">// 先更新新树的子树</span></span><br><span class="line">  updateHeight(tmp); <span class="comment">// 然后更新根结点</span></span><br><span class="line">  root = tmp; <span class="comment">// 将root改为新的根结点</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>接下来可以讨论AVL树插入新元素的操作了。当AVL树插入新结点后，可能造成左子树比右子树高<span class="math inline">\(2\)</span>，或者是右子树比左子树高<span class="math inline">\(2\)</span>。一共有四种可能（更详细的讨论在书上）：</p>
<ol type="1">
<li>LL型：左子树高-&gt;左子树的左子树高；右旋根结点解决</li>
<li>LR型：左子树高-&gt;左子树的右子树高；先左旋左子树，后右旋根结点</li>
<li>RR型：右子树高-&gt;右子树的右子树高；左旋解决</li>
<li>RL型：右子树高-&gt;右子树的左子树高；右旋右子树，后左旋根结点</li>
</ol>
<p>插入操作的代码：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">insert</span><span class="params">(Node* &amp;root, <span class="keyword">int</span> x)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (root==<span class="literal">NULL</span>) &#123;</span><br><span class="line">    root = newNode(x);</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span>(root-&gt;data &gt; x) &#123; <span class="comment">// 插入到左子树</span></span><br><span class="line">    insert(root-&gt;lchild, x);</span><br><span class="line">    updateHeight(root);</span><br><span class="line">    <span class="keyword">if</span>(getBalanceFactor(root) == <span class="number">2</span>) &#123; <span class="comment">// 如果左子树失衡了，因为是递归，总能保证从下往上调整失衡树</span></span><br><span class="line">			<span class="keyword">if</span>(getBalanceFactor(root-&gt;lchild) == <span class="number">1</span>) &#123; <span class="comment">// LL型</span></span><br><span class="line">        R(root);</span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span>(getBalanceFactor(root-&gt;lchild) == <span class="number">-1</span>) &#123; <span class="comment">// LR型</span></span><br><span class="line">        L(root-&gt;lchild);</span><br><span class="line">        R(root);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">else</span> &#123; <span class="comment">// 插入右子树</span></span><br><span class="line">    insert(root-&gt;rchild, x);</span><br><span class="line">    updateHeight(root);</span><br><span class="line">    <span class="keyword">if</span>(getBalanceFactor(root) == <span class="number">-2</span>) &#123; <span class="comment">// 如果左子树失衡了</span></span><br><span class="line">			<span class="keyword">if</span>(getBalanceFactor(root-&gt;rchild) == <span class="number">1</span>) &#123; <span class="comment">// RL型</span></span><br><span class="line">        R(root-&gt;child);</span><br><span class="line">        L(root);</span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span>(getBalanceFactor(root-&gt;rchild) == <span class="number">-1</span>) &#123; <span class="comment">// RR型</span></span><br><span class="line">        L(root);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>AVL树的删除，书上没有提及。</p>
<h2 id="并查集">并查集</h2>
<p>并查集是一种维护集合的数据结构，主要针对合并、查找与集合三个单词。</p>
<p>并查集的实现就是一个数组，</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> father[N];</span><br></pre></td></tr></table></figure>
<p><code>father[i]</code>指的是元素<code>i</code>的父亲结点。在并查集中，一个集合只有一个结点满足<code>father[i]=i</code>，叫做根结点，把这个根结点看做是集合的标志。</p>
<p>并查集支持的操作，</p>
<p>初始化：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span>(<span class="keyword">int</span> i =<span class="number">0</span>;i&lt;n;++i) &#123;</span><br><span class="line">  father[i] = i; <span class="comment">// 初始化时，默认每个结点的父结点都是自身</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>查找当前集合的根结点：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">findFather</span><span class="params">(<span class="keyword">int</span> i)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">while</span>(father[i]!=i) &#123;</span><br><span class="line">    i = father[i];</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> i;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>合并操作，如果两个元素实际是属于同一集合，就合并这两个元素分属的两集合的根结点，只保留一个根结点：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Union</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> fatherX = findFather(x);</span><br><span class="line">  <span class="keyword">int</span> fatherY = findFather(y);</span><br><span class="line">  <span class="keyword">if</span>(fatehrX!=fatherY) &#123;</span><br><span class="line">    father[fatherX] = fatherY; <span class="comment">// 集合X的根结点变换为集合Y的根结点</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>路径压缩操作，是一种简化<code>father</code>数组，从而提高寻找根结点效率的方法，直接让<code>father[i]</code>指向根结点，而不是父结点。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">findFather</span><span class="params">(<span class="keyword">int</span> i)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> tmp = i;</span><br><span class="line">  <span class="keyword">while</span>(father[i]!=i) &#123;</span><br><span class="line">    i = father[i];</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 此时i是根结点，在此从i结点出发，逐次修改结点的父结点为根结点</span></span><br><span class="line">  <span class="keyword">while</span>(father[tmp]!=tmp) &#123;</span><br><span class="line">    <span class="keyword">int</span> current_node = tmp;</span><br><span class="line">    tmp = father[tmp];</span><br><span class="line">    father[current_node] = i;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> i;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="堆">堆</h2>
<p>堆是一课完全二叉树。它区别于BST和AVL的是，它只规定根结点要<code>&gt;=</code>或者<code>&lt;=</code>两个结点，根结点是当前树最大值的完全二叉树叫做大顶堆，反之叫做小顶堆。</p>
<p>如果现在我们已经有一棵完全二叉树，如何把它调整为一个大顶堆？</p>
<p>原则是从右到左，从下到上依次检查当前结点是否满足条件：</p>
<ul>
<li>叶子结点无需调整</li>
<li>非叶子结点，将当前根结点与左、右子结点的最大值进行比较，如果当前根结点已经是最大值就无需变动；否则就互换，互换之后，由于左右子树在之前的调整过程中已经保证了大顶堆，那么新的根结点一定是最大值，只需要继续让被换以后的根结点与下一级子树进行比较即可。重复这个过程，直至无需再调整位置。</li>
</ul>
<p>类似的，如果我们想新建一个小顶堆，只需要总是寻找根结点、左、右结点中的最小值即可，整体过程与上述过程类似。</p>
<p>从代码的角度分析，如何实现大顶堆？按照之前的讨论，完全二叉树可以使用根结点在位置<span class="math inline">\(1\)</span>的数组进行存储。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> maxn = <span class="number">110</span>;</span><br><span class="line"><span class="keyword">int</span> heap[maxn]; <span class="comment">// 堆</span></span><br></pre></td></tr></table></figure>
<p>实现下上面的调整过程，让当前位置的<span class="math inline">\(i\)</span>一直向下调整。</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// high一般为n</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">downAdjust</span><span class="params">(<span class="keyword">int</span> low, <span class="keyword">int</span> high)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> i = low, child = <span class="number">2</span> * i;</span><br><span class="line">  <span class="keyword">while</span>(child &lt;= high) &#123;</span><br><span class="line">    <span class="keyword">if</span>(child+<span class="number">1</span> &lt;= high &amp;&amp; heap[child] &lt; heap[child]) &#123;</span><br><span class="line">      child = lchild + <span class="number">1</span>; <span class="comment">// 寻找child中的最大值，此时为右结点</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(heap[i]&gt;=heap[child]) </span><br><span class="line">      <span class="keyword">break</span>; <span class="comment">// 如果当前根结点已经保证是最大值，退出</span></span><br><span class="line">    swap(heap[i], heap[child]);</span><br><span class="line">    <span class="comment">// 交换位置后，继续遍历子树</span></span><br><span class="line">    i = child;</span><br><span class="line">    child = <span class="number">2</span> * child;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>建堆过程，从最右边，最低层的非叶子结点开始遍历，逐个执行上述函数：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">createHeap</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 如果完全二叉树有n个结点，那么非叶子结点有 取下限(n/2) 个</span></span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i = n/<span class="number">2</span>; i&gt;=<span class="number">1</span>; i--) &#123;</span><br><span class="line">    downAdjust(i, n);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>删除堆顶元素，让最末端的叶子结点替换堆顶元素，然后执行向下调整</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">deleteTop</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  heap[<span class="number">1</span>] = heap[n--];</span><br><span class="line">  downAdjust(<span class="number">1</span>, n);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>新增元素，让新元素放在堆的最末端，然后执行从下到上的调整</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// low一般为1， high是当前要进行向上调整的元素</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">upAdjust</span><span class="params">(<span class="keyword">int</span> low, <span class="keyword">int</span> high)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> i = high, parent = high / <span class="number">2</span>;</span><br><span class="line">  <span class="keyword">while</span>(high&gt;=low) &#123;</span><br><span class="line">    <span class="keyword">if</span> (heap[parent]&gt;=heap[i]) </span><br><span class="line">      <span class="keyword">break</span>; <span class="comment">// 父亲结点已经比当前结点大，无需再调整</span></span><br><span class="line">    swap(heap[parent], heap[i]);</span><br><span class="line">    i = parent;</span><br><span class="line">    parent = parent / <span class="number">2</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">insert</span><span class="params">(<span class="keyword">int</span> x)</span> </span>&#123;</span><br><span class="line">  heap[++n] = x;</span><br><span class="line">  upAdjust(<span class="number">1</span>, n);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>堆的基本元素讲完后，我们可以使用堆来实现堆排序，下面是一个实现从小到大递增排序的例子：</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">heapSort</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 建好堆之后，每次把堆顶元素和最末端的元素互换，然后执行向下调整</span></span><br><span class="line">  crateHeap();</span><br><span class="line">  <span class="keyword">for</span>(<span class="keyword">int</span> i = n; i&gt;<span class="number">1</span>; --i) &#123;</span><br><span class="line">    swap(heap[<span class="number">1</span>], heap[i]);</span><br><span class="line">    downAdjust(<span class="number">1</span>, i<span class="number">-1</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="哈夫曼树">哈夫曼树</h2>
<p>哈夫曼树的构建方法：</p>
<ul>
<li>初始状态的n个结点看做是单独的n个二叉树</li>
<li>选择根结点值最小的两个结点进行合并，生成新的父结点，父结点的值是两个结点的和</li>
<li>重复上述过程，直至形成一棵完整的二叉树</li>
</ul>
<p>叶子结点的权值<span class="math inline">\(\times\)</span>根结点该叶子结点的路径长度，叫做改叶子结点的带权路径长度。一棵二叉树的带权路径长度Weight Path Length of tree（WPL）是所有叶子结点带权路径长度的和。哈夫曼树是满足一组叶子结点，带权路径长度最小的树。在建立哈夫曼树的过程中，能够保证哈夫曼树的WPL是最小的，但是可能存在多个哈夫曼树，比如可能同时存在多个相同的最小值结点。</p>
<p>哈夫曼树用来计算某个值的时候，可以考虑使用优先队列/小顶堆，每次取出最小的两个值合并后再入队。</p>
<p>现在考虑给字符用01编码的问题，如果随便给字符编码，就可能导致某个叶子结点的编码是另一个叶子结点编码的前缀，这样在使用这种编码过程的时候就可能出现问题。如果任一字符的编码都不是另一字符的编码前缀，这种编码方式叫做前缀编码。</p>
<p>对于这种情况，可以使用任意一棵二叉树，根结点到叶子结点的路径（左子树0，右子树1）作为该叶子结点的编码。它能够满足前缀编码的要求。</p>
<p>但是，另一个问题是如果我们希望一段字符串的01编码长度最小呢？这就可以使用哈夫曼编码来解决，统计字符串中各个字符的出现次数作为权值，然后建立哈夫曼树，就能够保证最后字符串的编码一定是最短的。</p>
]]></content>
      <categories>
        <category>algorithm</category>
      </categories>
      <tags>
        <tag>book</tag>
      </tags>
  </entry>
  <entry>
    <title>AM-GCN</title>
    <url>/gnn/AM-GCN/</url>
    <content><![CDATA[<h1 id="am-gcn-adaptive-multi-channel-graph-convolutional-networks">AM-GCN: Adaptive Multi-channel Graph Convolutional Networks</h1>
<p>2020-8 KDD 2020</p>
<p>AM-GCN主要针对的是节点分类任务，主要贡献包括两点：</p>
<ol type="1">
<li>探究了之前的GCN方法是否能够较好的同时捕获topological structures和node features</li>
<li>设计了 a novel adaptive multi-channel GCN framework, 能够适应的同时捕获两种不同的信息</li>
</ol>
<span id="more"></span>
<h2 id="introduction">1 Introduction</h2>
<p>之前的图卷积神经网络方法使用节点特征（node features）作为初始输入，然后进行图卷积操作。</p>
<p>作者提出的问题：</p>
<blockquote>
<p>What information do GCNs really learn and fuse from topological structures and node features?</p>
</blockquote>
<p>一般的GNN中涉及到两种信息：</p>
<ul>
<li>拓扑结构（主要指一个节点周围有哪些邻居节点）</li>
<li>节点信息（主要指节点本身的特征信息，比如论文节点的标题、关键字等文本信息）</li>
</ul>
<p>这两个信息都可以用来进行图上的预测任务，但是哪个信息或者两个一起作用，对于最终的预测任务影响比较大？</p>
<p>AM-GCN主要针对的是节点分类任务，主要贡献包括两点：</p>
<ol type="1">
<li>探究了之前的GCN方法是否能够较好的同时捕获topological structures和node features</li>
<li>设计了 a novel adaptive multi-channel GCN framework, 能够适应的同时捕获两种不同的信息</li>
</ol>
<h2 id="fusion-capability-of-gcns">2 FUSION CAPABILITY OF GCNS</h2>
<p>探究GCN学习结构信息和节点特征信息的能力，设计了两个实验：</p>
<h3 id="case-1-random-topology-and-correlated-node-features">Case 1: Random Topology and Correlated Node Features</h3>
<p>实验设置：</p>
<ul>
<li>900节点，3 class</li>
<li>0.3的概率随机生成边</li>
<li>对于3类节点，节点特征使用三个相同协方差，不同均值的高斯分布初始化</li>
<li>GCN与基于节点特征的MLP对比</li>
</ul>
<p>结果：GCN分类准确率75.2%，MLP准确率100%</p>
<h3 id="case-2-correlated-topology-and-random-node-features">Case 2: Correlated Topology and Random Node Features</h3>
<p>实验设置：</p>
<ul>
<li>900节点，3 class</li>
<li>设计3 community，community内部建立边的概率是0.03， community内部建立边的概率是0.0015</li>
<li>随机生成节点特征</li>
<li>GCN与Deepwalk（忽略节点特征）</li>
</ul>
<p>结果：GCN分类准确率87%，MLP准确率100%</p>
<h2 id="am-gcn-the-proposed-model">3 AM-GCN: THE PROPOSED MODEL</h2>
<p>对于一个原始图的输入<span class="math inline">\(G=(\mathbf{A},\mathbf{X})\)</span>，<span class="math inline">\(\mathbf{A}\)</span>是邻接矩阵，<span class="math inline">\(\mathbf{X}\)</span>是节点特征矩阵，构造两个图：</p>
<ul>
<li>Topology Graph：<span class="math inline">\(\mathbf{A}_t=\mathbf{A}\)</span>，图原始的结构，与GCN一致</li>
<li>Feature Graph：<span class="math inline">\(\mathbf{A}_f\)</span>，该图中一个节点的相邻节点是k个特征相似的节点，不是原来的结构上的邻居节点</li>
</ul>
<p>构造Feature Graph，首先计算一个节点与其它所有节点的相似度，计算一个余弦相似度矩阵<span class="math inline">\(\mathbf{S}\in \mathbb{R}^{n\times n}\)</span> <span class="math display">\[
\mathbf{S}_{i,j}=\frac{\mathbf{x}_i\cdot \mathbf{x}_j}{|\mathbf{x}_i| |\mathbf{x}_j|}
\]</span> 然后对于节点<span class="math inline">\(i\)</span>，在<span class="math inline">\(\mathbf{S}\)</span>中选择前<span class="math inline">\(k\)</span>个相似度最大的节点作为feature node，得到<span class="math inline">\(\mathbf{A}_f\)</span>。</p>
<p>AM-GCN的整体模型图：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210110175132041.png" style="zoom:50%;" /></p>
<p>可以看到其中有三个主要核心模块，包括在Topology Graph和Feature Graph上的卷积，以及捕获两者共有特征的卷积。</p>
<h3 id="specific-convolution-module">3.1 Specific Convolution Module</h3>
<p>在Topology Graph上的卷积： <span class="math display">\[
\mathbf{Z}_t^{(l)} = ReLU(\tilde{D}^{-\frac{1}{2}}_t \tilde{A}_t \tilde{D}^{-\frac{1}{2}}_t \mathbf{Z}_t^{(l-1)} \mathbf{W}_t^{(l)} )
\]</span> 其中，<span class="math inline">\(\tilde{A}_t=A_t+I_t\)</span>，该卷积方法与GCN一模一样</p>
<p>在Feature Graph上的卷积： <span class="math display">\[
\mathbf{Z}_f^{(l)} = ReLU(\tilde{D}^{-\frac{1}{2}}_f \tilde{A}_f \tilde{D}^{-\frac{1}{2}}_f \mathbf{Z}_f^{(l-1)} \mathbf{W}_f^{(l)} )
\]</span> 其中，<span class="math inline">\(\tilde{A}_f=A_f+I_f\)</span></p>
<h3 id="common-convolution-module">3.2 Common Convolution Module</h3>
<p>实际上，feature space和topology space不是完全独立的，两者的信息可能互补然后一起可以用于预测任务。因此，AM-GCN设计了一个common module使用parameter sharing strategy捕获两者的通用特征。</p>
<p>在topology space中导出embedding： <span class="math display">\[
\mathbf{Z}_{ct}^{(l)} = ReLU(\tilde{D}^{-\frac{1}{2}}_t \tilde{A}_t \tilde{D}^{-\frac{1}{2}}_t \mathbf{Z}_{ct}^{(l-1)} \mathbf{W}_c^{(l)} )
\]</span> 在feature space中导出embedding： <span class="math display">\[
\mathbf{Z}_{cf}^{(l)} = ReLU(\tilde{D}^{-\frac{1}{2}}_f \tilde{A}_f \tilde{D}^{-\frac{1}{2}}_f \mathbf{Z}_{cf}^{(l-1)} \mathbf{W}_c^{(l)} )
\]</span> 最后，两个结合得到common embedding： <span class="math display">\[
\mathbf{Z}_{c} = (\mathbf{Z}_{ct} + \mathbf{Z}_{cf})/2
\]</span></p>
<h3 id="two-constraints">3.3 Two Constraints</h3>
<p>Consistency Constraint:</p>
<p>用来控制两个common embedding的consistency，首先将<span class="math inline">\(\mathbf{Z}_{ct}\)</span>和<span class="math inline">\(\mathbf{Z}_{cf}\)</span>使用<span class="math inline">\(l_2\)</span>正则归一化，然后得到下面节点之间的相似度 <span class="math display">\[
\mathbf{S}_T=\mathbf{Z}_{CTnor}\cdot \mathbf{Z}_{CTnor}^T \\
\mathbf{S}_F=\mathbf{Z}_{CFnor}\cdot \mathbf{Z}_{CFnor}^T
\]</span> 之后约束： <span class="math display">\[
\mathcal{L}_c=|| \mathbf{S}_T - \mathbf{S}_F ||^2
\]</span> Disparity Constraint:</p>
<p>因为<span class="math inline">\(\mathbf{Z}_{ct}^{(l)}\)</span>和<span class="math inline">\(\mathbf{Z}_{t}^{(l)}\)</span>都是从topology space中学习得到的，为了保证它们都是反映了不同的信息，因此使用Hilbert-Schmidt Independence Criterion (HSIC)加强它们的disparity。 <span class="math display">\[
HSIC(\mathbf{\mathbf{Z}_{T}},\ \mathbf{\mathbf{Z}_{CT}}) = (n-1)^{-2}tr(\mathbf{R} \mathbf{K}_T \mathbf{R} \mathbf{K}_{CT})
\]</span> 同样的，对于<span class="math inline">\(\mathbf{\mathbf{Z}_{F}},\ \mathbf{\mathbf{Z}_{CF}}\)</span>： <span class="math display">\[
HSIC(\mathbf{\mathbf{Z}_{F}},\ \mathbf{\mathbf{Z}_{CF}}) = (n-1)^{-2}tr(\mathbf{R} \mathbf{K}_F \mathbf{R} \mathbf{K}_{CF})
\]</span> 最终， <span class="math display">\[
\mathcal{L}_d =HSIC(\mathbf{\mathbf{Z}_{T}},\ \mathbf{\mathbf{Z}_{CT}}) +HSIC(\mathbf{\mathbf{Z}_{F}},\ \mathbf{\mathbf{Z}_{CF}})
\]</span></p>
<h3 id="attention-mechanism">3.4 Attention Mechanism</h3>
<p><span class="math display">\[
(\alpha_t, \alpha_c, \alpha_f)=att(\mathbf{Z}_T, \mathbf{Z}_C, \mathbf{Z}_F) 
\]</span></p>
<p>以计算<span class="math inline">\(\alpha_t\)</span>为例： <span class="math display">\[
\omega_T = \mathbf{q}^T \cdot tanh(\mathbf{W} \mathbf{z}_T + \mathbf{b}) \\
\alpha_T = softmax(\omega_T)
\]</span> 最终AM-GCN得到的节点embedding为： <span class="math display">\[
\mathbf{Z} = \mathbf{\alpha}_T \cdot \mathbf{Z}_T + \mathbf{\alpha}_C \cdot \mathbf{Z}_C + \mathbf{\alpha}_F \cdot \mathbf{Z}_F
\]</span></p>
<h2 id="experiments">4 EXPERIMENTS</h2>
<h3 id="node-classification">4.1 Node Classification</h3>
<p>在7个数据集上进行评估，数据集主要是论文的引文网络和社交网络</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210110175336414.png" style="zoom:50%;" /></p>
<p>注意其中的每个数据集训练集有三个级别，分别对应每一类的节点有标签的比例为20%， 40%和60%，测试集恒定每一类由1000个节点评估。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210110180203704.png" style="zoom:50%;" /></p>
<p>其中，kNN-GCN是AM-GCN只在feature graph上进行聚合。</p>
<p>对比GCN和kNN-GCN一个比较有意思的结果是不聚合邻居，有时候根据特征聚合相似的节点，也可能取得更好的结果。</p>
<h3 id="analysis-of-variants">4.2 Analysis of Variants</h3>
<p>比较设计的约束的作用：</p>
<ul>
<li><p>AM-GCN-w/o: AM-GCN without constraints <span class="math inline">\(L_c\)</span> and <span class="math inline">\(L_d\)</span></p></li>
<li><p>AM-GCN-c: AM-GCN with the consistency constraint <span class="math inline">\(L_c\)</span></p></li>
<li><p>AM-GCN-d: AM-GCN with the disparity constraint <span class="math inline">\(L_d\)</span></p></li>
</ul>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210110180340350.png" style="zoom:50%;" /></p>
<p>可以看出来，一般情况下，consistency constraint比disparity constraint更加重要</p>
<h3 id="visualization">4.3 Visualization</h3>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210110175813097.png" style="zoom:50%;" /></p>
<h3 id="analysis-of-attention-mechanism">4.4 Analysis of Attention Mechanism</h3>
<p>分析不同数据集下，三个不同embedding的注意力值的分布</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210110165439556.png" style="zoom:50%;" /></p>
<p>可以看到，不同数据集下，三个不同方面的embedding对应的注意力值不同，说明哪个方面包含了更加丰富的信息是依赖于具体数据的。</p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>GNN</category>
      </categories>
  </entry>
  <entry>
    <title>Beyond-Homo</title>
    <url>/gnn/Beyond-Homo/</url>
    <content><![CDATA[<h1 id="beyond-homophily-in-graph-neural-networks-current-limitations-and-effective-designs">Beyond Homophily in Graph Neural Networks: Current Limitations and Effective Designs</h1>
<p>本文讨论了graph的heterophily</p>
<span id="more"></span>
<div class="pdf-container" data-target="Beyond-Homophily-in-Graph-Neural-Networks.pdf" data-height="1000px"></div>
]]></content>
      <categories>
        <category>Paper</category>
        <category>GNN</category>
      </categories>
  </entry>
  <entry>
    <title>Deeper-insights-gcn</title>
    <url>/gnn/Deeper-insights-gcn/</url>
    <content><![CDATA[<h1 id="deeper-insights-into-graph-convolutional-networks-for-semi-supervised-learning">Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning</h1>
<p>AAAI 2018，引用量578。</p>
<p>作者对GCN的机制进行了探讨，认为GCN是Laplacian smoothing的一种特殊形式。并且形式化证明了当GCN堆叠到很多层时，所有节点的表示会趋向于同一表示。发现GCN在带有label的data较少的情况下，表现出的性能差，为了解决这一问题，提出了一种联合训练co-traning的方法，主要是使用随机游走和GCN两种方法，寻找不同label下的most confident vertices，然后使用这些most confident vertices继续训练。</p>
<span id="more"></span>
<blockquote>
<p>Many interesting problems in machine learning are being revisited with new deep learning tools. For graph-based semisupervised learning, a recent important development is graph convolutional networks (GCNs), which nicely integrate local vertex features and graph topology in the convolutional layers. Although the GCN model compares favorably with other state-of-the-art methods, its mechanisms are not clear and it still requires considerable amount of labeled data for validation and model selection.</p>
<p>In this paper, we develop deeper insights into the GCN model and address its fundamental limits. First, we show that the graph convolution of the GCN model is actually a special form of Laplacian smoothing, which is the key reason why GCNs work, but it also brings potential concerns of oversmoothing with many convolutional layers. Second, to overcome the limits of the GCN model with shallow architectures, we propose both co-training and self-training approaches to train GCNs. Our approaches signiﬁcantly improve GCNs in learning with very few labels, and exempt them from requiring additional labels for validation. Extensive experiments on benchmarks have veriﬁed our theory and proposals.</p>
</blockquote>
<p>作者认为的GCN的优点：</p>
<ul>
<li>图卷积-即拉普拉斯平滑操作，让不同class的特征能够在图上传播</li>
<li>MLP是一个强大的feature extractor</li>
</ul>
<p>缺点：</p>
<ul>
<li>图卷积是一个localized ﬁlter，当labeled data较少的时候，性能较差</li>
<li>使用neural network要求具备较多的训练数据以及使用额外的验证集进行模型选择</li>
</ul>
<p>作者证明graph convolution是laplacian smoothing的过程：</p>
<p>laplacian smoothing在1995年就被提出来，A signal processing approach to fair surface design.</p>
<p>定义为：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210504154544997.png" style="zoom:50%;" /></p>
<p>其中的<span class="math inline">\(\gamma\)</span>是参数，控制当前vertex和邻居vertex的特征的weight。<span class="math inline">\(\mathbf{x}_j\)</span>是邻居vertex。</p>
<p>将上面的形式写为矩阵形式： <span class="math display">\[
\begin{align}
Y &amp;= (1-\gamma)X + \gamma \tilde{A} \tilde{D}^{-1} X \\
&amp;= X - \gamma (X - \tilde{A} \tilde{D}^{-1} X ) \\
&amp;= X - \gamma \tilde{D}^{-1} (\tilde{D} - \tilde{A}) X \\
&amp;= X - \gamma \tilde{D}^{-1} \tilde{L} X
\end{align}
\]</span> 对上面的式子进行简化，令 <span class="math inline">\(\gamma = 1\)</span>，替换<span class="math inline">\(\tilde{D}^{-1} \tilde{L}\)</span>为<span class="math inline">\(\tilde{D}^{-1/2} \tilde{L} \tilde{D}^{-1/2}\)</span></p>
<p>得到下面的式子： <span class="math display">\[
\begin{align}
Y &amp;= X - \tilde{D}^{-1/2} \tilde{L} \tilde{D}^{-1/2} X \\
&amp;= (I - \tilde{D}^{-1/2} \tilde{L} \tilde{D}^{-1/2} ) X \\
&amp;= (I - \tilde{D}^{-1/2} (\tilde{D} - \tilde{A}) \tilde{D}^{-1/2} ) X \\
&amp;= \tilde{D}^{-1/2} \tilde{A} \tilde{D}^{-1/2} X
\end{align}
\]</span> 最后得到了GCN。</p>
<p>这一操作背后的原理是属于同一类的vertex倾向于在一个cluster中，smoothing操作让它们具有了相近的表示，这使得之后的classiﬁcation更加容易。</p>
<blockquote>
<p>The Laplacian smoothing computes the new features of a vertex as the weighted average of itself and its neighbors’. Since vertices in the same cluster tend to be densely connected, the smoothing makes their features similar, which makes the subsequent classiﬁcation task much easier.</p>
</blockquote>
<p>另外，为了让GCN能够在data更受限制的情况下进行学习，作者结合ParWalks这种随机游走的方法，寻找各个class下most confident vertices——the nearest neighbors to the labeled vertices of each class。</p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>GNN</category>
      </categories>
  </entry>
  <entry>
    <title>GCN</title>
    <url>/gnn/GCN/</url>
    <content><![CDATA[<h1 id="semi-supervised-classification-with-g-raph-convolutional-networks">SEMI-SUPERVISED CLASSIFICATION WITH G RAPH CONVOLUTIONAL NETWORKS</h1>
<p>ICLR 2017</p>
<p>贡献：提出了一种模拟卷积神经网络的变种，直接在图结构上进行类似的卷积操作的神经网络。通过对谱卷积神经网络（Spectral GNN）进行一阶估计简化（ﬁrst-order approximation），提出了一阶图卷积神经网络——GCN。</p>
<span id="more"></span>
<h2 id="introduction">1 Introduction</h2>
<p><strong>motivation</strong>：一般情景下，对图节点进行半监督的分类任务依赖于假设：</p>
<blockquote>
<p>connected nodes in the graph are likely to share the same label.</p>
</blockquote>
<p>相邻节点倾向于拥有相似的信息，传统的做法是设计基于图的正则项（graph-based regularization）加入到损失函数中，例如下面的图拉普拉斯正则项（a graph Laplacian regularization term）</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210628103015253.png" /></p>
<p><span class="math inline">\(L_0\)</span>是有监督的损失，针对graph中有标签的节点；<span class="math inline">\(L_{reg}\)</span>是图拉普拉斯正则项，它假设邻居拥有与中心节点相似的信息。通过设计上面的损失函数，实际上标签的信息就实现了平滑的传播（label information is smoothed over the graph）。</p>
<p>但是作者认为这种做法可能会限制模型的性能，因为一个graph的edge不一定总是描述节点之间的相似性，它可能包含其它的信息。比如可能是不相似的节点。</p>
<p><strong>method</strong>：作者没有继续使用上面的正则化方法，而是直接在graph上建立神经网络，在具体点就是直接基于邻接矩阵或者其它能够描述graph的形式作为输入，输出编码后的信息。</p>
<blockquote>
<p>we encode the graph structure directly using a neural network model <span class="math inline">\(f(X, A)\)</span> and train on a supervised target <span class="math inline">\(L_0\)</span> for all nodes with labels, thereby avoiding explicit graph-based regularization in the loss function.</p>
</blockquote>
<p>这样定义的<span class="math inline">\(f(X, A)\)</span> 在根据损失<span class="math inline">\(L_0\)</span>进行梯度计算时，就能够依赖邻接矩阵<span class="math inline">\(A\)</span>进行梯度的分发(distribute)，即实现了对于图结构的学习，无论是有label还是没有label的节点都能够学习到合适的representations。</p>
<blockquote>
<p>Conditioning <span class="math inline">\(f(\cdot)\)</span> on the adjacency matrix of the graph will allow the model to distribute gradient information from the supervised loss <span class="math inline">\(L_0\)</span> and will enable it to learn representations of nodes both with and without labels.</p>
</blockquote>
<h2 id="preliminaries">2 Preliminaries</h2>
<h3 id="graph-definition">Graph definition</h3>
<p>为了能够利用图结构进行学习，首先我们需要采取某种方式表示图。一个图的常用表达形式是<span class="math inline">\(G=(V,E)\)</span>，<span class="math inline">\(V\)</span>是节点（vertices）的集合，<span class="math inline">\(E\)</span>是边（edge）的集合。如果存在边<span class="math inline">\(e=u,v\)</span>，那么<span class="math inline">\(u\)</span>可以被称作<span class="math inline">\(v\)</span>的邻居，可以说节点<span class="math inline">\(u\)</span>和<span class="math inline">\(v\)</span>是邻接的（adjacent）。</p>
<p>图有几种不同的代数表达形式：</p>
<ul>
<li>邻接矩阵（Adjacency matrix）：对于一个简单图<span class="math inline">\(G=(V,E)\)</span>，邻接矩阵<span class="math inline">\(A\in \mathcal{R}^{n\times n}\)</span>定义为：</li>
</ul>
<p><span class="math display">\[
A_{ij} =
\begin{cases}
1,  &amp; \mbox{if }\{ v_i, v_j \}\in E \mbox{ and } i \not= j \\
0, &amp; \mbox{if }\mbox{ otherwise}
\end{cases}
\]</span></p>
<ul>
<li>度矩阵（Degree matrix）：度矩阵<span class="math inline">\(D\in \mathcal{R}^{n\times n}\)</span>是一个对角矩阵s：</li>
</ul>
<p><span class="math display">\[
D_{ii}=d(v_i)
\]</span></p>
<ul>
<li>拉普拉斯矩阵（Laplacian matrix）：拉普拉斯矩阵定义为度矩阵减去邻接矩阵，<span class="math inline">\(L=D-A\)</span>：</li>
</ul>
<p><span class="math display">\[
L_{ij} =
\begin{cases}
d(v_i),  &amp; \mbox{if } i = j \\
-1, &amp; \mbox{if }\{ v_i, v_j \}\in E \mbox{ and } i\not=j \\
0, &amp; \mbox{otherwise}
\end{cases}
\]</span></p>
<ul>
<li>对称归一化拉普拉斯矩阵（Symmetric normalized Laplacian）：将上面的拉普拉斯矩阵归一化： <span class="math display">\[
\begin{align}
L^{sym}&amp;=D^{-\frac{1}{2}}LD^{-\frac{1}{2}} \\
&amp;=I-D^{-\frac{1}{2}}AD^{-\frac{1}{2}}
\end{align}
\]</span> 最终得到了归一化拉普拉斯矩阵，矩阵中的元素定义为： <span class="math display">\[
L_{ij}^{sym} =
\begin{cases}
1,  &amp; \mbox{if } i = j \mbox{ and } d(v_i)\not=0 \\
-\frac{1}{\sqrt{d(v_i)d(v_j)}}, &amp; \mbox{if }\{ v_i, v_j \}\in E \mbox{ and } i\not=j \\
0, &amp; \mbox{otherwise}
\end{cases}
\]</span></li>
</ul>
<p>接下来我们讨论的图是无向图，那么拉普拉斯矩阵是对称矩阵，归一化之后的图拉普拉斯矩阵是半正定对称矩阵。</p>
<p>基于归一化拉普拉斯矩阵，我们可以将它对角化，<span class="math inline">\(L^{sym}=U\Lambda U^T\)</span>，其中<span class="math inline">\(U=[\mathbf{u}_0,\mathbf{u}_1, \dots,\mathbf{u}_n]\in \mathcal{R}^{n\times n}\)</span>是特征向量矩阵，<span class="math inline">\(\Lambda_{ii}=\lambda_i\)</span>是特征值。这一对角化实对称矩阵的操作在线性代数中叫做谱分解，<span class="math inline">\(\Lambda\)</span>称作谱（spectrum）。实对称矩阵的特征向量一定是正交的（orthogonal），特征值一定是实数。特征向量构成了谱空间的基。在图的信号处理中，一个图信号（graph signal）定义为<span class="math inline">\(x\in \mathcal{R}^n\)</span>，每个节点都有一个对应的数值，这一图信号也叫做节点的特征，可以拥有多个特征，那么把多个图信号按列排序就得到了一个图的特征矩阵： <span class="math display">\[
X\in \mathcal{R}^{n\times D}
\]</span> <span class="math inline">\(X_i\)</span>表示第<span class="math inline">\(i\)</span>个节点的特征，<span class="math inline">\(X_{ij}\)</span>表示第<span class="math inline">\(i\)</span>个节点的第<span class="math inline">\(j\)</span>个特征。</p>
<p>在介绍GCN之前，需要介绍首个定义在图上进行的卷积操作原理。</p>
<p>在图信号处理中，模仿一般的傅里叶变换定义了图傅里叶变换（graph Fourier transform），就是将处于原来特征空间下的图信号投影转换到谱空间中， <span class="math display">\[
\mathcal{F}(x)=U^Tx
\]</span> 傅里叶逆变换定义为： <span class="math display">\[
\mathcal{F}^{-1}(x)=U\hat{x}
\]</span> 由于图不满足平移不变性，我们希望直接在特征空间下定义图的卷积算子<span class="math inline">\(f*g\)</span>比较困难，<span class="math inline">\(x\)</span>是输入的图信号（signal），<span class="math inline">\(g\)</span>是过滤器（filter）。但利用卷积定理，信号卷积的傅里叶变换等价于信号傅里叶变换的乘积。 <span class="math display">\[
\mathcal{F}({x*g})=\mathcal{F}(x)\cdot \mathcal{F}(g)
\]</span> 那么图卷积算子可以定义为： <span class="math display">\[
{x*g}=\mathcal{F}^{-1}(\mathcal{F}(x)\cdot \mathcal{F}(g))
\]</span> 这样我们无需考虑原特征空间中的卷积，只需要先将信号转换为谱空间中的信号，在谱空间中与谱空间中的过滤器做乘法，最后利用傅里叶逆变换转换到原来的空间中，就实现了图卷积。</p>
<p>更具体的图卷积算子定义为： <span class="math display">\[
\begin{align}
{x*g}&amp;=\mathcal{F}^{-1}(\mathcal{F}(x)\cdot \mathcal{F}(g)) \\
&amp;=U(U^Tx\cdot U^Tg) \\
\end{align}
\]</span> 其中，<span class="math inline">\(U^Tg\)</span>是一个向量，可以看做是对角矩阵<span class="math inline">\(g_{\theta}=diag(U^Tg)\)</span>，最后谱卷积算子定义为： <span class="math display">\[
{x*g}=Ug_{\theta}U^Tx
\]</span> 所有基于谱空间的图卷积方法都是在尝试寻找更合适的<span class="math inline">\(g_{\theta}\)</span>。</p>
<h3 id="fast-approximate-convolutions-on-graphs">FAST APPROXIMATE CONVOLUTIONS ON GRAPHS</h3>
<p>Spectral CNN（Spectral Convolutional Neural Network）直接将<span class="math inline">\(g_{\theta}\)</span>作为一系列可以学习的参数，得到了下面的卷积公式： <span class="math display">\[
H^{(l+1)}_{:,j}=\sigma(\sum_{i=1}^{f_{l}} U\Theta^{l}_{i,j}H^{(l)}_{:,i}) \ (j=1,2,3\dots,f_l)
\]</span> 这里任然需要计算特征向量，计算代价很大。</p>
<p>之后切比雪夫网络（Chebyshev Spectral CNN，ChebNet）对Spectral CNN进行简化，使用切比雪夫多项式简化<span class="math inline">\(g_{\theta}\)</span>，令<span class="math inline">\(g_{\theta}=\sum_{i=0}^K\theta_i T_i(\tilde{\Lambda})\)</span>，其中<span class="math inline">\(\tilde{\Lambda}=2\Lambda/\lambda_{max}-I_n\)</span>得到了下面的卷积公式： <span class="math display">\[
\begin{align}
{x*g} &amp;\approx U\sum_{i=0}^K \theta_i T_i(\tilde{\Lambda}) U^T x \\
&amp;= \sum_{i=0}^K \theta_i T_i(\tilde{L}) x
\end{align}
\]</span> 其中，<span class="math inline">\(\tilde{L}=2L/\lambda_{max}-I_n\)</span>。这样ChebNet不需要再计算特性向量<span class="math inline">\(U^T\)</span>。</p>
<h2 id="gcn">3 GCN</h2>
<p>GCN在ChebNet的基础上，进一步将K阶多项式限制到了1阶，假设<span class="math inline">\(\lambda_{max}=2\)</span>得到了新的卷积算子： <span class="math display">\[
{x*g} \approx \sum_{i=0}^K \theta_i T_i(\tilde{L}) \approx \theta_0x + \theta_1x(L-I_n)x=\theta_0x + \theta_1 D^{-\frac{1}{2}}AD^{-\frac{1}{2}} x
\]</span> 由于半监督图节点分类学习的数据量少，令0阶系数和1阶系数相同，减小学习的参数量。 <span class="math display">\[
\begin{align}
{x*g} &amp;\approx \theta_0x + \theta_1 D^{-\frac{1}{2}}AD^{-\frac{1}{2}} x \\
&amp;\approx \theta (I_n + D^{-\frac{1}{2}}AD^{-\frac{1}{2}})x
\end{align}
\]</span> 最终提出了具有多层结构的图卷积神经网络（GCN），每一层使用的卷积函数定义如下： <span class="math display">\[
H^{(l+1)}=\sigma (\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}H^{(l)}W^{(l)})
\]</span> 其中<span class="math inline">\(\tilde{D}\)</span>和<span class="math inline">\(\tilde{A}\)</span>是区别于之前的度矩阵和邻接矩阵， <span class="math display">\[
\tilde{A}=A+I_N\\
\tilde{D}_{ii}=\sum_j{\tilde{A}_{ij}}
\]</span> 原因是在实验中发现，如果使用原来的形式会导致训练的不稳定性以及梯度消失/爆炸的问题，因此重新归一化。</p>
<p>GCN用来做半监督的节点分类任务，在实现的时候使用了两层GCN，最后经过Softmax输出预测值。损失函数使用交叉熵。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20201129220200051.png" /></p>
<h2 id="experiments">4 EXPERIMENTS</h2>
<p>使用以下四方面的实验：</p>
<ul>
<li>semi-supervised document classiﬁcation in citation networks</li>
<li>semi-supervised entity classiﬁcation in a bipartite graph extracted from a knowledge graph</li>
<li>an evaluation of various graph propagation models</li>
<li>a run-time analysis on random graphs</li>
</ul>
<p>使用的数据集：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20201129220829777.png" /></p>
<p>需要注意的一点是对于知识图谱这种有向图，GCN将关系拆出来作为一个联通两个节点的新节点，这样知识图谱就转换为了无向图。</p>
<h3 id="semi-supervised-node-classification">4.1 SEMI-SUPERVISED NODE CLASSIFICATION</h3>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20201129222635232.png" /></p>
<h3 id="evaluation-of-propagation-model">4.2 EVALUATION OF PROPAGATION MODEL</h3>
<p>对于GCN机制的一个探究。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20201129223242035.png" /></p>
<h3 id="training-timeper-epoch">4.3 TRAINING TIMEPER EPOCH</h3>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20201129223143658.png" /></p>
<h2 id="discussion">5 DISCUSSION</h2>
<p>切比雪夫网络是利用切比雪夫多项式对Spectra CNN进行的简化估计，无需再计算特征向量<span class="math inline">\(U\)</span>；GCN是对切比雪夫网络进一步的简化，将K阶多项式限制到了1阶，同时令0阶系数和1阶系数相同，得到了更简洁的图卷积形式。</p>
<p>优点：</p>
<ul>
<li>直接将图卷积算子简化到了一步线性计算的程度</li>
<li>形式上来看已经和一般的神经网络非常相似了，方便与已有的深度学习方法进行结合</li>
</ul>
<p>缺点：</p>
<ul>
<li>内存的限制，需要训练全图</li>
<li>无法学习有向图以及边的特征</li>
</ul>
]]></content>
      <categories>
        <category>Paper</category>
        <category>GNN</category>
      </categories>
  </entry>
  <entry>
    <title>GNN-1000-layers</title>
    <url>/gnn/GNN-1000-layers/</url>
    <content><![CDATA[<h1 id="training-graph-neural-networks-with-1000-layers">Training Graph Neural Networks with 1000 Layers</h1>
<p>ICML 2021</p>
<p>这篇文章通过在GNN中引入grouped reversible connections，实现了将GNN拓展到1000层，可能是当前最深的GNN之一。这篇文章的意义在于，实现了GNN的层数与模型所需的显存无关，使用较少的显存就可以在显存基本不增加的情况下，任意增加GNN深度。</p>
<p>下图是作者提出的Rev-GNN在ogbn-proteins数据集上的结果，很少的memory，达到了很好的效果。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220323191910030.png" alt="image-20220323191910030" style="zoom:50%;" /></p>
<span id="more"></span>
<p>看一下作者的核心思路，在一层的GNN中的grouped reversible connections。首先，将输入划分的节点特征矩阵<span class="math inline">\(X\in \mathbb{R}^{N\times D}\)</span>随机划分为不同的group：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220323192420194.png" alt="image-20220323192420194" style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220323192439309.png" alt="image-20220323192439309" style="zoom:50%;" /></p>
<p>然后作者基于划分好的group产生新的输出：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220323192539787.png" alt="image-20220323192539787" style="zoom:50%;" /></p>
<p>上面的式子里，<span class="math inline">\(f_{w_i}\)</span>表示是图卷积操作。注意这里的输出<span class="math inline">\(X_i^\prime\)</span>，除了<span class="math inline">\(X_1^\prime\)</span>依赖于<span class="math inline">\(X_0^\prime\)</span>外，其它的<span class="math inline">\(X_i^\prime\)</span>都是由前一个的<span class="math inline">\(X_{i-1}^\prime\)</span>推出的。</p>
<p>这样做的好处是，可以通过输出直接推导出输入。比如用在backwards过程中：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220323192914104.png" alt="image-20220323192914104" style="zoom:50%;" /></p>
<p>因此，在作者的RevGNN中，只需要保留最后一个GNN层的输出即可，前面所有层的中间状态都可以反向求出。从这里也可以看出来，<span class="math inline">\(X_0^\prime\)</span>之所以不从group 1开始累加，就是为了后续能够反向重构输入。否则的话，在后续重构<span class="math inline">\(X_1^\prime\)</span>，需要提前知道<span class="math inline">\(X_0^\prime\)</span>，但是想知道<span class="math inline">\(X_0^\prime\)</span>又需要知道<span class="math inline">\(X_1^\prime\)</span>，成为了死锁。</p>
<p>另外，作者提到了需要normalization layers和dropout layers：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220323195400866.png" alt="image-20220323195400866" style="zoom:50%;" /></p>
<p>dropout操作会影响重构输入，如果每层dropout都不一样，backward的时候，每层都需要提前保存dropout，因此作者直接让所有层都是保持一样的dropout设置。</p>
<p>以上就是作者的核心思想，同时作者还做了两个拓展：</p>
<ol type="1">
<li>Weight-tied GNNs.</li>
</ol>
<p>意思是让每一层的参数量都固定一样，这样来减小模型参数量。实验结果发现性能并不如每层都有自己的参数的模型，训练速度也差不多一样，优点就是参数量减小。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220323195848002.png" alt="image-20220323195848002" style="zoom:50%;" /></p>
<ol start="2" type="1">
<li>Deep Equilibrium GNNs</li>
</ol>
<p>使用implicit differentiation的方法来训练Weight-tied GNN，好处是训练比较快。这一部分没有特别理解，但是大概意思是通过不断的迭代，最终模型能够达到某个平衡点。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220323200027329.png" alt="image-20220323200027329" style="zoom:50%;" /></p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>GNN</category>
      </categories>
      <tags>
        <tag>GNN</tag>
      </tags>
  </entry>
  <entry>
    <title>GNN-size-generalization</title>
    <url>/gnn/GNN-size-generalization/</url>
    <content><![CDATA[<h1 id="from-local-structures-to-size-generalization-in-graph-neural-networks">From Local Structures to Size Generalization in Graph Neural Networks</h1>
<p>ICML 2021</p>
<p>作者主要讨论了GNN对于graph size generalization问题的性质探究。具体一点是指GNN在一个small graph上训练，然后在一个更大的large graph上测试的场景。</p>
<p>主要贡献：</p>
<ul>
<li>提出了graph的local structure的一种定义，d-pattern。GNN在对于相同的d-pattern会产生相同的输出。因此使用d-pattern可以作为GNN表达能力的一种抽象。</li>
<li>理论上和实验上证明了GNN在size不同的graph上，不能保证学习到的模型是有足够size generalization能力的。</li>
<li>提出了一种基于自监督的方法（Self-Supervised Learning，SSL）来提升size generalization能力，分别有无监督（unsupervised）和半监督（semi-supervised）两种loss设置。训练过程采用了预训练和多任务学习两种不同的学习过程。</li>
</ul>
<span id="more"></span>
<h2 id="introduction">Introduction</h2>
<p>作者主要的研究场景：small graph训练+large graph测试</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220328201522509.png" alt="image-20220328201522509" style="zoom:40%;" /></p>
<p>为什么要研究这个问题？作者的出发点：</p>
<ol type="1">
<li>理论上，graph的size的差异是很大的，虽然训练好一个graph可以让它在任意size的graph（保证是统一domain下的graph）上运行，但是效果好吗？什么情况下能够做到size泛化？这个问题很有趣（intriguing）但是还没被充分研究。</li>
<li>实际上，很多large graph想要获得准确的label是很困难的，想要获得large graph的label可能是非常困难的优化问题，也可能对于人来说想要准确的给复杂的graph打label也是很难的。因此，如果GNN能够做到在small graph上训练好，然后很好的泛化到large graph上，就是一个很有意义的研究。</li>
</ol>
<h2 id="overview">Overview</h2>
<p>几个作者希望声明的argument：</p>
<ol type="1">
<li><p>作者提出的d-pattern是研究GNN表达能力的一种合适的抽象表达（<strong>d-patterns are a correct notion for studying the expressivity of GNNs.</strong>）。依赖于d-pattern，GNN可以输出独立的任意值，对于具有一样d-pattern的node来说，GNN会输出一样的值。因此对于现有的GNN来说，d-pattern直接限制了它的表达能力。</p></li>
<li><p>small graph和large graph之间的d-pattern差异，暗示了可能存在某种糟糕的优化选择，导致GNN无法做到size generalization（<strong>d-pattern discrepancy implies the existence of bad global minima.</strong>）。</p></li>
<li>GNN在一般的情况下，会倾向于收敛到不泛化的解（<strong>GNNs converge to non-generalizing solutions.</strong>）。作者进行了实验上的证明，并且也发现，如果尝试不断改变small graph的分布，GNN泛化能力会有相应的提升</li>
<li>GNN的size generalization可以被提升（<strong>Size generalization can be improved.</strong>）。作者提出了新的SSL的训练方法，可以提升GNN的size generalization能力。</li>
<li><p>GNN的size泛化，不是简单的L1或L2防止过拟合问题，实际上，如果单纯的加入正则项，反而会让GNN的size generalization能力降低。</p></li>
</ol>
<h2 id="gnns-and-local-graph-patterns">GNNs and local graph patterns</h2>
<p>来看一下作者定义的d-pattern：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220328203208523.png" alt="image-20220328203208523" style="zoom:50%;" /></p>
<p>这里的定义与WL-test类似。看看示例图：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220328203249132.png" alt="image-20220328203249132" style="zoom:40%;" /></p>
<p>作者提出的两个定理：</p>
<ol type="1">
<li>对于具有相同d-pattern的node，任意GNN都会输出相同的值。</li>
</ol>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220328203453181.png" alt="image-20220328203453181" style="zoom:50%;" /></p>
<p>这个定理是说GNN能够表达的映射函数，如果节点的d-pattern一样，那么GNN输出也一样；如果d-pattern不一样，那么GNN输出可能一样，可能不一样。联想到GIN中提出的单射问题，和这个理论是能够联系的。GIN提出的单射问题是希望不同d-pattern能够对于不同的输出。</p>
<ol start="2" type="1">
<li>对于具有不同d-pattern的node，假设各自不同的d-pattern有不同的label，那么总存在一个GNN能够完美拟合。</li>
</ol>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220328203635494.png" alt="image-20220328203635494" style="zoom:40%;" /></p>
<h2 id="bad-global-minima-exist">”Bad” global minima exist</h2>
<p>在这一部分，作者提出GNN可能学习到泛化能力弱的解。</p>
<p>同样，作者提出了两个定理：</p>
<ol type="1">
<li>存在一个GNN在small graph上效果很好，但是对于large graph（包含没有在small graph上出现过的d-pattern），可能有任意范围的error。</li>
</ol>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220328204225483.png" alt="image-20220328204225483" style="zoom:40%;" /></p>
<ol start="2" type="1">
<li>一个和上面定理相似的定理，但是描述了small graph和large graph的d-pattern分布差异和可能导致的error。存在一个GNN在small graph上，表现好（指对于d-pattern集合A，<span class="math inline">\(error&lt;\epsilon\)</span>），但是在large graph上的error更大（指让large graph error最大的集合A的误差）。</li>
</ol>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220328204906574.png" alt="image-20220328204906574" style="zoom:40%;" /></p>
<h2 id="towards-improving-size-generalization">Towards improving size generalization</h2>
<p>来看一下作者如果尝试解决size generalization问题。首先，作者同时从small graph和large graph上构造了pattern-tree，然后用这个pattern-tree进行学习任务。</p>
<p>pattern-tree：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220328205133102.png" alt="image-20220328205133102" style="zoom:40%;" /></p>
<p>左上角是原来的graph，右上角是node对应的pattern-tree，底部是要预测的值（向量），也就是计算每一层的节点数量。</p>
<p>为什么要使用这个pattern-tree呢？作者在前面发现，GNN的size generalization做不好，就是因为在large graph上会有unseen d-pattern，那么如果提前想办法学习好small和large graph的d-pattern的信息，让两者的d-pattern表示有某种程度的对齐（通过一起训练的方法），是不是就能够提升模型效果？</p>
<p>如果构造pattern-tree：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220328205701565.png" alt="image-20220328205701565" style="zoom:50%;" /></p>
<p>简单说，就是迭代的往叶子结点上添加它的邻居节点。</p>
<p>怎么使用？</p>
<p>两种训练策略：</p>
<ul>
<li>Pretraining：第一阶段，训练GNN，预测pattern-tree的descriptor；第二阶段，固定GNN值，预测目标task。</li>
</ul>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220328210257357.png" alt="image-20220328210257357" style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220328210309307.png" alt="image-20220328210309307" style="zoom:50%;" /></p>
<ul>
<li>Multitask training：同时训练pattern-tree task和main task。</li>
</ul>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220328210353315.png" alt="image-20220328210353315" style="zoom:50%;" /></p>
<p>两种设置的示意图：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220328210152599.png" alt="image-20220328210152599" style="zoom:50%;" /></p>
<p>实验发现，还是pretraining的方式更好一点。</p>
<p>作者还尝试了另外的semi-supervised setup，即加入一小部分large graph中有label的data，加入到前面的训练loss中。</p>
<h2 id="appendix">Appendix</h2>
<p>作者在附录提供了定理详细的证明过程，我只是粗略的看了一遍从直观上认识定理证明是否正确，没有严谨的推导。但是，有一个有意思的前人（Small relu networks are powerful memorizers: a tight analysis of memorization capacity.）提出的定理可以学习：</p>
<ul>
<li>对于各不相同的输入<span class="math inline">\(\mathbf{x}_i\)</span>，输出是<span class="math inline">\(y_i\in[-1,1]\)</span>，总存在一个三层的<span class="math inline">\(ReLU\)</span>网络可以完美拟合</li>
</ul>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220328211224061.png" alt="image-20220328211224061" style="zoom:50%;" /></p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>GNN</category>
      </categories>
      <tags>
        <tag>GNN</tag>
      </tags>
  </entry>
  <entry>
    <title>GraphSAGE</title>
    <url>/gnn/GraphSAGE/</url>
    <content><![CDATA[<h1 id="inductive-representation-learning-on-large-graphs">Inductive Representation Learning on Large Graphs</h1>
<p>NIPS 2017 GraphSAGE</p>
<p>主要贡献：</p>
<ul>
<li>提出了一种inductive学习图结构的方法，使用节点的特征（文本、度等）作为输入，进行无监督学习</li>
<li>构造了三个节点分类（node classification）的数据集</li>
</ul>
<span id="more"></span>
<h2 id="introduction">1 Introduction</h2>
<p>从transductive到inductive 在GCN中训练需要训练整个图，所有节点的embedding，之后基于这些训练过的embedding进行测试，这叫做transductive； GraphSAGE计划设计一个inductive的方法，不训练所有的节点。在测试集中的节点在训练时是不可见的，使用节点的特征作为原始输入，然后使用训练好的网络在测试集中评估。</p>
<p>提出了GraphSAGE（SAmple and aggreGAte）</p>
<h2 id="method">2 Method</h2>
<figure>
<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20201220093929039.png" alt="image-20201220104337756" /><figcaption>image-20201220104337756</figcaption>
</figure>
<figure>
<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20201220104337756.png" alt="image-20201220093929039" /><figcaption>image-20201220093929039</figcaption>
</figure>
<p>核心在于如何聚合邻居节点的信息：</p>
<ol type="1">
<li>首先使用节点特征（文本等）作为初始输入</li>
<li>之后均匀随机采样固定数量的直接相邻邻居节点，定义特定的聚合函数AGGREGATE，聚合邻居信息（不包括自身的特征）</li>
<li>拼接自身的特征与邻居的信息，过一层神经网络，得到获得一阶邻居后的输出</li>
<li>上面的过程重复K次，每个节点在k次聚和时，采样<span class="math inline">\(S_k\)</span>个邻居的k-1阶表示，得到k阶表示</li>
</ol>
<p>损失函数： <span class="math display">\[
J(\mathbf{z}_u) = -log(\sigma(\mathbf{z}_u^T \mathbf{z}_v))-Q\mathbb{E}_{v_n\sim P_n(v)}log(\sigma(-\mathbf{z}_u^T \mathbf{z}_{v_n}))
\]</span> <span class="math inline">\(v\)</span>是邻居节点；<span class="math inline">\(v_n\)</span>是负样本；<span class="math inline">\(P_n(v)\)</span>是负采样分布；<span class="math inline">\(Q\)</span>是负采样的数量。这个loss的含义是让邻居节点相似，增大不相关的节点差异。</p>
<p>下面是GraphSAGE的核心方法，聚合函数。</p>
<p>在论文里提出了四个聚合函数：</p>
<ol type="1">
<li>-GCN：<span class="math inline">\(\mathbf{h}_v^k = \sigma(W\cdot \mbox{Mean}(\{\mathbf{h}_v^{k-1}\} \cup \{\mathbf{h}_u^{k-1},\ u\in \forall N(v) \} ))\)</span></li>
<li>-Mean：先平均邻居表示，之后与中心节点表示拼接后过一层神经网络，<span class="math inline">\(\mbox{AGGREGATE}^{\mbox{mean}}_k=\mbox{Mean}(\{\mathbf{h}_u^{k-1},\ u\in \forall N(v) \})\)</span></li>
<li>-LSTM：使用LSTM聚合邻居信息，每次聚合时先随机打乱邻居节点的顺序</li>
<li>-Pooling：<span class="math inline">\(\mbox{AGGREGATE}^{\mbox{pool}}_k=\mbox{max}(\{\sigma(W_{pool}\mathbf{h}_u^{k-1}+\mathbf{b}),\ u\in \forall N(v) \})\)</span></li>
</ol>
<h2 id="experiments">3 Experiments</h2>
<p>三个实验：</p>
<ul>
<li>Web of Science引文网络分类任务，判断paper属于哪个subject。</li>
<li>Reddit中发送的post的分类任务，判断用户发送的post属于哪个community</li>
<li>protein-protein interaction (PPI)分类任务，判断蛋白质的功能</li>
</ul>
<p>对应的构造了三个数据集</p>
<ul>
<li>Wos Data：从Web of Science Core Collection中收集，2000-2005年6个生物领域的论文，标签就是这6个生物领域。使用2000-2004年数据作为训练，使用30%的2005年数据作为验证集，70%作为测试集。最终数据集包括了302,424节点，平均degree 9.15。在实验时，使用论文abstract的sentence embedding以及节点的degree作为初始特征输入。</li>
<li>Reddit Data：从Reddit 2014年9月中构造数据，选择了50个大的Reddit communities，如果用户在两个post下进行了评论，就把这两个post连接起来。20天训练，剩下的3天验证，7天测试。最终包含232,965节点，平均度492。实验时，post的title embedding，comment embedding，post的打分，comment的数量作为初始特征输入。</li>
<li>PPT Data：从Molecular Signatures Database中构造数据集，20个graph训练，2个graph验证，2个graph测试。每个graph来自不同的人体组织，平均graph有2373个节点，平均degree 28.8，一共121个标签。</li>
</ul>
<p>在验证集上寻找各个模型最好的参数，然后在测试集上评估。</p>
<p>实验时，还对比了监督学习（直接与标签进行cross-entropy）和无监督学习</p>
<figure>
<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20201221090604439.png" alt="image-20201220103317625" /><figcaption>image-20201220103317625</figcaption>
</figure>
<p>最终，作者发现K=2相对是比较好的选择，同时，采样s邻居数量<span class="math inline">\(S_1\cdot S_2 &lt; 500\)</span>较好，实验时使用的GraphSAGE都是K=2，<span class="math inline">\(S_1=25\)</span>，<span class="math inline">\(S_2=10\)</span>。</p>
<h2 id="minibatch-pseudocode">4 Minibatch pseudocode</h2>
<figure>
<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20201220103317625.png" alt="mage-20201221090604439" /><figcaption>mage-20201221090604439</figcaption>
</figure>
<p>首先采样在对batch <span class="math inline">\(B\)</span>进行K阶训练，需要用到的所有节点。<span class="math inline">\(B^k\)</span>包括了所有在训练<span class="math inline">\(k+1\)</span>时需要用到的节点的<span class="math inline">\(k\)</span>阶表示。</p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>GNN</category>
      </categories>
  </entry>
  <entry>
    <title>HAN</title>
    <url>/gnn/HAN/</url>
    <content><![CDATA[<h1 id="heterogeneous-graph-attention-network">Heterogeneous Graph Attention Network</h1>
<p>2019-4-13</p>
<h2 id="introduction">1 INTRODUCTION</h2>
<p>HAN设计了两个层次的attention机制，</p>
<ul>
<li>Semantic-level attention：在不同的meta-path中选择weight</li>
<li>Node-level attention：对于一个节点，它的邻居的weight</li>
</ul>
<span id="more"></span>
<h2 id="related-work">2 RELATED WORK</h2>
<blockquote>
<p>The graph convolutional neural work generally falls into two categories, namely spectral domain and non-spectral domain.</p>
</blockquote>
<h2 id="the-proposed-model">4 THE PROPOSED MODEL</h2>
<p>总的来说，HAN包括了两个层次的attention，node-level和semantic-level，node-level attention的输出作为semantic-level层次的输入。</p>
<h3 id="node-level-attention">4.1 Node-level Attention</h3>
<p>对于<span class="math inline">\((i,j,\Phi)\)</span>，<span class="math inline">\(i,j\)</span>表示节点，<span class="math inline">\(\Phi\)</span>表示meta-path。</p>
<p>node-level attention针对的目标是同一个meta-path下的nodes的weight。</p>
<p>首先根据node type确定投影embedding， <span class="math display">\[
h^{&#39;}_i=M_{\phi_i}h_i \\
\phi_i: node \ i \ type
\]</span> 计算attention value， <span class="math display">\[
\alpha_{ij}=\frac{exp(\sigma(a^T[h_i^{&#39;}||h_j^{&#39;}]))}{\sum_{k\in N_i^{\Phi}}  exp(\sigma(a^T[h_i^{&#39;}||h_k^{&#39;}]))}
\]</span> 把所有node的embedding结合起来， <span class="math display">\[
z_i^\Phi = \sigma(\sum_{j\in N_i^{\Phi}} \alpha_{ij}^\Phi h_{ij}^{&#39;} )
\]</span> 类似于GAT，采用multi-head attention， <span class="math display">\[
z_i^\Phi = ||_{k=1}^K \sigma(\sum_{j\in N_i^{\Phi}} \alpha_{ij}^\Phi h_{ij}^{&#39;} )
\]</span> 这是一种meta-path下一个节点<span class="math inline">\(i\)</span>的最终输出，对于所有的<span class="math inline">\(\Phi\)</span>与全部的node，产生<span class="math inline">\(\{z_i^{\Phi_0}, z_i^{\Phi_1},\dots z_i^{\Phi_p}\}\)</span>。</p>
<h2 id="semantic-level-attention">4.2 Semantic-level Attention</h2>
<p>要计算各种类型的meta-path的weight，就要在全局的情况下计算， <span class="math display">\[
\omega_{\Phi_p}=\frac{1}{|V|}\sum_{i\in V}q^T tanh(Wz_i^{\Phi_p}+b) \\
\beta_{\Phi_p}=softmax(\omega_{\Phi_p})
\]</span> 最后求和，得到最终的embedding， <span class="math display">\[
Z_i=\sum_{p=1}^P \beta_{\Phi_p}z_i^{\Phi_p}
\]</span> <img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20200221215000895.png" alt="image-20200221215000895" style="zoom: 33%;" /></p>
<h2 id="experiments">5 EXPERIMENTS</h2>
<p>略</p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>GNN</category>
      </categories>
  </entry>
  <entry>
    <title>HGSL</title>
    <url>/gnn/HGSL/</url>
    <content><![CDATA[<h1 id="heterogeneous-graph-structure-learning-for-graph-neural-networks">Heterogeneous Graph Structure Learning for Graph Neural Networks</h1>
<p>AAAI 2021</p>
<p>作者声称是首个尝试为异质图神经网络寻找最优的图结构进行学习的方法，提出了HGSL（Heterogeneous Graph Structure Learning）。核心方法有两个，异质图结构学习和图神经网络。 <img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210706213222126.png" style="zoom:40%;" /> <span id="more"></span></p>
<h2 id="introduction">Introduction</h2>
<p><strong>motivation</strong>：目前的异质图神经网络基于一个假设，学习使用的graph是足够好的。但是实际上这个假设不一定总能够满足。两个方面的原因，（1）在建模graph的时候，使用到的信息难免会包含错误的信息，导致最终的graph是不够好的（2）另一个原因是异质图结构本身与下游任务是独立的，不一定是有利于下游任务的最优解。为了解决上面的问题，图结构学习graph structure learning (GSL)被提出来，但是这些方法主要是在考虑同质图，无法很好的考虑异质图中的异质性以及异质图中存在的复杂的交互。</p>
<p><strong>method</strong>：提出HGSL，首先学习合适的graph structure，然后在这个graph structure上使用GCN进行学习。这种heterogeneous graph structure learning是核心创新点，包括三种graph的融合，<strong>feature similarity graph</strong>，<strong>feature propagation graph</strong>,和<strong>semantic graph</strong>。</p>
<blockquote>
<p>Heterogeneous Graph Neural Networks (HGNNs) have drawn increasing attention in recent years and achieved outstanding performance in many tasks. The success of the existing HGNNs relies on one fundamental assumption, i.e., the original heterogeneous graph structure is reliable. However, this assumption is usually unrealistic, since the heterogeneous graph in reality is inevitably noisy or incomplete. Therefore, it is vital to learn the heterogeneous graph structure for HGNNs rather than rely only on the raw graph structure. In light of this, we make the ﬁrst attempt towards learning an optimal heterogeneous graph structure for HGNNs and propose a novel framework HGSL, which jointly performs Heterogeneous Graph Structure Learning and GNN parameter learning for classiﬁcation. Different from traditional homogeneous graph structure learning, considering the heterogeneity of different relations in heterogeneous graph, HGSL generates each relation subgraph separately. Speciﬁcally, in each generated relation subgraph, HGSL not only considers the feature similarity by generating feature similarity graph, but also considers the complex heterogeneous interactions in features and semantics by generating feature propagation graph and semantic graph. Then, these graphs are fused to a learned heterogeneous graph and optimized together with a GNN towards classiﬁcation objective. Extensive experiments on real-world graphs demonstrate that the proposed framework signiﬁcantly outperforms the state-of-the-art methods.</p>
</blockquote>
<h2 id="method">Method</h2>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210706213222126.png" style="zoom:40%;" /></p>
<h3 id="feature-graph-generator">Feature Graph Generator</h3>
<p>基于node feature，通过计算相似度，学习node之间潜在的relationship。</p>
<p>对于边类型<span class="math inline">\(r\)</span>，首先，对于<span class="math inline">\(r\)</span>下的所有edge的头/尾node <span class="math inline">\(i\)</span>，根据node的类型对node feature进行转换</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210706213656455.png" style="zoom:50%;" /></p>
<p>之后，计算利用余弦相似性计算两个节点的相似性</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210706213845698.png" style="zoom:50%;" /></p>
<p>设计一个阈值，然后创建graph</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210706213919202.png" style="zoom:50%;" /></p>
<p>这个graph，是边类型<span class="math inline">\(r\)</span>下的不同类型的头、尾实体的feature similarity graph <span class="math inline">\(\mathbf{S}^{FS}_r\)</span>。</p>
<h3 id="feature-propagation-graph">Feature Propagation Graph</h3>
<p>不同的关系<span class="math inline">\(r\)</span>，不同的node feature之间是存在complex的交互interaction的。为了建模这种复杂的交互，HGSL让node features和topology structure产生交互，然后构造一个feature propagation graph。核心思想是具有相似特征的节点可能具有相似的邻居。</p>
<p>关系r的邻接矩阵是<span class="math inline">\(\mathbf{A}_r\)</span>，头node和尾node可能具有不同的type。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210706214453495.png" style="zoom:50%;" /></p>
<p>对于相同类型type的头结点<span class="math inline">\(i\)</span>和<span class="math inline">\(j\)</span>，构造一个头结点的相似特征图</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210706215121910.png" style="zoom:50%;" /></p>
<p>之后，这些相似头结点可以通过拓扑结构传播，最终实现效果是相似头结点可以往相似头结点传播消息，获得了head feature propagation graph。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210706215417582.png" style="zoom:50%;" /></p>
<p>类似的，构造相似尾结点图，然后传播，获得了tail feature propagation graph。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210706215456493.png" style="zoom:50%;" /></p>
<p>之后，通过获得单纯的特征图、头实体的特征-拓扑结构交互图、尾实体的特征-拓扑结构交互图，进行融合，使用一个channel attention layer，学习一个<span class="math inline">\(1\times 1\times 3\)</span>的卷积核<span class="math inline">\(W^{Feat}_{\Psi,r}\)</span>。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210706215705310.png" style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210706215720881.png" style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210706215931644.png" style="zoom:50%;" /></p>
<h3 id="semantic-graph-generator">Semantic Graph Generator</h3>
<p>接下来，是学习多阶拓扑结构信息的合适图。在异质图中，不同阶的邻居信息当然是差异非常大的。</p>
<blockquote>
<p>The semantic graph is generated depending on the high-order topology structure in HIN, describing the multi-hop structural interactions between two nodes.</p>
</blockquote>
<p>HGSL使用MP2Vec去进行学习，定义了<span class="math inline">\(M\)</span>个元路径</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210706220259483.png" style="zoom:50%;" /></p>
<p>对于所有的node，学习到<span class="math inline">\(M\)</span>个embedding集合，</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210706220338461.png" style="zoom:50%;" /></p>
<p>对于表示不同semantic的metapath信息，同样是构造一个相似图</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210706220514947.png" style="zoom:50%;" /></p>
<p>使用channel attention layer融合semantic subgraph，</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210706220632763.png" style="zoom:50%;" /></p>
<h3 id="overall-generated-graph">Overall generated graph</h3>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210706220738042.png" style="zoom:50%;" /></p>
<h3 id="optimization">Optimization</h3>
<p>前面为每个关系都学习了一个融合的graph <span class="math inline">\(\mathbf{A}_r\)</span>，接下来作者直接使用GCN进行学习，通过直接认为只要有两个相连的node就可以认为是1，构造了<span class="math inline">\(\mathbf{A}^\prime\)</span>，推测就是直接所有的<span class="math inline">\(\mathbf{A}_r\)</span>相加。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210706221223318.png" style="zoom:50%;" /></p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>GNN</category>
      </categories>
      <tags>
        <tag>GNN</tag>
      </tags>
  </entry>
  <entry>
    <title>HGT</title>
    <url>/gnn/HGT/</url>
    <content><![CDATA[<h1 id="heterogeneous-graph-transformer">Heterogeneous Graph Transformer</h1>
<p>本文的贡献主要有三点：</p>
<ol type="1">
<li>针对异质图设计了Heterogeneous Graph Transformer，HGT，用于处理Web-scale的异质网络。</li>
<li>为了能够训练大规模图，提出了HGSampling采样方法，在Open Academic Graph (OAG)上进行了验证。</li>
<li>针对动态异质图，引入relative temporal encoding，能够处理任意的动态信息</li>
</ol>
<span id="more"></span>
<p>WWW 2020</p>
<h2 id="introduction">Introduction</h2>
<p><strong>Motivation</strong> ：对于heterogeneous graph已有的处理方法有基于meta-path的方法，以及最近出现的GNN方法。但是这些方法都面临问题。</p>
<ol type="1">
<li>大多数的方法为不同的heterogeneous graph设计meta path，要求specific domain knowledge</li>
<li>很多方法要么忽略不同type的差异，直接使用共通的weight，要么直接认为不同type之间是完全独立的，为不同的方法定义各自独立的weight</li>
<li>它们大多数忽略了heterogeneous graph中存在的动态特性dynamic nature</li>
<li>它们intrinsic design无法modeling Web-scale heterogeneous graph</li>
</ol>
<blockquote>
<p>First, most of them involve the design of meta paths for each type of heterogeneous graphs, requiring specific domain knowledge;</p>
<p>Second, they either simply assume that different types of nodes/edges share the same feature and representation space or keep distinct non-sharing weights for either node type or edge type alone, making them insufficient to capture heterogeneous graphs’ properties;</p>
<p>Third, most of them ignore the dynamic nature of every (heterogeneous) graph;</p>
<p>Finally, their intrinsic design and implementation make them incapable of modeling Web-scale heterogeneous graphs.</p>
</blockquote>
<p><strong>Method</strong>：作者提出了HGT期望能够解决上面的四个问题。</p>
<ol type="1">
<li>To handle graph heterogeneity，引入node- and edge-type dependent attention mechanism. 在计算边<span class="math inline">\(&lt;s, t&gt;\)</span>的attention时，使用meta relation <span class="math inline">\(⟨node\ type\ of\ s,\ edge\ type\ of\ e\ between\ s\ \&amp;\ t,\ node\ type\ of\ t⟩\)</span>。给不同的type定义不同的weight matrix，然后组合起来成为这条边的weight matrix。这样子的话不同type的weight matrix就可以互相交互，互相影响。weight matrix被用来计算attention。另外，由于GNN的天性，聚合多阶邻居实际就是在被认为学习“soft” meta paths。另外根据attention，又能够更好的区分学习到的这些soft meta path。</li>
<li>To handle graph dynamics，类似于Transformer position encoding，定义了relative temporal encoding (RTE)。不是把不同timestamp的graph看做不同的图，而是直接把带有不同RTE的node一起聚合。</li>
<li>To handle Web-scale graph data，提出了HGSampling，采样的subgraph具有较为均匀的node type分布，同时还保证the sampled sub-graphs dense for minimizing the loss of information。</li>
</ol>
<p>先来看几个定义：</p>
<p>Heterogeneous Graph（a.k.a. heterogeneous information networks）</p>
<blockquote>
<p>Definition 1. Heterogeneous Graph: A heterogeneous graph is defined as a directed graph G = (V, E, A, R) where each node v ∈ V and each edge e ∈ E are associated with their type mapping functions τ(v) : V → A and ϕ(e) : E → R, respectively.</p>
</blockquote>
<p>“异质”的本质就是来源各不相同，type不同。</p>
<p>Meta Relation</p>
<blockquote>
<p>For an edge <span class="math inline">\(e = (s,t)\)</span> linked from source node s to target node t, its meta relation is denoted as <span class="math inline">\(⟨\tau (s),\phi (e),\tau (t)⟩\)</span>。</p>
</blockquote>
<p>常用的meta path可以看做是一系列这样的meta relation的组合</p>
<p>Dynamic Heterogeneous Graph.</p>
<blockquote>
<p>To model the dynamic nature of real-world (heterogeneous) graphs, we assign an edge <span class="math inline">\(e = (s,t)\)</span> a timestamp <span class="math inline">\(T\)</span>, when node <span class="math inline">\(s\)</span> connects to node <span class="math inline">\(t\)</span> at <span class="math inline">\(T\)</span>.</p>
</blockquote>
<p>当一个node <span class="math inline">\(s\)</span>在时间<span class="math inline">\(T\)</span>链接到<span class="math inline">\(t\)</span>的时候，认为这条边的timestamp就是<span class="math inline">\(T\)</span>，并且在以后不再更改。</p>
<h2 id="method">Method</h2>
<p>整体结构：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210414163722393.png" /></p>
<p>注意这里，A-Linear是根据node <span class="math inline">\(t\)</span>的type决定的。同时使用了残差结构。</p>
<p>聚合函数就是直接相加。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210414170116901.png" /></p>
<p>核心是两部分，产生消息，然后产生注意力。</p>
<p>产生消息：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210414163415151.png" /></p>
<p>邻居node的type和邻居relation的type的weight相乘。</p>
<ul>
<li><p>邻居节点的type矩阵：用于转换node type独有的特征分布</p></li>
<li><p>邻居边的type矩阵：用于进行edge type的转换</p></li>
</ul>
<p>产生attention：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210414170336206.png" /></p>
<p>这里在计算attention的时候就使用了meta relation。矩阵相乘的操作表示着parameter sharing。</p>
<p>与原始的transformer比较有三点不同：</p>
<ol type="1">
<li><p>考虑异质性，在计算Query和Key vector的时候使用不同类型的weight matrix。</p></li>
<li><p>在Q和K相乘时，中间加入了<span class="math inline">\(W_ϕ(e)^{ATT}\)</span>，加入它可以建模edge的信息</p></li>
<li><p>加入了μ∈R^{|A|×|R|×|A|} ，它是一个全局的meta relation weight，衡量全局意义上的元关系重要性</p></li>
</ol>
<p>除去vanilla Transformer中已经使用的K-Q计算注意力，值得注意的是加入了一个<span class="math inline">\(\mu \in \mathbb{R}^{|\mathcal{A}|\times |\mathcal{R}|\times |\mathcal{A}|}\)</span>。为所有的meta relation组合定义了一个全局范围的权重。这样导致的后果是除去单纯局部的attention，加入了global attention，能够更adaptively的学习attention。</p>
<h2 id="rte">RTE</h2>
<p>前面提到了RTE，它的核心思想是为不同timestamp的edge学习不同的表示，然后加到邻居node <span class="math inline">\(s\)</span>的表表示<span class="math inline">\(H[s]\)</span>上。</p>
<p>首先计算时间差，</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210414170312971.png" /></p>
<p>，然后编码，使用一个scalar生成一个embedding</p>
<p><span class="math inline">\(2i,\ 2i+1\)</span>应该是dim</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210414180544283.png" /></p>
<p>最后加到邻居node <span class="math inline">\(s\)</span>的表表示<span class="math inline">\(H[s]\)</span>上。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210414163541401.png" /></p>
<h2 id="hgsampling">HGSampling</h2>
<p>为了训练大规模的graph，必须进行采样，每次只训练一部分的graph，即采样subgraph。这一操作在graphSAGE中已经有相应的算法。但是问题在于，这样随机的采样导致后果是sub-graph在不同type下采样的数量非常imbalance。</p>
<blockquote>
<p>To address this issue, different sampling-based methods [1, 2, 7, 29] have been proposed to train GNNs on a subset of nodes. However, directly using them for heterogeneous graphs is prone to get sub-graphs that are extremely imbalanced regarding different node types, due to that the degree distribution and the total number of nodes for each type can vary dramatically.</p>
</blockquote>
<p>HGSampling算法能够保证两点：</p>
<ol type="1">
<li>不同类型的node和edge具有相近的数量</li>
<li>每次采样得到的sub-graph足够密集能够用于降低loss，同时减少sample variance</li>
</ol>
<p>核心思想是为不同的node type，根据重要程度，采样相同数量的node。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210414180603410.png" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210414152012151.png" /></p>
<h2 id="experiment">Experiment</h2>
<p>核心数据集，Open Academic Graph (OAG)</p>
<blockquote>
<p>OAG consists of more than 178 million nodes and 2.236 billion edges—the largest publicly available heterogeneous academic dataset. In addition, all papers in OAG are associated with their publication dates, spanning from 1900 to 2019.</p>
</blockquote>
<p>为了验证模型的泛化性，同时从OAG中抽离出来两个不同的子集，Computer Science (CS) and Medicine (Med) academic graphs。</p>
<p>对于所有的edge，增加self和reverse relation。</p>
<p>prediction task有四个：</p>
<ul>
<li>the prediction of Paper–Field (L1)</li>
<li>Paper–Field (L2)</li>
<li>Paper–Venue</li>
<li>Author Disambiguation</li>
</ul>
<p>前三个实际是分类任务，最后一个是link prediction，使用了NTN。</p>
<blockquote>
<p>For author disambiguation, we select all the authors with the same name and their associated papers. The task is to conduct link prediction between these papers and candidate authors. After getting the paper and author node representations from GNNs, we use a Neural Tensor Network to get the probability of each author-paper pair to be linked.</p>
</blockquote>
<p>指标：</p>
<ul>
<li>NDCG</li>
<li>MRR</li>
</ul>
<p>使用2015年以前的数据作为训练集，2015-2016年的数据作为验证集，2016-2019年的数据作为测试集。</p>
<p>初始化的特征：</p>
<ul>
<li>paper：使用提前训练好的XLNet，然后根据title的word，使用attention平均，捕获语义特征</li>
<li>author：所有发表论文的特征，然后平均</li>
<li>field，venue和institute：使用metapath2vec，捕获结构特征</li>
</ul>
<p>为了公平比较，对于其它的baseline，在输入之前增加一层adaptation layer，把不同type的feature投影到同一分布下。</p>
<p>实现细节：</p>
<ul>
<li>256 dim</li>
<li>8 multi-head number</li>
<li>3 layer</li>
<li>All baselines are optimized via the AdamW optimizer</li>
<li>Cosine Annealing Learning Rate Scheduler</li>
<li>200 epoch</li>
<li>select the one with the lowest validation loss</li>
</ul>
]]></content>
      <categories>
        <category>Paper</category>
        <category>GNN</category>
      </categories>
  </entry>
  <entry>
    <title>HetSANN</title>
    <url>/gnn/HetSANN/</url>
    <content><![CDATA[<h1 id="an-attention-based-graph-neural-network-for-heterogeneous-structural-learning">An Attention-based Graph Neural Network for Heterogeneous Structural Learning</h1>
<p><a href="https://github.com/didi/hetsann">HetSANN</a>，AAAI 2020</p>
<p>提出了Heterogeneous Graph Structural Attention Neural Network (HetSANN），主要创新点有三个：</p>
<ul>
<li>对于预测标签任务，采用多任务学习，不同type的节点进行预测有不同的classifier（实际是全连接层+softmax）</li>
<li>针对edge和reversed edge，除了一般的基于拼接的方法计算attention外，提出了voice-sharing product的计算注意力方法。</li>
<li>在不同type的邻居信息转换中，提出了一个保持weight matrix的cycle consistent的方法。</li>
</ul>
<span id="more"></span>
<h2 id="introduction">Introduction</h2>
<p>作者认为如果使用基于meta-path的方法有以下缺点</p>
<ol type="1">
<li>meta-path的选择依赖于专家，并且需要手动设计</li>
<li>在meta-path中间的节点和边的信息被忽略。</li>
</ol>
<h2 id="method">Method</h2>
<p>看一下模型的整体结构：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210803193220627.png" alt="image-20210803193220627" style="zoom:50%;" /></p>
<h3 id="type-aware-attention-layer-tal">Type-aware Attention Layer (TAL)</h3>
<p>采用多头注意力（一般是8个头）</p>
<p>首先是基于type的邻居信息转化，node <span class="math inline">\(i\)</span> 提供给node <span class="math inline">\(j\)</span>。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210803193433545.png" alt="image-20210803193433545" style="zoom:50%;" /></p>
<p>注意，这里的<span class="math inline">\(W\)</span>是根据中心node和邻居node的type同时区分的。</p>
<p>然后基于注意力聚合邻居信息，下面的是一般的GAT的方法，作者叫做<em>concat product</em>。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210803193632456.png" alt="image-20210803193632456" style="zoom:50%;" /></p>
<p>需要注意的是，这里的注意力向量<span class="math inline">\(\alpha_r\)</span>，是每个edge type各有一个。然后就是基于softmax的attention聚合。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210803193716164.png" alt="image-20210803193716164" style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210803193733856.png" alt="image-20210803193733856" style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210803193750622.png" alt="image-20210803193750622" style="zoom:50%;" /></p>
<p>实际上，作者还提出了<em>voice-sharing</em>的注意力计算方法，主要是希望考虑关系和逆关系之间的对应联系。让注意力向量<span class="math inline">\(\alpha_r\)</span>互为负数，然后利用相加计算注意力。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210803194003584.png" alt="image-20210803194003584" style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210803194127412.png" alt="image-20210803194127412" style="zoom:50%;" /></p>
<p>作者起名叫voice-sharing是因为以下的原因：</p>
<blockquote>
<p>The voice is the concept of English grammar including active voice and passive voice. Here we refer the active voice to the directed edge (cite, write, etc.) and refer the passive voice to the reversed edge (cited, written, etc.).</p>
</blockquote>
<p>最后的输出，就是多头注意力拼接+残差</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210803194515638.png" alt="image-20210803194515638" style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210803194536089.png" alt="image-20210803194536089" style="zoom:50%;" /></p>
<p>整个TAL层如图所示。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210803194602158.png" alt="image-20210803194602158" style="zoom:50%;" /></p>
<h3 id="multi-task-learning">Multi-task Learning</h3>
<p>对于不同type node的预测，定义了不同的output layer（一个全连接层）和softmax。但是gnn的参数是一样的。</p>
<h3 id="cycle-consistency-loss">Cycle-consistency Loss</h3>
<p>这一点很有意思，作者认为对于node的状态转化可以形成一个循环：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210803194815041.png" alt="image-20210803194815041" style="zoom: 33%;" /></p>
<p>从中心节点node <span class="math inline">\(j\)</span>出发，有一个self-loop，作者认为self-loop之后的状态应该和从<span class="math inline">\(j\rightarrow i,\ i\rightarrow i,\ i\rightarrow j\)</span>的循环一样。即下面的式子：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210803195143924.png" alt="image-20210803195143924" style="zoom:50%;" /></p>
<p>由于matrix的逆矩阵比较难算，作者直接定义了一个新的逆矩阵</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210803195313907.png" alt="image-20210803195313907" style="zoom:50%;" /></p>
<p>最后，上面的两个约束体现在loss中：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210803195347065.png" alt="image-20210803195347065" style="zoom:50%;" /></p>
<p>最终的loss：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210803195411879.png" alt="image-20210803195411879" style="zoom:50%;" /></p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>GNN</category>
      </categories>
      <tags>
        <tag>GNN</tag>
      </tags>
  </entry>
  <entry>
    <title>How-Powerful-of-GNN</title>
    <url>/gnn/How-Powerful-of-GNN/</url>
    <content><![CDATA[<h1 id="how-powerful-are-graph-neural-networks">HOW POWERFUL ARE GRAPH NEURAL NETWORKS?</h1>
<p>This paper:</p>
<ol type="1">
<li>characterize how expressive different GNN variants are in learning to represent and distinguish between different graph structures</li>
<li>show that GNNs are at most as powerful as the WL test in distinguishing graph structures.</li>
<li>identify graph structures that cannot be distinguished by popular GNN variants, such as GCN (Kipf &amp; Welling, 2017) and GraphSAGE (Hamilton et al., 2017a)</li>
<li>develop a simple neural architecture, Graph Isomorphism Network (GIN)</li>
</ol>
<span id="more"></span>
<div class="pdf-container" data-target="HOW-POWERFUL-ARE-GRAPH-NEURAL-NETWORKS.pdf" data-height="1000px"></div>
]]></content>
      <categories>
        <category>Paper</category>
        <category>GNN</category>
      </categories>
  </entry>
  <entry>
    <title>InteratE</title>
    <url>/gnn/InteractE/</url>
    <content><![CDATA[<h1 id="interacte-improving-convolution-based-knowledge-graph-embeddings-by-increasing-feature-interactions">InteractE: Improving Convolution-based Knowledge Graph Embeddings by Increasing Feature Interactions</h1>
<p>2019-11-12</p>
<p>在ConvE的基础上，进行了特征dim随机排列，棋盘状的排列，以及多channel的卷积。</p>
<span id="more"></span>
<h2 id="introduction">1 Introduction</h2>
<p>由三元组表示的知识图谱在很多地方有广泛的应用：</p>
<ul>
<li>关系导出</li>
<li>问题回答</li>
<li>规范化</li>
<li>推荐系统</li>
<li>对话系统</li>
</ul>
<p>但是目前的知识图谱是imcomplete的，针对这个问题<em>link prediction</em>任务出现了。该任务的主要目标是根据已有知识图谱当中的事实(fact)来推测缺失的事实。常用的方法就是将所有的entity和relation转变为低维表示，然后利用这些低维表示通过一个得分函数（score function）来预测新的事实。</p>
<p>基于以上的思想，有几种不同类别的方法：</p>
<ul>
<li>基于翻译距离的方法：TransE、TransH</li>
<li>基于语义匹配的方法：HoleE、ANALOGY</li>
<li>神经网络方法：NTN、ConvE</li>
</ul>
<p>用CNN来学习KGE能够比一般的NN更有expressive，学习到非线性的信息；同时CNN本身就能保证参数效率，防止过度参数化。</p>
<h2 id="related-work">2 Related Work</h2>
<p>ConvE： <span class="math display">\[
\psi_r(e_s,e_o)=relu(vec(relu([\bar{e_s}; \bar{r_r}] \star \cal{w})) W)e_o
\]</span> 在2018年提出来的ConvE存在interaction被限制的缺点，因此提出了InteractE期望能够解决该问题。</p>
<h2 id="notation-and-deﬁnitions">3 Notation and Deﬁnitions</h2>
<p>实体<span class="math inline">\(s\)</span>和关系<span class="math inline">\(r\)</span>的<span class="math inline">\(d\)</span>维表示： <span class="math display">\[
e_s=(a_1,\dots,a_d) \\
e_r=(b_1,\dots,b_d)
\]</span> Reshaping Function： <span class="math display">\[
\phi: R^d\times R^d=R^{m\times n}
\]</span> <span class="math inline">\(\phi\)</span>函数有以下几种类型：<span class="math inline">\(\phi_{stk},\ \phi_{alt},\ \phi_{chk}\)</span></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20200405164138063.png" /></p>
<h2 id="interacte-details">4 InteractE Details</h2>
<p>总体结构</p>
<figure>
<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20200405191734032.png" alt="image-20200405171149898" /><figcaption>image-20200405171149898</figcaption>
</figure>
<h3 id="feature-permutation">4.1 Feature Permutation</h3>
<p>拿到向量<span class="math inline">\(e_s,\ e_r\)</span>先进行特征随机排列，也就是对于<span class="math inline">\(e_s=(a_1,\dots,a_d)\  e_r=(b_1,\dots,b_d)\)</span>进行随机排序。一共可以进行<span class="math inline">\(t\)</span>次。得到， <span class="math display">\[
P_t=[(e_s^1,e_r^1),\cdots,(e_s^t,e_r^t)]
\]</span></p>
<h3 id="checkered-reshaping">4.2 Checkered Reshaping</h3>
<p>对于上一步的结果，使用格子/棋盘reshape成二维矩阵， <span class="math display">\[
\phi_{chk}(P_t)=[\phi(e_s^1,e_r^1),\cdots,\phi(e_s^t,e_r^t)]
\]</span></p>
<h3 id="circular-convolution">4.3 Circular Convolution</h3>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20200405191900132.png" /></p>
<p>循环卷积， <span class="math display">\[
[x\star \omega]_{p,q}=\sum_{i=-\lfloor {k/2}\rfloor}^{\lfloor {k/2}\rfloor} \sum_{j=-\lfloor {k/2}\rfloor}^{\lfloor {k/2}\rfloor} x_{[p-i]_m,\ [q-j]_n} \omega_{i,j}
\]</span></p>
<h3 id="score-function">4.4 Score Function</h3>
<p><span class="math display">\[
\psi_r(e_s,e_o)=g(vec(f(\phi(P_t) \star \cal{w})) W)e_o
\]</span></p>
<h2 id="experimental-setup">5 Experimental Setup</h2>
<p>数据集</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20200405192616146.png" /></p>
<h3 id="performance-comparison">5.1 Performance Comparison</h3>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20200405193118638.png" /></p>
<p>在整体性能的比较上来看，对于ConvE是有较明显的提升。同时在WN18RR数据集上的效果不显著。这一点和在ConvE里面发现的现象一致，因为这个数据集可能更适合于浅层的模型因为它的平均关联度就很低。</p>
<h3 id="effect-of-feature-reshaping-and-circular-convolution">5.2 Effect of Feature Reshaping and Circular Convolution</h3>
<p>探究重组方法以及循环卷积对于效果的影响。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20200405171149898.png" /></p>
<p>可以看出来：</p>
<ul>
<li>多数情况下，采用同样的reshape方式，循环卷积要优于标准卷积</li>
<li>随着interaction的增多，效果逐渐增强，而且循环卷积+棋盘式总能取得最好的结果</li>
</ul>
<h3 id="effect-of-feature-permutations">5.3 Effect of Feature Permutations</h3>
<p>探究特征随机排列的次数对于模型结果的影响</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20200405193453990.png" /></p>
<p>可以观察到，实际上较小的组合数量就能够取得较优的结果，1/2次随机的排列组合就能够取得不错的结果。再增加组合次数会造成过度参数化。</p>
<h3 id="evaluation-on-different-relation-types">5.4 Evaluation on different Relation Types</h3>
<p>对于不同类型的关系进行预测的结果。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20200405171009766.png" /></p>
<p>可以看出来首先InteractE是全面优于ConvE的；另外，它更善于建模N-1和N-N的关系，而RotatE更擅长建模1-1的关系。</p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>KGE</category>
      </categories>
  </entry>
  <entry>
    <title>Introduction-GNN</title>
    <url>/gnn/Introduction-GNN/</url>
    <content><![CDATA[<h1 id="introduction-to-graph-neural-networks">Introduction to Graph Neural Networks</h1>
<p>Note of book 《Introduction to Graph Neural Networks》</p>
<span id="more"></span>
<h2 id="introduction">Introduction</h2>
<p>所谓的图graph就是一种数据结构，由节点集合与边集合组成。具有图结构的数据存在于众多的领域中，包括社交网络、化学分子、知识图谱等。最近图神经网络的出现使得在图上的深度学习成为了可能，图神经网络也受到了大量的研究关注。</p>
<p>图神经网络起源于两大部分，卷积神经网络（Convolutional Neural Network）与图嵌入（Graph Embedding）。</p>
<p>CNN在图像（image）上的成功在于它能够导出多尺度（multi-scale）局部化（localized）的空间特征。应用CNN的三个关键点在于局部连接（local connection）、共享参数（shared weight）以及多层结构（multi-layer）。这三个点对于图同样是很重要的，首先图中的绝大多数结构都是局部连接的；其次，共享参数能够帮助减少传统的基于谱图理论的图算法计算复杂度；最后，多层的结构能够用来捕获图中的层级结构信息。但是，由于图像是规则的欧几里得域（Euclidean domian）下的数据结构，而图是非欧几里得数据的（non-Euclidean），因此我们无法直接将传统的CNN应用到图上。我们需要一种新的模型，在保留CNN的三个关键特征的同时，能够处理非欧式空间下的图结构。</p>
<blockquote>
<p>简单辨析下欧几里得的数据以及非欧几里得下的数据：</p>
<p>在机器学习中处理的数据可以分为欧几里得和非欧几里得两类。两者的核心区别在于是否“排列整齐”，前者对于数据，可以使用<span class="math inline">\(\mathbb{R}^n\)</span>的空间进行描述，不丢失任何的原始信息。这一类数据的代表是图像、文字、一般的数值数据等。非欧几里得的数据实际广泛存在，主要有图（graph）和流型（mainfold）两类。对于这一类的数据，比如graph中的每个节点的邻居节点、邻居边都各不相同，数量不一，无法使用一个<span class="math inline">\(n\)</span>维的空间来完全描述此类数据。</p>
</blockquote>
<p>图嵌入的发展收到词嵌入（word embedding），Deepwalk被认为是首个学习图嵌入的方法，它将图中的每个节点都表示为了<span class="math inline">\(n\)</span>维的嵌入向量。Deepwalk首先在图上进行随机游走采样，之后应用SkipGram方法学习到合适的嵌入表示。在Deepwalk后，node2Vec，LINE，TADW这些方法逐步发展。但是这些方法存在两方面的缺点，一个是由于需要随机游走，学习到的方法很难直接用到新数据上，泛化性较差；另一方面是没有参数共享，导致随着图节点增多，模型参数也不断增加。</p>
<p>图神经网络在前两者的基础上，首先尝试将整个图都嵌入到欧式空间中，之后应用CNN的思想增强模型的表达能力，让嵌入能够表示更多的原始信息。</p>
<p>在这本书中，对于GNN的分类为：</p>
<ul>
<li>循环图神经网络（recurrent graph neural networks）</li>
<li>卷积图神经网络（convolutional graph neural networks）</li>
<li>图自编码器（graph auto-encoders）</li>
<li>时空图神经网络（spatial-temporal graph neural networks）</li>
</ul>
<h2 id="vanilla-graph-neural-networks">Vanilla Graph Neural Networks</h2>
<p>关于图神经网络GNN的概念实际上在2005年前后就已经有对应概念的提出[The graph neural network model; Graphical-based learning environments for pattern recognition]。</p>
<p>接下来介绍一个原始的模型vanilla GNN。它针对的是无向同质的图。</p>
<p>核心包括两个函数，局部转移函数（local transition function）以及局部输出函数（local output function）。 <span class="math display">\[
\mathbf{h}_v = f(\mathbf{x}_v,\mathbf{x}_{co[v]}, \mathbf{x}_{ne[v]}, \mathbf{h}_{ne[v]},) \\
\mathbf{o}_v = g(\mathbf{h}_v, \mathbf{x}_v)
\]</span> 其中，<span class="math inline">\(\mathbf{x}_{co[v]}\)</span>是邻居边的特征，<span class="math inline">\(\mathbf{x}_{ne[v]}\)</span>是邻居节点的特征，<span class="math inline">\(\mathbf{h}_{ne[v]}\)</span>是邻居节点的隐藏状态。</p>
<p>vanilla GNN不断迭代更新函数<span class="math inline">\(h\)</span> <span class="math inline">\(T\)</span>步，直到到达固定点，使得<span class="math inline">\(\mathbf{h}_v^T\approx \mathbf{h}_v^{T-1}\)</span>。这一操作的原理是Banach’s ﬁxed point theorem[An Introduction to Metric Spaces and Fixed Point Theory]。到达不动点之后，再进行梯度下降。</p>
<p>vanilla GNN的几个缺点：</p>
<ul>
<li>计算不够有效，每次都需要不断迭代T步之后才能进行梯度下降，实际上一般的神经网络是直接产生一个输出后就可进行梯度更新。</li>
<li>在T步的迭代中，vanilla GNN一直使用的是一样的参数，这就导致图的层级结构信息没有能够被显式的学习。实际上我们可以让模型的每次迭代都学习不同的参数。</li>
<li>vanilla GNN没有有效的建模边的信息。</li>
<li>如果图中的节点数量很多，使用不动点这种原理，可能会导致各个节点过平滑，差异性不够</li>
</ul>
<h2 id="graph-convolutional-networks">Graph Convolutional Networks</h2>
<p>主要有两类在图上进行卷积操作的GNN，一类是谱空间下卷积操作，一类是空间领域下的卷积操作。</p>
<h3 id="spectral-methods">Spectral Methods</h3>
<h4 id="spectral-gnn">Spectral GNN</h4>
<p>2014年。Spectral networks and locally connected networks on graphs</p>
<p>通过对图拉普拉斯矩阵进行特征分解，定义了在傅里叶域下的卷积操作。 <span class="math display">\[
\mathbf{g}_\theta \star \mathbf{x} = \mathbf{U} \mathbf{g}_\theta \mathbf{U}^T \mathbf{x}
\]</span> 详细的可以看之前的GCN笔记。</p>
<h4 id="chebnet">ChebNet</h4>
<p>2011年。Convolutional neural networks on graphs with fast localized spectral ﬁltering.</p>
<p>对上面公式中的<span class="math inline">\(\mathbf{g}_\theta\)</span>使用切比雪夫多项式进行估计，从而无需计算特征向量<span class="math inline">\(\mathbf{U}\)</span>。</p>
<h4 id="gcn">GCN</h4>
<p>2017年。Semi-supervised classiﬁcation with graph convolutional networks.</p>
<p>出现了著名的图卷积算子，它实际是在CheNet进一步的简化，约定了切比雪夫多项式只包括前两步。 <span class="math display">\[
\mathbf{Z}=\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} \mathbf{X} \Theta
\]</span></p>
<h4 id="agcn">AGCN</h4>
<p>2018年，Adaptive graph convolutional neural networks.</p>
<p>AGCN假设除了图的结构能够表现图中节点的关系外，它们之间应该还存在某种隐式的联系。</p>
<h3 id="spatial-methods">Spatial Methods</h3>
<p>基于空间域的方法是直接在graph上进行操作，不需要计算特征向量。</p>
<p>Spatial methods 和 Spectral methods的对比：</p>
<p>由于效率、通用性和灵活性问题，空间模型优于光谱模型。首先，光谱模型的效率低于空间模型。谱模型要么需要进行特征向量计算，要么需要同时处理整个图。由于空间模型通过信息传播直接在图域中执行卷积，因此空间模型更适合于大图形。计算可以在一批节点中进行，而不是整个图。其次，依赖于图Fourier基的谱模型很难推广到新的图。它们假设一个固定的图。对图的任何扰动都会导致特征基的变化。另一方面，基于空间的模型在每个节点上执行局部的图形卷积，在这些节点上可以很容易地在不同的位置和结构之间共享权重。第三，基于谱的模型仅适用于无向图。基于空间的模型更灵活地处理多源图形输入</p>
<h4 id="neural-fps">Neural FPS</h4>
<p>2015年，Convolutional networks on graphs for learning molecular ﬁngerprints</p>
<p>核心思想是对于具有不同度数的节点学习不同的权值矩阵。缺点是很难直接应用到大规模的图上。</p>
<h4 id="patchy-san">PATCHY-SAN</h4>
<p>2016年，Learning convolutional neural networks for graphs.</p>
<p>核心思想是对于图中的每个节点，基于广度优先搜索选择固定<span class="math inline">\(k\)</span>个邻居作为接受域，这样就将图处理问题转化为了传统的欧几里得数据问题，最后输入到CNN中进行处理。</p>
<h4 id="dcnn">DCNN</h4>
<p>2016年，Diﬀusion-convolutional neural networks.</p>
<p>扩散卷积神经网络</p>
<h4 id="dgcn">DGCN</h4>
<p>2018年，Dual graph convolutional networks for graph-based semisupervised classiﬁcation</p>
<p>重点读：双向卷积神经网络，既考虑了局部一致性，也考虑了全局一致性</p>
<h4 id="lgcn">LGCN</h4>
<p>2018年，Large-scale learnable graph convolutional networks.</p>
<h4 id="monet">MONET</h4>
<p>2017年，Geometric deep learning on graphs and manifolds using mixture model CNNs.</p>
<h4 id="graphsage">GraphSAGE</h4>
<p>2017年</p>
<h2 id="graph-attention-network">Graph Attention Network</h2>
<h3 id="gat">GAT</h3>
<p>2018年，Graph attention networks</p>
<p>使用多头注意力机制。</p>
<h3 id="gaan">GaAN</h3>
<p>2018年，GaAN: Gated attention networks for learning on large and spatiotemporal graphs.</p>
<p>同样使用多头注意力机制，但是使用了key-value的机制。</p>
<h2 id="graph-recurrent-networks">Graph Recurrent Networks</h2>
<p>融合了gate机制的模型，核心思想是提升图信息long term聚和的效果。</p>
<h3 id="ggnn">GGNN</h3>
<p>2016年，Gated graph sequence neural networks.</p>
<p>利用GRU，每次聚合<span class="math inline">\(T-1\)</span>步下的邻居信息，输入GRU，得到最后的输出</p>
<h3 id="tree-lstm">Tree-LSTM</h3>
<p>2015年，Improved semantic representations from treestructured long short-term memory networks.</p>
<p>利用LSTM</p>
<h3 id="graph-lstm">Graph LSTM</h3>
<p>2017年，Cross-sentence N-ary relation extraction with graph LSTMs.</p>
<h3 id="sentence-lstm">Sentence-LSTM</h3>
<p>2018年，Sentence-state LSTM for text representation.</p>
<p>将文本直接转换为graph，然后使用GNN进行学习，在很多NLP任务上表现出了很好的性能。</p>
<h2 id="graph-redidual-networks">Graph Redidual Networks</h2>
<p>使用残差链接，为了缓解在GCN中聚合多层信息效果反而下降的情况。</p>
<h3 id="highway-gcn">Highway GCN</h3>
<p>2018年，Semi-supervised user geolocation via graph convolutional networks.</p>
<p>在更新节点状态时，使用了门机制。</p>
<h3 id="jump-knowledge-network">Jump Knowledge Network</h3>
<p>2018年，Representation learning on graphs with jumping knowledge networks.</p>
<p>每一层都会直接连接向最后一层。</p>
<h3 id="deepgcns">DEEPGCNS</h3>
<p>2019年，DeepGCNs: Can GCNs go as deep as CNNs?</p>
<p>使用skip connection和dense connection解决两个问题：GNN中的梯度消失以及过度平滑。</p>
<h2 id="heterogeneous-graph">Heterogeneous Graph</h2>
<ul>
<li>HAN：</li>
<li>PP-GCN：Fine-grained event categorization with heterogeneous graph convolutional networks.</li>
<li>ActiveHNE：Activehne: Active heterogeneous network embedding.</li>
</ul>
<h2 id="multi-dimensional-graph">Multi-Dimensional Graph</h2>
<ul>
<li>Multi-dimensional graph convolutional networks</li>
</ul>
<h2 id="sampling">Sampling</h2>
<ul>
<li>GraphSAGE（2017）: 邻居节点随机采样</li>
<li>PinSage（2018）: 基于邻居节点重要性采样，Graph convolutional neural networks for web-scale recommender systems.</li>
<li>FastGCN：FastGCN: Fast learning with graph convolutional networks via importance sampling</li>
<li>Adaptive sampling towards fast graph representation learning：参数化，可训练的采样器</li>
<li>SSE：Learning steady-states of iterative algorithms over graphs.</li>
</ul>
<h2 id="graph-auto-encoder">Graph Auto Encoder</h2>
<ul>
<li>GAE: Variational graph auto-encoders.</li>
<li>ARGA: Adversarially regularized graph autoencoder for graph embedding.</li>
<li>DGI: Deep graph infomax.</li>
</ul>
<h2 id="general-framework">General Framework</h2>
<ul>
<li>MPNN：</li>
<li>NLNN：Non-local neural networks.</li>
<li>GN：Relational inductive biases, deep learning, and graph networks.</li>
</ul>
]]></content>
      <categories>
        <category>Paper</category>
        <category>GNN</category>
      </categories>
  </entry>
  <entry>
    <title>M-GNN</title>
    <url>/gnn/M-GNN/</url>
    <content><![CDATA[<h1 id="robust-embedding-with-multi-level-structures-for-link-prediction">Robust Embedding with Multi-Level Structures for Link Prediction</h1>
<p>这篇文章提出了一种multi-level graph neural network，M-GNN。使用GIN中的MLP学习结构信息，然后提出了一种基于KG中图的不同粒度进行建模的方法。它会从原始的KG出发，不断合并邻居节点，合并边，构造出一系列不同粒度的graph，在这些graph上进行图卷积操作，得到最后的输出。除了一般的链路预测实验，作者还进行了在不同稀疏度以及加入noising edges的实验。</p>
<span id="more"></span>
<h2 id="graph-coarsening">Graph Coarsening</h2>
<p>和一般的GNN消息聚合方式不同，M-GNN希望能够建模KG中不同尺度中的信息。</p>
<p>首先构造k个Coarsened graph：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210826143747570.png" alt="image-20210826143747570" style="zoom:50%;" /></p>
<p>核心是两种合并结构的方法：<strong>edge coarsening</strong>和<strong>neighbor coarsening</strong>。</p>
<p>M-GNN考虑在图中的不同relation包括不同的结构信息：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210826144601695.png" alt="image-20210826144601695" style="zoom:50%;" /></p>
<p>对于1-1 structure，使用<strong>edge coarsening</strong>，1-1 structure是指两个edge没有相连的相同实体。edge coarsening直接把这样的edge分别看做是新的super node，对edge进行了coarsening。如下图a所示。edge coarsening使得节点包括了更多阶邻居的信息。</p>
<p>对于1-n或者n-1 structure，同一关系下的不同邻居实体共享某种相同的信息，可以进行聚合，把两个邻居实体合并为新的super node，叫做<strong>neighbor coarsening</strong>。如下图b, c所示。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210826145202651.png" alt="image-20210826145202651" style="zoom:50%;" /></p>
<p>对于n-n structure，可以看做是多个1-n或者n-1结构，不需要单独处理。</p>
<p>通过先neighbor coarsening压缩实体，然后edge coarsening进一步压缩graph中实体和关系数量。</p>
<h2 id="multi-level-gnn">Multi-Level GNN</h2>
<p>与产生不同粒度的graph的顺序相反，在进行GNN的消息传递时，先从最粗粒度的graph进行学习，然后到更细粒度的graph，每个graph对应一层GNN。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210826150307208.png" alt="image-20210826150307208" style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210826150339407.png" alt="image-20210826150339407" style="zoom:50%;" /></p>
<p>公式中的<span class="math inline">\(S\)</span>是指实体到实体的对应矩阵。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210826150505564.png" alt="image-20210826150505564" style="zoom:50%;" /></p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>GNN</category>
      </categories>
      <tags>
        <tag>GNN</tag>
        <tag>KGE</tag>
      </tags>
  </entry>
  <entry>
    <title>MAGNN</title>
    <url>/gnn/MAGNN/</url>
    <content><![CDATA[<h1 id="magnn-metapath-aggregated-graph-neural-network-for-heterogeneous-graph-embedding">MAGNN: Metapath Aggregated Graph Neural Network for Heterogeneous Graph Embedding</h1>
<p>看一下整体结构：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210425135447705.png" style="zoom:50%;" /></p>
<span id="more"></span>
<p><strong>Motivation</strong>：</p>
<p>对于GNN，大部分的GNN假设graph是同质图，但是很多实际的graph都是异质图。</p>
<p>对于处理异质图的传统方法，是基于meta path的方法。但是这些基于meta path的方法存在几个缺点：</p>
<ul>
<li>The model does not leverage node content features, so it rarely performs well on heterogeneous graphs with rich node content features</li>
<li>The model discards all intermediate nodes along the metapath by only considering two end nodes，模型不考虑meta path中的中间节点</li>
<li>模型只依赖于单个meta path，The model relies on a single metapath to embed the heterogeneous graph.</li>
</ul>
<p><strong>Method</strong>：</p>
<p>提出MAGNN。对于不同type的node，先是linear projection到统一feature space中。之后，经过下面几个步骤：</p>
<p>对于同一metapath下的节点，编码单个邻居信息：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220309182244290.png" alt="image-20220309182244290" style="zoom:50%;" /></p>
<p>实验了三个具体的方法</p>
<ul>
<li>Mean：</li>
</ul>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210425135012239.png" style="zoom:67%;" /></p>
<ul>
<li>Linear Mean：</li>
</ul>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220309182100917.png" alt="image-20220309182100917" style="zoom:50%;" /></p>
<ul>
<li>Relational rotation encoder：</li>
</ul>
<p>模仿RotatE，可以建模metapath下的节点的序列信息，这一点区别于上面的方法。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210425135214910.png" style="zoom: 67%;" /></p>
<h2 id="intra-metapath-aggregation">Intra-metapath Aggregation</h2>
<p>对于同一metapath下的节点，基于注意力聚合。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220309182403668.png" alt="image-20220309182403668" style="zoom:50%;" /></p>
<p>这里需要注意的是就是在计算注意力加入了target node embedding。</p>
<p>并且使用了多头注意力。</p>
<h2 id="inter-metapath-aggregation">Inter-metapath Aggregation</h2>
<p>计算在全局下，所有metapath的weight。首先对于不同type的node，相加相同mepath的embedding。需要注意的是，不是计算单个node的不同metapath。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210425140032603.png" style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210425140219668.png" style="zoom:50%;" /></p>
<p>计算注意力：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210425134123312.png" style="zoom:50%;" /></p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>GNN</category>
      </categories>
  </entry>
  <entry>
    <title>MPNN</title>
    <url>/gnn/MPNN/</url>
    <content><![CDATA[<h1 id="neural-message-passing-for-quantum-chemistry">Neural Message Passing for Quantum Chemistry</h1>
<p>ICML 2017</p>
<p>Google Brain, Google DeepMind</p>
<p>本文就提出了一种图上进行监督学习的泛化框架Message Passing Neural Networks (MPNNs)</p>
<span id="more"></span>
<h2 id="introduction">1. Introduction</h2>
<p>虽然化学家们已经尝试将机器学习应用到化学任务上，但是之前的很多工作还是在围绕特征工程打交道。虽然神经网络在其它很多领域已经很成功，但是在化学领域还处在很初始的阶段。</p>
<p>最近，随着high throughput experiments的进步，量子化学计算与分子动态模拟产生了大量的数据，导致之前的经典方法无法再处理这样数据量的数据，需要一种新的更灵活的方法。</p>
<p>而在化学分子上设计的神经网络需要满足图同构的情况下不变：</p>
<blockquote>
<p>The symmetries of atomic systems suggest neural networks that operate on graph structured data and are invariant to graph isomorphism might also be appropriate for molecules.</p>
</blockquote>
<p>本文就提出了一种图上进行监督学习的泛化框架Message Passing Neural Networks (MPNNs)</p>
<p>预测任务是预测小型有机分子化学属性。</p>
<p>使用QM9数据集。</p>
<h2 id="message-passing-neural-networks">2. Message Passing Neural Networks</h2>
<p>MPNN，泛化了至少之前的8种方法。分为两大阶段，message passing phase和readout phase。</p>
<h3 id="message-passing-phase">message passing phase</h3>
<p>包括两个函数，消息函数Message Function和Update Function。</p>
<p>Message Function：用来产生消息，<span class="math inline">\(M_t(h_v^t, h_w^t, e_{v,w})\)</span> <span class="math display">\[
m_v^{t+1}=\sum_{w\in N(v)} M_t(h_v^t, h_w^t, e_{vw})
\]</span> Update Function: 更新节点状态 <span class="math display">\[
h_v^{t+1}=U_t(h_v^t, m_v^{t+1})
\]</span></p>
<h3 id="readout-phase">readout phase</h3>
<p>这一阶段是针对图级别的任务。 <span class="math display">\[
y^\prime=R(\{ h_v^T | v\in G \})
\]</span></p>
<h2 id="mpnn-variants">3 MPNN Variants</h2>
<p>接下来描述MPNN中具体实现的时候使用的结构。</p>
<p>基于GG-NN进行探究，</p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>GNN</category>
      </categories>
  </entry>
  <entry>
    <title>node2vec</title>
    <url>/gnn/node2vec/</url>
    <content><![CDATA[<p>本文就提出了一种无监督的方法。核心思想：通过特定的游走方式进行采样，对于每个点都会生成 对应的序列。再将这些序列视为文本导入skip-gram模型，即可得 到每个节点的向量</p>
<span id="more"></span>
<div class="pdf-container" data-target="node2vec-Scalable-Feature-Learning-for-Networks.pdf" data-height="1000px"></div>
]]></content>
      <categories>
        <category>Paper</category>
        <category>Graph Embedding</category>
      </categories>
  </entry>
  <entry>
    <title>revisting-GCN</title>
    <url>/gnn/revisting-GCN/</url>
    <content><![CDATA[<h1 id="revisiting-graph-neural-networks-all-we-have-is-low-pass-filters">Revisiting Graph Neural Networks: All We Have is Low-Pass Filters</h1>
<p>这篇文章中，作者从图信号处理GSP的角度出发，有三方面的贡献：</p>
<ul>
<li>首先实验发现大多数的信息隐藏在邻居信息的低频特征中，并且低频特征的信息以及足够丰富；提出了假设1：输入特征包括低频真实特征和噪声。真实特征为机器学习任务提供了足够的信息。</li>
<li>将图信号与传播矩阵相乘对应于低通滤波（low-pass filters），并且提出了gfNN(graph filter neural network)用于分析GCN和SGC</li>
<li>在假设1下，认为SGC、GCN 和 gfNN 的结果与使用真实特征的相应神经网络的结果相似。</li>
</ul>
<span id="more"></span>
<p>作者首先做了一个实验，通过图傅里叶变化，使用不同的频率的信息经过mlp进行预测。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210628205446190.png" style="zoom:50%;" /></p>
<p>实验结果：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210628205516246.png" style="zoom:50%;" /></p>
<p>结果分析：高频的邻居信息与中心节点差异较大，可能是噪声；低频的意思是变化不剧烈，中心节点的信号与邻居节点的信号差值不大。虽然人工增加了噪声，但是在低频下没有太多变化。低频特征足以提供足够的信息。</p>
<p>之后，作者证明了将特征矩阵<span class="math inline">\(X\)</span>与邻居矩阵相乘就是作为低通滤波器。</p>
<p>证明过程来自<a href="https://www.zhihu.com/question/427800721/answer/1547978404">知乎回答</a>，不是论文本身的内容。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210628205850099.png" style="zoom:50%;" /></p>
<p>也就是说，与正则化的邻接矩阵相乘时，由于所有的特征都是大于等于0的，因此低频特征对应的<span class="math inline">\(p(\lambda)\)</span>大，而高频特征对应的<span class="math inline">\(p(\lambda)\)</span>小，即起到了一个低通滤波的作用。降低高频特征中的噪声，加强低频特征中的信息。</p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>GNN</category>
      </categories>
      <tags>
        <tag>GNN</tag>
      </tags>
  </entry>
  <entry>
    <title>GNN-Collection</title>
    <url>/collection/GNN-Collection/</url>
    <content><![CDATA[<h1 id="collection-of-gnn-papers">Collection of GNN papers</h1>
<ul>
<li><a href="#highway-gnn">Highway GNN（ACL 2018）</a></li>
<li><a href="#hgsl">HGSL（AAAI 2021）</a></li>
<li>HGAT（EMNLP 2019）</li>
<li>HetGNN（KDD 2019）</li>
<li><a href="/gnn/HetSANN/" title="HetSANN">HetSANN</a>
（AAAI 2020）</li>
<li>RHINE（AAAI 2019）</li>
<li>JK（ICML 2018）</li>
<li>PATHCON（KDD 2021）</li>
<li>HeteGNN（WSDM 2021）</li>
<li>KGNN（IJCAI 2020）</li>
<li>CPRL（NAACL 2021）</li>
<li>CLHG（ACL 2021）</li>
<li>EAGCN（Neurocomputing）</li>
<li>ETGAT（ACL-IJCNLP 2021）</li>
<li>GAEAT（CIKM 2020）</li>
<li>M-GNN（IJCAI 2019）</li>
<li>RDGCN（IJCAI 2019）</li>
<li>SLiCE（WWW 2021）</li>
<li>M<sup>2</sup>GNN（WWW 2021）</li>
<li>LGNN（IJCAI 2021）</li>
<li>RevGNN（ICML 2021）</li>
</ul>
<span id="more"></span>
<h2 id="highway-gnn">Highway GNN</h2>
<p><a href="https://github.com/%20afshinrahimi/geographconv"><strong>Semi-supervised User Geolocation via Graph Convolutional Networks</strong></a> ACL 2018</p>
<p>应用场景是社交媒体上的用户定位。单纯的在GNN上的创新点是使用Gate机制来控制传入的邻居的信息。</p>
<p>在每一层，借鉴Highway networks的思路，计算一个门weight</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210708195301778.png" style="zoom:50%;" /></p>
<h2 id="hgsl">HGSL</h2>
<p><strong>Heterogeneous Graph Structure Learning for Graph Neural Networks</strong> AAAI 2021</p>
<a href="/gnn/HGSL/" title="[详细博客]">[详细博客]</a>
<p>作者声称是首个尝试为异质图神经网络寻找最优的图结构进行学习的方法，提出了HGSL（Heterogeneous Graph Structure Learning）。核心方法有两个，异质图结构学习和图神经网络。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210706213222126.png" style="zoom:40%;" /></p>
<p><strong>motivation</strong>：目前的异质图神经网络基于一个假设，学习使用的graph是足够好的。但是实际上这个假设不一定总能够满足。两个方面的原因，（1）在建模graph的时候，使用到的信息难免会包含错误的信息，导致最终的graph是不够好的（2）另一个原因是异质图结构本身与下游任务是独立的，不一定是有利于下游任务的最优解。为了解决上面的问题，图结构学习graph structure learning (GSL)被提出来，但是这些方法主要是在考虑同质图，无法很好的考虑异质图中的异质性以及异质图中存在的复杂的交互。</p>
<p><strong>method</strong>：提出HGSL，首先学习合适的graph structure，然后在这个graph structure上使用GCN进行学习。这种heterogeneous graph structure learning是核心创新点，包括三种graph的融合，<strong>feature similarity graph</strong>，<strong>feature propagation graph</strong>,和<strong>semantic graph</strong>。</p>
<h2 id="hgat">HGAT</h2>
<p><strong>Heterogeneous Graph Attention Networks for Semi-supervised Short Text Classification</strong> EMNLP 2019</p>
<p>为短文本分类任务（semi-supervised short text classification）设计了一个异质图神经网络HGAT。</p>
<p>首先是利用原始文本构造一个异质图（HIN），把不同来源的文本组合到一起。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210728180026169.png"   style="zoom:50%;" /></p>
<p>重点在于，其中的node type各不相同，各自具有差异性很大的特征。</p>
<p>然后是设计的网络结构，重点在于设计了一个两层的attention。</p>
<p>不同type的node有不同的卷积核：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210728180219517.png"   style="zoom:50%;" /></p>
<p>然后，type-level的attention，聚合邻居下所有相同type的node embedding，然后计算attention weight。这样同一type下的所有neighbor node共享一个type level的weight。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210728180348109.png"   style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210728180319175.png"  style="zoom:50%;" /></p>
<p>不同type之间softmax。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210728180440793.png"   style="zoom:50%;" /></p>
<p>然后是node-level的attention，不同邻居node，计算attention。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210728180538577.png"   style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210728180555435.png"   style="zoom:50%;" /></p>
<p>最后结果：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210728180643981.png"  style="zoom:50%;" /></p>
<h2 id="hetgnn">HetGNN</h2>
<p><a href="https://github.com/chuxuzhang/KDD2019_HetGNN">Heterogeneous Graph Neural Network</a> KDD 2019</p>
<p>作者提出了一种同时处理node content和heterogeneous graph structure的GNN，HetGNN。</p>
<p>看一下整体结构：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210802205726358.png"   style="zoom:50%;" /></p>
<p>核心模块有三方面：</p>
<p><strong>Sampling Heterogeneous Neighbors</strong>：使用了random walk with restart (RWR)的邻居采样策略，需要注意的是这个采样策略保证对于node <span class="math inline">\(v\)</span>，能够采样到所有不同类型的邻居。然后相同类型的邻居聚合到一起。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210802205956510.png"   style="zoom:50%;" /></p>
<p><strong>Encoding Heterogeneous Contents</strong>：对于不同格式的content，使用不同的网络进行处理，然后使用Bi-LSTM进行融合，不同type的node有自己的Bi-LSTM网络。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210802210216977.png"   style="zoom:50%;" /></p>
<p><strong>Aggregating Heterogeneous Neighbors</strong>：对于相同类型的邻居，先基于Bi-LSTM进行聚合。然后不同类型的邻居基于attention进行聚合。</p>
<h2 id="hetsann">HetSANN</h2>
<a href="/gnn/HetSANN/" title="[个人详细博客]">[个人详细博客]</a>
<p><a href="https://github.com/didi/hetsann"><strong>An Attention-based Graph Neural Network for Heterogeneous Structural Learning</strong></a> AAAI 2020 HetSANN</p>
<p>提出了Heterogeneous Graph Structural Attention Neural Network (<a href="/gnn/HetSANN/" title="HetSANN">HetSANN</a>），主要创新点有三个：</p>
<ul>
<li>对于预测标签任务，采用多任务学习，不同type的节点进行预测有不同的classifier（实际是全连接层+softmax）</li>
<li>针对edge和reversed edge，除了一般的基于拼接的方法计算attention外，提出了voice-sharing product的计算注意力方法。</li>
<li>在不同type的邻居信息转换中，提出了一个保持weight matrix的cycle consistent的方法。</li>
</ul>
<p>看一下模型的整体结构：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210803193220627.png"   style="zoom:50%;" /></p>
<p>核心是一个注意力层，TAL层如图所示。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210803194602158.png"   style="zoom:50%;" /></p>
<p>首先是基于type的邻居信息转化，node <span class="math inline">\(i\)</span> 提供给node <span class="math inline">\(j\)</span>。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210803193433545.png"   style="zoom:50%;" /></p>
<p>然后基于注意力聚合邻居信息，下面的是一般的GAT的方法，作者叫做<em>concat product</em>。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210803193632456.png"  style="zoom:50%;" /></p>
<p>需要注意的是，这里的注意力向量<span class="math inline">\(\alpha_r\)</span>，是每个edge type各有一个。然后就是基于softmax的attention聚合。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210803193716164.png"   style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210803193733856.png"   style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210803193750622.png"   style="zoom:50%;" /></p>
<p>实际上，作者还提出了<em>voice-sharing</em>的注意力计算方法，主要是希望考虑关系和逆关系之间的对应联系。让注意力向量<span class="math inline">\(\alpha_r\)</span>​互为负数，然后利用相加计算注意力。详见博客。</p>
<h2 id="rhine">RHINE</h2>
<p><strong>Relation Structure-Aware Heterogeneous Information Network Embedding</strong> AAAI 2019</p>
<p>这篇文章不是GNN领域的文章，但是由于它也尝试捕获relation在结构上的角色，所以干脆放到一起了。</p>
<p>它核心创新点是把所有的relation划分为了两类：</p>
<ul>
<li>Afﬁliation Relations (ARs)：one-centeredby-another structures</li>
<li>Interaction Relations (IRs)：peer-to-peer structures</li>
</ul>
<p>划分的依据是作者根据不同relation的头尾节点类型的平均数量比，对于关系<span class="math inline">\(&lt;u,r,v&gt;\)</span>：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210804154636282.png"   style="zoom:50%;" /></p>
<p>里面的<span class="math inline">\(\overline{d}_{t_u}\)</span>，<span class="math inline">\(t_u\)</span>表示的是头节点<span class="math inline">\(u\)</span>的类型，<span class="math inline">\(\overline{d}_{t_u}\)</span>是指这一类型下的所有节点的平均度degree。在这样的网络中，能够确定某个relation两边的entity type，所以可以这样评估。但是在KG中，无法确定entity的type，也就无法这样计算。</p>
<p><span class="math inline">\(D(r)\)</span>比较小的划分为IR关系，<span class="math inline">\(D(r)\)</span>比较大的划分为AR关系。</p>
<p>这样划分完之后，对于AR关系和IR关系使用两种不同的embedding model。</p>
<p>AR，直接评估两个点之间的欧氏距离。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210804155417686.png"  style="zoom:50%;" /></p>
<p>IR，借助TransE的思想，建模这种1-1的关系。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210804155434262.png"   style="zoom:50%;" /></p>
<h2 id="jk">JK</h2>
<p><strong>Representation Learning on Graphs with Jumping Knowledge Networks</strong> ICML 2018</p>
<p>作者认为一般GCN模型实际假定了为不同的node都学习固定范围/半径的邻居信息，这种情况下不一定是最优解。比如通常GCN只需要两层就达到了最优解，但是对于一个graph来说，有的node可能是tree-like的，两层邻居也只包含了很少的邻居信息，而有的node是expander-like core，两层邻居就包含了非常多的邻居信息。比如下面的GooglePlus社交网络：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210804172507196.png"   style="zoom:50%;" /></p>
<p>因此，作者希望设计一种方法能够实现adaptively adjust (i.e., learn) the inﬂuence radii for each node and task。提出了<em>Jumping Knowledge Networks (JK-Nets)</em>。</p>
<p>主要结构：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210804172641046.png"   style="zoom: 33%;" /></p>
<p>JUMP的意思是每一层输出都jump到最后一层，在最后一层进行layer aggregation。</p>
<p>作者提出三种方法</p>
<ul>
<li>Concatenation</li>
<li>Max-pooling</li>
<li>LSTM-attention：双向LSTM</li>
</ul>
<p>简单的论文的实验结果看，前两个方法还不错，但是后面的LSTM-attention，效果并不好。通过使用前面的JK设计，作者能够在不同数据集下，基于更多更深的GCN层达到最好的结果。</p>
<h2 id="pathcon">PATHCON</h2>
<p><a href="https://github.com/hwwang55/PathCon"><strong>Relational Message Passing for Knowledge Graph Completion</strong></a> KDD 2021</p>
<a href="/kge/PATHCON/" title="[个人详细博客]">[个人详细博客]</a>
<p>在这篇论文中，作者只考虑了KG中的relation embedding，没有学习entity embedding。更具体的说，学习两个方面的结构信息，relational context和relation paths。前者是头/尾实体的邻居relation，后者是头尾实体在KG中相连的relational path。提出了<a href="https://github.com/hwwang55/PathCon">PATHCON</a></p>
<p>作者预测的是relation prediction，<span class="math inline">\(&lt;h,?,t&gt;\)</span>，区别于常见的head/tail prediction，这样情况下relation prediction的候选项是所有的relation，会少很多候选项。这篇文章，作者还提出了一个新的数据集，DDB14，基于医药和疾病的一个知识图谱。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210807105609410.png"   style="zoom:50%;" /></p>
<h2 id="hetegnn">HeteGNN</h2>
<p><strong>HeteGCN: Heterogeneous Graph Convolutional Networks for Text Classification</strong> WSDM 2021</p>
<p>针对文本预测任务，简化TEXTGCN，将原来整个TEXTGCN中使用的graph分解为几个不同的小graph，每个graph有自己的<span class="math inline">\(W_r\)</span>。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210809161131674.png"   style="zoom:50%;" /></p>
<h2 id="kgnn">KGNN</h2>
<p><a href="https://github.com/xzenglab/KGNN"><strong>KGNN: Knowledge Graph Neural Network for Drug-Drug Interaction Prediction</strong></a> IJCAI 2020</p>
<p>针对DDI问题（Drug-drug interaction），首先从数据集中构造一个关于drug的KG，然后使用GNN捕获drug的邻居信息。在GNN上没有太大的创新，发现在聚合的时候使用自身embedding与邻居embedding各自具有不同的weight matrix比较合适。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210809171558677.png"   style="zoom:50%;" /></p>
<h2 id="cprl">CPRL</h2>
<p><strong>Heterogeneous Graph Neural Networks for Concept Prerequisite Relation Learning in Educational Data</strong> NAACL 2021</p>
<p>CPRL（concept prerequisite relation learning），在GNN上没有太大创新，主要是属于应用场景的一个创新。针对概念之间的依赖关系进行预测，作者创建了一个异质图，然后直接使用R-GCN进行学习，方法上没有太多可以借鉴的地方。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210810105105557.png"   style="zoom:50%;" /></p>
<p>这里使用了一个Siamese network，以前没见过。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210810105626109.png"   style="zoom: 33%;" /></p>
<h2 id="clhg">CLHG</h2>
<p><a href="https://github.com/%20TencentGameMate/gnn_cross_lingual"><strong>Cross-lingual Text Classiﬁcation with Heterogeneous Graph Neural Network</strong></a> ACL 2021</p>
<p>CLHG（Cross-Lingual Heterogeneous GCN），针对跨语言的文本分类任务，使用HGCN捕获不同语言的异质信息。这篇文章在GNN上没有太大创新，直接使用了前面HGAT的方法，根据邻居节点类型有不同的weight matrix。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210810151243856.png"   style="zoom:50%;" /></p>
<h2 id="eagcn">EAGCN</h2>
<p><a href="https://github.com/Luckick/EAGCN"><strong>Multi-view spectral graph convolution with consistent edge attention for molecular modeling</strong></a> Neurocomputing</p>
<p>EAGCN，预测任务是molecular graph property prediction，核心创新点个人认为是把异质图根据edge type分为不同view的graph，然后在molecular graph的背景下，同一个type的edge有不同的取值，这些取值会有不同的weight scalar。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210811150803687.png"   style="zoom:50%;" /></p>
<p>另外作者开源了项目，里面有attention的可视化这些操作，如果需要可以参考。</p>
<h2 id="eigat">EIGAT</h2>
<p><strong>Incorporating Global Information in Local Attention for Knowledge Representation Learning</strong> ACL 2021</p>
<p>核心创新点在于建模KG中实体的重要性，为每个实体赋值一个实数scalar，然后根据邻居实体的重要性评估中心实体的重要性。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210824160654200.png"   style="zoom:50%;" /></p>
<p><strong>local attention</strong></p>
<p>与KBGAT的方法一样。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210824160807701.png"  style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210824160745666.png"   style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210824160828759.png"   style="zoom:50%;" /></p>
<p><strong>entity importance</strong></p>
<p>核心创新点，</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210824160908960.png"   style="zoom:50%;" /></p>
<p>从in edge出发，聚合邻居的重要性，注意，这里融合了前面计算的message的重要性。每个邻居提供的重要性是相对于自身所有的out message重要性来计算的。</p>
<p>其中<span class="math inline">\(d\)</span>是一个超参，第一项是为了给KG中没有in-degree的实体一个初始值。</p>
<p>在实验时，所有的<span class="math inline">\(EI\)</span>初始化为0.1，<span class="math inline">\(d\)</span>初始化为0.85。</p>
<p>最后，两种attention进行融合。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210824161639285.png"   style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210824161622524.png"   style="zoom:50%;" /></p>
<h2 id="gaeat">GAEAT</h2>
<p><strong>GAEAT: Graph Auto-Encoder Attention Networks for Knowledge Graph Completion</strong> CIKM 2020</p>
<p>CIKM的short track。实际没有什么创新，使用KBGAT作为编码器，然后DistMult作为解码器。不过可以作为对比的Baseline。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210825164436321.png"   style="zoom:50%;" /></p>
<h2 id="m-gnn">M-GNN</h2>
<p><strong>Robust Embedding with Multi-Level Structures for Link Prediction</strong> IJCAI 2019</p>
<a href="/gnn/M-GNN/" title="[个人详细博客]">[个人详细博客]</a>
<p>这篇文章提出了一种multi-level graph neural network，M-GNN。使用GIN中的MLP学习结构信息，然后提出了一种基于KG中图的不同粒度进行建模的方法。它会从原始的KG出发，不断合并邻居节点，合并边，构造出一系列不同粒度的graph，在这些graph上进行图卷积操作，得到最后的输出。除了一般的链路预测实验，作者还进行了在不同稀疏度以及加入noising edges的实验。</p>
<p>和一般的GNN消息聚合方式不同，M-GNN希望能够建模KG中不同尺度中的信息。</p>
<p>首先构造k个Coarsened graph：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210826143747570.png"   style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210826145202651.png"   style="zoom:50%;" /> 最后模型结构。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210826150505564.png"   style="zoom:50%;" /></p>
<h2 id="rdgcn">RDGCN</h2>
<p><strong>Relation-Aware Entity Alignment for Heterogeneous Knowledge Graphs</strong> IJCAI 2019</p>
<a href="#">Post not found: gnn/RDGCN [个人详细博客]</a>
<p><a href="https://github.com/StephanieWyt/RDGCN"><strong>RDGCN</strong></a> (Relation-aware Dual-Graph Convolutional Network)，预测任务是KG的实体对齐，主要是为了捕获更多的在dual KG中的relation的信息。核心创新点是对于dual KG（即要对齐的两个KG），构造了Dual Relation Graph，捕获relation和relation之间的联系。之后在这个Dual Relation Graph上学习relation的表示，融入到original KG中进行entity的表示学习，最终用于entity之间的对齐。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210827161832004.png"   style="zoom:50%;" /></p>
<h2 id="slice">SLiCE</h2>
<p><a href="https://github.com/pnnl/SLICE"><strong>Self-Supervised Learning of Contextual Embeddings for Link Prediction in Heterogeneous Networks</strong></a> WWW 2021</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210830195131960.png"   style="zoom:50%;" /></p>
<p>作者希望考虑的是，单个节点在特定的subgraph下的表示。对于节点对，利用随机游走寻找两个节点之间的context subgraph。首先，利用一个embedding function，在全图下学习每个node的初始表示，作为global embedding。</p>
<p>然后，有两个阶段，pre-training和Fine-tuning。pre-training预测被mask掉的node，fine-tuning进行link prediction。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210830200013984.png"   style="zoom:50%;" /></p>
<h2 id="m2gnn">M<sup>2</sup>GNN</h2>
<p><strong>Mixed-Curvature Multi-Relational Graph Neural Network for Knowledge Graph Completion</strong> WWW 2021</p>
<p>首个将mixed-curvature geometry与GNN联系起来学习KGE的方法，作者尝试利用不同的空间对KG中的异质性结构进行建模。但是由于对mixed-curvature space和manifold不了解，看不懂论文内容。之后可以找时间仔细补充下基本知识。可参照</p>
<p>John M Lee. 2013. Smooth manifolds. In Introduction to Smooth Manifolds. Springer, 1–31.</p>
<h2 id="lgnn">LGNN</h2>
<p><strong>Node-wise Localization of Graph Neural Networks</strong> IJCAI 2021</p>
<p>作者认为对于整个图学习同样的weight matrix，可能导致模型倾向于建模最常见的pattern，而不是针对不同node的不同的local context进行学习。作者让graph中不同node拥有不同的weight matrix。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210921195057584.png"   style="zoom:50%;" /></p>
<p>具体有两个Node-level localization和Edge-level localization.</p>
<p><strong>Node-level localization</strong></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210921195215512.png"   style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210921195243176.png"   style="zoom:50%;" /></p>
<p>注意，这里没有给不同node都定义新的vector，而是直接从上一层的邻居直接mean聚合，然后进行转换，生成的向量<span class="math inline">\(a_v\)</span>和<span class="math inline">\(b_v\)</span>之后用于生成node <span class="math inline">\(v\)</span>的weight matrix。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210921195358969.png"   style="zoom:50%;" /></p>
<p>注意这里，是把<span class="math inline">\(a_v\)</span>和<span class="math inline">\(b_v\)</span>作为一行，然后复制，最后作用到graph global matrix<span class="math inline">\(W_l\)</span>上。</p>
<p><strong>Edge-level localization</strong></p>
<p>作者对node <span class="math inline">\(v\)</span>的不同邻居edge进一步建模：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210921195557949.png"   style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210921195613526.png"  style="zoom:50%;" /></p>
<p>最后聚合：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210921195641977.png"   style="zoom:50%;" /></p>
<h2 id="srgcn">SRGCN</h2>
<p><strong>SRGCN: Graph-based multi-hop reasoning on knowledge graphs</strong> Neurocomputing 2021</p>
<p>这篇文章在预测<span class="math inline">\(&lt;h, r, t&gt;\)</span>的时候，首先构建<span class="math inline">\(h\)</span>和<span class="math inline">\(t\)</span>之间的graph，然后在这个graph上，逐步使用R-GCN得到对于尾实体的预测embedding，最后使用MLP获得score。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220303170039139.png"   style="zoom:40%;" /></p>
<p>图中的label指的是从头实体出发，遇到的第几阶邻居。一个实体与头实体之间存在多个不同长度的path时，以最长的path作为label。</p>
<p>之后在使用R-GCN进行图卷积时，并不是以头实体为中心不断的聚合邻居。而是将头实体作为一开始，不断聚合到下一阶邻居实体上，直到聚合到具有最大label的实体上。</p>
<h2 id="chen-et-al.">Chen et al.</h2>
<p><strong>Learning graph attention-aware knowledge graph embedding</strong> Neurocomputing 2021</p>
<p>这篇文章核心是提出了一种新的在KG上计算attention的方法，有三个部分：entity attention、relation attention和structure attention。最核心的创新点是计算structural attention。</p>
<p>Entity attention：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220303183431157.png"   style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220303183447999.png"   style="zoom:50%;" /></p>
<p>Relation attention：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220303183517954.png"   style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220303183538437.png"   style="zoom:50%;" /></p>
<p>Structure attention：</p>
<p>使用带重启机制的随机游走方法（Random Walk with Restart，RWR），</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220303183705455.png"   style="zoom:50%;" /></p>
<p>其中，<span class="math inline">\(w_i \in \mathbb{R}^{N\times 1}\)</span>，其中的entry <span class="math inline">\(p\)</span>表示实体<span class="math inline">\(i\)</span>通过随机游走到达实体<span class="math inline">\(p\)</span>的概率，这个概率越大，表示这两个实体在结构上的相关性越大。</p>
<p>然后，由于每个实体<span class="math inline">\(i\)</span>都有一个对应的<span class="math inline">\(w_i\)</span>，计算邻居边<span class="math inline">\(&lt;i,j&gt;\)</span>在结构上的权重，使用了jaccard相似度计算方法，核心思想是某个实体<span class="math inline">\(p\)</span>如果同时出现在实体<span class="math inline">\(i\)</span>和实体<span class="math inline">\(j\)</span>的邻居中，那么如果实体<span class="math inline">\(i\)</span>和实体<span class="math inline">\(j\)</span>的结构相似度越大，实体<span class="math inline">\(p\)</span>在<span class="math inline">\(w_i\)</span>和<span class="math inline">\(w_j\)</span>中的差距应该越小。因此有：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220303184150967.png"   style="zoom:50%;" /></p>
<p>最后是softmax：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220303184212826.png"   style="zoom:50%;" /></p>
<p>整体结构图：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220303184251972.png"   style="zoom:30%;" /></p>
<h2 id="mte">MTE</h2>
<p><strong>Relation-based multi-type aware knowledge graph embedding</strong> Neurocomputing 2021</p>
<p>这篇文章将本体（ontology）考虑到了GNN当中，从而学习KGE。ontology是描述entity的类型的语法树。</p>
<p>作者将ontology树使用bi-directional transformer model获得关于type的embedding。其中的输入是从root到leaf的序列。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220303202610179.png"   style="zoom:40%;" /></p>
<p>获得type的embedding之后，对于某个具体实体<span class="math inline">\(e\)</span>，不同type的比重应该不同。作者认为如果实体<span class="math inline">\(e\)</span>链接的triples中，关系<span class="math inline">\(r\)</span>属于某个type <span class="math inline">\(t\)</span>的数量越多，则比重越大。比如在上图，对于实体<em>Ang_Lee</em>，类型<em>director</em>的比重应该比<em>actor</em>更大，因为属于<span class="math inline">\(director\)</span>的triple数量更多。</p>
<p>实体<span class="math inline">\(e\)</span>的type embedding应该是：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220303203115074.png"   style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220303203130712.png"   style="zoom:50%;" /></p>
<p>上面第二个公式的含义就是统计属于某个type <span class="math inline">\(t\)</span>的triples的数量占比。</p>
<p>之后，作者提出一种基于relation的attention聚合方法。</p>
<p>单个relation下的实体聚合：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220303203413691.png"   style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220303203428588.png"   style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220303203449437.png"   style="zoom:50%;" /></p>
<p>多个relation的聚合：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220303203516612.png"   style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220303203532350.png"   style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220303203547104.png"   style="zoom:50%;" /></p>
<h2 id="revgnn">RevGNN</h2>
<p><strong>Training Graph Neural Networks with 1000 Layers</strong> ICML 2021</p>
<a href="/gnn/GNN-1000-layers/" title="[个人详细博客]">[个人详细博客]</a>
<p>这篇文章通过在GNN中引入grouped reversible connections，实现了将GNN拓展到1000层，可能是当前最深的GNN之一。这篇文章的意义在于，实现了GNN的层数与模型所需的显存无关，使用较少的显存就可以在显存基本不增加的情况下，任意增加GNN深度。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220323192539787.png"   style="zoom:50%;" /></p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>GNN</category>
      </categories>
      <tags>
        <tag>Collection</tag>
      </tags>
  </entry>
  <entry>
    <title>KGE-Collection</title>
    <url>/collection/KGE-Collection/</url>
    <content><![CDATA[<h1 id="collection-of-kge-papers-volume-1">Collection of KGE Papers Volume 1</h1>
<p>Collection of KGE papers, volume 1.</p>
<p>Now it contains models of:</p>
<ul>
<li>HoLE (AAAI 2016)</li>
<li>TACT(AAAI 2021)</li>
<li>TransF(ICPR 2018)</li>
<li>TransCoRe(JCST 2018)</li>
<li>Interpreting-KGE(ICLR 2021)</li>
<li>TuckER(EMNLP 2019)</li>
<li>MLP for KGE(KDD 2014)</li>
<li>TransH(AAAI 2014)</li>
<li>TransA(arXiv 2015)</li>
<li>TransR(AAAI 2015)</li>
<li>TransD(IJNLP 2015)</li>
<li>TransG(ACL 2016)</li>
<li>CP for KGE(ICML 2018)</li>
<li>SimplE(NIPS 2018)</li>
<li>Complex(ICML 2016)</li>
<li>REInceptionE(AAAI 2020)</li>
<li>R-MeN(ACL 2020)</li>
<li>HypER(ICANN 2019)</li>
<li>RSN(ICML 2019)</li>
<li>NSCaching(ICDE 2019)</li>
</ul>
<span id="more"></span>
<h2 id="hole">HoLE</h2>
<p><strong>Holographic Embeddings of Knowledge Graphs</strong> AAAI 2016</p>
<p>这篇文章提出了holographic embeddings (HOLE)，来学习KG的compositional vector space representations。</p>
<p><strong>motivation</strong>：However, existing embedding models that can capture rich interactions in relational data are often limited in their scalability. Vice versa, models that can be computed efficiently are often considerably less expressive.</p>
<p><strong>methods</strong>：直接从subject entity embedding和object entity embedding中，使用circular correlation获得新的embedding，称作holograph embedding，然后使用这个holograph embedding与relation embedding做点积，得到预测概率。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210418184909978.png" style="zoom:50%;" /></p>
<p>一个图示：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210511102258964.png" style="zoom:50%;" /></p>
<p>从这个图能够看出来，Circular Correlation可以看做是tensor dot的一种压缩方式，它的输出结果的每一维都是tensor dot结果的一部分。</p>
<h2 id="tact">TACT</h2>
<p><strong>Topology-Aware Correlations Between Relations for Inductive Link Prediction in Knowledge Graphs</strong> AAAI 2021</p>
<a href="/kge/TACT/" title="[个人详细博客]">[个人详细博客]</a>
<p><a href="https://github.com/MIRALab-USTC/KG-TACT">TACT</a>，作者主要考虑的是inductive link prediction，使用gnn，捕获relation之间的语义上的关联性，即semantic correlation。作者认为relation之间的关联性通过relation的拓扑结构得到体现，因此，作者将所有的relation之间相连的拓扑结构分为7种，在relation形成的graph中进行学习，提出了RCN。</p>
<p>然后看一下整体结构：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210511102445139.png" style="zoom:50%;" /></p>
<h2 id="transf">TransF</h2>
<p><strong>Knowledge Graph Embedding with Multiple Relation Projections</strong> ICPR 2018</p>
<p>基于翻译的方法，在TransR的思想上的改进。考虑了每个relation不是独立的，而是具有Correlation，比如关系<em>/people/person/place_of_birth</em>和<em>/people/person/nationality</em>就有较强的相关性，比如居住在纽约的人大概率是美国人。为了解决这个问题，作者直接将每个relation独立的matrix分为一系列的basis space的组合，对于不同relation有不同的组合系数。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210511102512774.png" style="zoom:50%;" /></p>
<p>公式：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210418181121701.png" style="zoom:50%;" /></p>
<p>在实验中，在FB15k-237数据集上，作者使用了维度100，s数量5；在WN18RR数据集上，维度50，s数量5。最后使用TransR的方法投影：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210511110706718.png" style="zoom:50%;" /></p>
<h2 id="transcore">TransCoRe</h2>
<p><strong>Modeling the Correlations of Relations for Knowledge Graph Embedding</strong> JCST</p>
<p>作者考虑了关系之间的correlation，首先利用SVD和PCC方法分析了TransE这些方法学习到的relation embedding之间的相关性，然后发现在所有relation组成的matrix中，存在low-rank的structure。因此，作者直接将relation matrix拆分为两个矩阵的乘积，一个是通用矩阵，一个是关系矩阵，每一列对应不同的relation。 <span class="math display">\[
\mathbf{R}=\mathbf{U}\mathbf{V}
\]</span> <img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210511110904912.png" style="zoom:50%;" /></p>
<p>在这种情况下，矩阵<span class="math inline">\(\mathbf{U}\)</span>的列是关系空间的basis</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210511110958068.png" style="zoom:50%;" /></p>
<p>最后</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210518225522673.png" style="zoom:50%;" /></p>
<h2 id="interpreting-kge">Interpreting-KGE</h2>
<p><strong>INTERPRETING KNOWLEDGE GRAPH RELATION REPRESENTATION FROM WORD EMBEDDINGS</strong> ICLR 2021</p>
<p>从最新的基于PMI（pointwise mutual information (PMI)）的对于word embedding的理解角度出发，尝试从relation描述的subject和object entity的语义关联性角度进行解释；将所有的relation分为3类，并且解释了为了捕获它们的特征需要如何学习。</p>
<p><strong>很多地方没看懂。</strong></p>
<p>三个不同的分类：</p>
<ul>
<li>highly related (R);</li>
<li>generalised specialisation (S);</li>
<li>and generalised context-shift (C).</li>
</ul>
<p>三者是包含关系，C&gt;S&gt;R</p>
<p>在WN18RR下不同关系的分类，</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210510170730416.png" style="zoom:50%;" /></p>
<p>FB15k-237的relation大多是type C，也就是说该数据集中的relation没有特别明显的结构联系。</p>
<h2 id="tucker">TuckER</h2>
<p><strong>TuckER: Tensor Factorization for Knowledge Graph Completion</strong> EMNLP 2019</p>
<p>这篇文章使用1996年就被提出来的分解方法 Tucker decomposition，提出了TuckER，TuckER的主要结构如下：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210520160757408.png" style="zoom:50%;" /></p>
<p>其中的参数<span class="math inline">\(W\)</span>是所有关系共享的，<span class="math inline">\(e_s,w_r,e_o\)</span>是subject, relation和object entity的embedding，都是向量化的表示。</p>
<p>具体计算公式是，沿着不同维度（mode）进行乘法运算</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/v2-930f8a5f6ef0dd9db35e1e6f5a7f112f_1440w.png" style="zoom:50%;" /></p>
<p>这篇文章可以考虑用来简化R-GCN中的<span class="math inline">\(W_r\)</span>。</p>
<h2 id="mlp-for-kge">MLP for KGE</h2>
<p><strong>Knowledge Vault: A Web-Scale Approach to Probabilistic Knowledge Fusion</strong> KDD 2014</p>
<p>这篇文章介绍了Knowledge Vault</p>
<blockquote>
<p>a Web-scale probabilistic knowledge base that combines extractions from Web content (obtained via analysis of text, tabular data, page structure, and human annotations)</p>
</blockquote>
<p>在文章中，使用了MLP来获得KGE，主要用于评估构造的KG中的edge存在的概率，主要方法：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210520193708498.png" style="zoom:50%;" /></p>
<p>其中的<span class="math inline">\(\beta \in \mathbb{R}^{L\times 1}\)</span>，<span class="math inline">\(u_s, w_p, v_o\)</span>是subject、relation和object。</p>
<h2 id="transh">TransH</h2>
<p><strong>Knowledge Graph Embedding by Translating on Hyperplanes</strong> AAAI 2014</p>
<p>这篇文章的贡献有两点</p>
<ul>
<li>在TransE的基础上，提出了TransH，将subject和object entity投影到不同relation的超平面上，该超平面由一个法向量<span class="math inline">\(w_r\)</span>决定，超平面上有一个偏移向量<span class="math inline">\(d_r\)</span>，用于算头实体和尾实体投影之间的偏移。</li>
<li>使用了一个简单基于统计的，能够减小负采样错误率的方法。原理是one-to-many的relation应该倾向于替换head entity；many-to-one的relation倾向于替换tail entity。直接统计number of tail entities per head entity和number of head entities per tail entity，然后使用二元分布，计算替换head或者tail entity的概率。这种方法区别于以前的uniform的采样，可以叫做Bernoulli采样。</li>
</ul>
<p>TransH的结构：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210520220253182.png" style="zoom:50%;" /></p>
<p>数学公式：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210524162955001.png" style="zoom:50%;" /></p>
<p>在训练中，保证<span class="math inline">\(||w_r||_2=1\)</span>，同时<span class="math inline">\(w_r \perp d_r\)</span>。</p>
<h2 id="transa">TransA</h2>
<p><strong>TransA: An Adaptive Approach for Knowledge Graph Embedding</strong> arxiv 2015</p>
<p>这篇文章提出了一个新的计算loss的方法，将计算欧氏距离，换为计算马氏距离。</p>
<p>作者认为对于以前的loss形式过于简单</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210524163344113.png" style="zoom:50%;" /></p>
<p>缺点两个：</p>
<ul>
<li>这个loss metric实际是在计算一个球面等位超面spherical equipotential hyper-surfaces，这种方式过于简单，不够灵活，泛化</li>
</ul>
<blockquote>
<p>Firstly, due to the inflexibility of loss metric, current translation-based methods apply spherical equipotential hyper-surfaces with different plausibilities, where more near to the centre, more plausible the triple is.</p>
</blockquote>
<ul>
<li>它实际是假设embedding的不同entry的weight在计算最终loss的时候一样</li>
</ul>
<blockquote>
<p>Secondly, because of the oversimplified loss metric, current translation-based methods treat each dimension identically.</p>
</blockquote>
<p>作者提出的新指标，将计算欧氏距离，换为计算马氏距离：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210520220226480.png" style="zoom:50%;" /></p>
<h2 id="transr">TransR</h2>
<p><strong>Learning Entity and Relation Embeddings for Knowledge Graph Completion</strong> AAAI 2015</p>
<p>这篇文章改进了TransE和TransH认为embedding都在相同的semantic space中。TransR认为不同关系具有不同的space，实体在entity space下，提出了TransR。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210520223201288.png" style="zoom:50%;" /></p>
<p>数学公式：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210520223130667.png" style="zoom:50%;" /></p>
<p>能够看到，还是有单独的relation embedding。</p>
<p>作者还提出了一个改进版，CTransR（Cluster-based TransR）。它对于一个单独的relation r，将不同的(head, tail)对分成几个不同的clusters，不同的clusters拥有自己的relation vector <span class="math inline">\(r_c\)</span>，整个relation下的所有clusters有一个共同的relation vector <span class="math inline">\(r\)</span>，此时的scoring function为：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210524193736657.png" style="zoom:50%;" /></p>
<h2 id="transd">TransD</h2>
<p><strong>Knowledge Graph Embedding via Dynamic Mapping Matrix </strong> IJCNLP 2015</p>
<p>作者将所有的entities和relations都赋予了两个vectors，一个vectors和以前的embedding一样，作为实体和关系的向量化embedding；一个vectors用来构件转换矩阵：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210520223607761.png" style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210524193757495.png" style="zoom:50%;" /></p>
<h2 id="transg">TransG</h2>
<p><strong>TransG : A Generative Model for Knowledge Graph Embedding</strong> ACL 2016</p>
<p>这是第一篇在KGE上使用generative model的论文，第一次明确提出并且讨论<strong>multiple relation semantics</strong>，即一个relation可能有不同的语义含义，这个语义含义是由它链接的头/尾实体决定的。</p>
<blockquote>
<p>In spite of the success of these models, none of the previous models has formally discussed the issue of multiple relation semantics that a relation may have multiple meanings revealed by the entity pairs associated with the corresponding triples.</p>
</blockquote>
<p>为了让relation有更多不同的表示，作者使用Bayesian non-parametric inﬁnite mixture embedding model [1]的思路，为每个relation都学习多个不同的component。</p>
<p>[1] The indian buffet process: An introduction and review.</p>
<p>对生成模型不了解，这篇文章没有细读。</p>
<h2 id="cp-for-kge">CP for KGE</h2>
<p><strong>Canonical tensor decomposition for knowledge base completion</strong> ICML 2018</p>
<p><a href="https://github.com/facebookresearch/kbc" class="uri">https://github.com/facebookresearch/kbc</a></p>
<p>作者集中在使用canonical decomposition of tensors (also called CANDECOMP/PARAFAC or CP)来学习KGE。对于CP的改进集中在两方面：</p>
<ul>
<li>引入关系的逆关系，分别具有不同的表示，显示使用这种办法能够很好的提升效果</li>
<li>使用tensor nuclear p-norms正则化CP，虽然最后结果显示没有很显著的提升CP效果</li>
</ul>
<p>最终效果没有超越ComplEX，但是提升CP效果很多。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210526215137325.png" style="zoom:50%;" /></p>
<p>CP分解：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210530213931363.png" style="zoom:50%;" /></p>
<p>其中的<span class="math inline">\(\mathbf{X}\)</span>是整个KG张量。</p>
<p>后续调研张量分解的方法可以从这篇文章出发。</p>
<h2 id="simple">SimplE</h2>
<p><strong>SimplE Embedding for Link Prediction in Knowledge Graphs</strong> NIPS 2018</p>
<p><a href="https://github.com/Mehran-k/SimplE">SimplE</a> 使用CP分解，改进了一般的CP分解，与一般的CP分解一样，每个entity有两种表示对于head和tail，每个relation有唯一的表示。在预测triple是否成立时，同时用原关系和逆关系是否成立进行平均打分。在论文中，作者证明了SimplE是fully expressivene的。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210530214019324.png" style="zoom:50%;" /></p>
<p>其中的<span class="math inline">\(&lt;&gt;\)</span>函数是向量内积，定义为：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210526215101906.png" style="zoom:67%;" /></p>
<h2 id="complex">Complex</h2>
<p><strong>Complex Embeddings for Simple Link Prediction</strong> ICML 2016</p>
<p><a href="https://github.com/ttrouill/complex">Complex</a>应该是首个将KGE中的embedding从实数域扩展到复数域的方法，它的思想还是基于矩阵分解的思路，但是在RESCAL和DistMult这些方法的基础上，通过引入复数域，能够建模关系的对称/不对称关系，同时还能保证参数的有效性。因为在复数域的向量内积就变为了Hermitian (or sesquilinear) dot product，拥有了一个共轭的转置表示conjugate-transpose</p>
<p>具体公式：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210626220840153.png" style="zoom:50%;" /></p>
<p>其中的，<span class="math inline">\(Re\)</span>是实数部分，<span class="math inline">\(Im\)</span>是虚数部分。</p>
<p>之后，预测概率</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210626221427316.png" style="zoom:50%;" /></p>
<h2 id="reinceptione">REInceptionE</h2>
<p><a href="https://github.com/JuneTse/ReInceptionE."><strong>ReInceptionE: Relation-Aware Inception Network with Joint Local-Global Structural Information for Knowledge Graph Embedding</strong></a> AAAI 2020</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210707181438099.png" style="zoom:40%;" /></p>
<p>使用Inception网络学习对所有的<span class="math inline">\((h,r,?)\)</span>都可以编码为一个embedding。</p>
<p>如果要查询<span class="math inline">\((h,r,?)\)</span>，<span class="math inline">\(h,r\)</span>可以通过inception网络变为为一个查询embedding。</p>
<p>对于头实体<span class="math inline">\(h\)</span>的邻居，都可以通过inception网络变为邻居embedding，之后使用查询embedding去计算邻居embedding注意力，然后进行聚合。</p>
<p>对于要查询的关系<span class="math inline">\(r\)</span>，对于图中所有属于<span class="math inline">\(r\)</span>的头尾实体应该可以提供某种特定的特征，因此，利用查询embedding，计算所有属于<span class="math inline">\(r\)</span>的头实体的注意力然后聚合；同样，聚合特定的尾实体的信息。</p>
<p>最后，融合三方面的信息，通过一个MLP，进行预测。</p>
<h2 id="r-men">R-MeN</h2>
<p><a href="https://github.com/daiquocnguyen/%20R-MeN"><strong>A Relational Memory-based Embedding Model for Triple Classiﬁcation and Search Personalization</strong></a> R-MeN ACL 2020</p>
<p>为了能够记忆KG中的三元组之间可能存在的潜在依赖，提出了R-MeN方法，模型图</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210709221208885.png" style="zoom:50%;" /></p>
<p>在一个三元组<span class="math inline">\((s,r,o)\)</span>中，首先编码为三个不同的embedding</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210709221647609.png" style="zoom:50%;" /></p>
<p>模仿Transformer加入了位置embedding。然后，设计了一个记忆力单元<span class="math inline">\(M\)</span>，设计中<span class="math inline">\(M\)</span>应该拥有<span class="math inline">\(N\)</span>个memory slot，每一行是一个记忆力插槽。但是在实现的时候发现只有一个记忆力插槽的时候效果最好。</p>
<p>之后，依次输入<span class="math inline">\(x_t\)</span>，使用transformer的注意力机制聚合<span class="math inline">\(x_t\)</span>和<span class="math inline">\(M\)</span>。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210709222104512.png" style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210709222043286.png" style="zoom:50%;" /></p>
<p>在这个之后，加入了MLP，残差，gate。实际从结果来看，没有特别大的提升。</p>
<h2 id="hyper">HypER</h2>
<p><a href="https://github.com/ibalazevic/HypER"><strong>Hypernetwork Knowledge Graph Embeddings</strong></a> ICANN 2019</p>
<p>这篇文章在ConvE的基础上改进，提出了HypER，使用relation embedding通过一个hypernetwork（Hypernetworks. In: International Conference on Learning Representations.）为每个关系都产生一个1D卷积核。和ConvE有的区别是不使用2D的卷积，不需要reshape entity embedding和relation embedding。作者另外证明了这种1D卷积的方法最终可以归类到tensor factorization中。</p>
<blockquote>
<p>A hypernetwork is an approach by which one network generates weights for another network, that can be used to enable weight-sharing across layers and to dynamically synthesize weights given an input.</p>
</blockquote>
<p>结构图：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210719154018301.png" style="zoom:50%;" /></p>
<p>核心公式：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210719154108887.png" style="zoom:50%;" /></p>
<p>其中的<span class="math inline">\(vec^{-1}\)</span>是重新将向量转化为矩阵形式。<span class="math inline">\(w_r\)</span>就是relation embedding，它的实际维度与entity embedding维度一致。</p>
<p>从tensor operation的角度看HypER。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210719154328117.png" style="zoom:50%;" /></p>
<h2 id="rsn">RSN</h2>
<p><a href="https://github.com/nju-websoft/RSN"><strong>Learning to Exploit Long-term Relational Dependencies in Knowledge Graphs</strong></a> ICML 2019</p>
<p>使用带残差的RNN的方法建模KG的relational path，预测实体对齐和链路预测两个任务。</p>
<p>核心的模型结构，首先提出了一种Biased random walk sampling，偏好采用更深的实体路径，输出relational path。</p>
<p>然后使用RSN(Recurrent skipping network)建模这个path，核心思想在于强调relational path中triple的重要性。将subject entity的hidden state作为残差输出到object entity的上一步hidden state中。一个relational path元素个数为奇数，头尾都是实体，<span class="math inline">\((x_1,\dots,x_{odd})\)</span>。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210719200737571.png"   style="zoom:50%;" /></p>
<p>然后，对实体和关系做区别对待：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210719200912943.png"   style="zoom:50%;" /></p>
<p>模型结构：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210719201218600.png"   style="zoom:50%;" /></p>
<p>使用Type-based noise contrastive estimation(NCE)进行优化，对这个优化方法没有很了解。</p>
<h2 id="nscaching">NSCaching</h2>
<p><a href="https://github.com/yzhangee/NSCaching"><strong>NSCaching: Simple and Efﬁcient Negative Sampling for Knowledge Graph Embedding</strong></a> ICDE 2019</p>
<a href="/kge/NSCaching/" title="[个人详细博客]">[个人详细博客]</a>
<p>提出了一种针对KGE的动态负采样方法<a href="https://github.com/yzhangee/NSCaching">NSCaching</a>，核心思想是得分高的负样本很重要但是数量少，因此，作者直接使用cache来保存得分高的负样本，同时随着训练动态更新cache，可以看做是基于GAN的负采样方法的distilled版本。</p>
<p>在训练KGE的时候，负样本的质量很重要，也就是说那些越难与正样本区分的负样本可能越重要。<em>high-quality negative triplets should have large scores</em>，因为基于embedding的model实际上对于大多数负样本不敏感，给出的都是比较低的打分。如果使用random采样，采样得到的负样本，激活函数如果是sigmoid函数，那么如果负样本得分在&lt;&lt;0的区间内，那么梯度会很小，造成梯度消失的问题。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210722173128966.png" style="zoom:40%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210722173851214.png"   style="zoom:40%;" /></p>
<h2 id="structure">StructurE</h2>
<p><strong>Structural context-based knowledge graph embedding for link prediction</strong> Neurocomputing 2022</p>
<p>这篇文章是基于trans的KGE方法，它对于两个预测任务<span class="math inline">\(&lt;h, t, ?&gt;\)</span>和<span class="math inline">\(&lt;?, r, t&gt;\)</span>分别设计了不同的score function。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220303152549137.png"   style="zoom:50%;" /></p>
<p>核心是两个公式：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220303152702651.png"   style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220303152749096.png"   style="zoom:50%;" /></p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>KGE</category>
      </categories>
      <tags>
        <tag>Collection</tag>
      </tags>
  </entry>
  <entry>
    <title>TemporalKG-survey-list</title>
    <url>/collection/TemporalKG-survey-list/</url>
    <content><![CDATA[<h1 id="temporal-knowledge-graph">Temporal Knowledge Graph</h1>
<p>对目前出现的temporal KG相关论文和资源的调研文献list。</p>
<span id="more"></span>
<table>
<colgroup>
<col style="width: 17%" />
<col style="width: 14%" />
<col style="width: 17%" />
<col style="width: 14%" />
<col style="width: 17%" />
<col style="width: 17%" />
</colgroup>
<thead>
<tr class="header">
<th>标题</th>
<th>任务</th>
<th>解决问题</th>
<th>主要技术</th>
<th>数据集</th>
<th>参考价值</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>RotateQVS: Representing Temporal Information as Rotations in Quaternion Vector Space for Temporal Knowledge Graph Completion. (ACL 22)</td>
<td>知识图谱补全</td>
<td>认为之前的方法无法很好的建模时空relation以及relation之间的联系</td>
<td>Trans系列</td>
<td>ICEWS14、ICEWS05-15、YAGO11k、 GDELT</td>
<td>参考如何在复杂的向量空间下建模时空信息</td>
</tr>
<tr class="even">
<td>Improving Time Sensitivity for Question Answering over Temporal Knowledge Graphs (ACL 22)</td>
<td>QA</td>
<td>针对一个问题可能涉及多个不同时间片进行了探究</td>
<td>通过引入一个time-order学习的任务改进了TCompLEx方法</td>
<td>CRONQUESTIONS （目前出现的最大的利用TKG的QA数据集）</td>
<td>可以参考改进的学习任务</td>
</tr>
<tr class="odd">
<td>Search from History and Reason for Future: Two-stage Reasoning on Temporal Knowledge Graphs (ACL 21)</td>
<td>知识图谱补全</td>
<td>作者任务在TKG下的推理，需要两个步骤，首先是线索搜集，然后是具体的推理。大多数的方法忽略了第一步</td>
<td>RL和GNN</td>
<td>ICE14、ICE05-15、ICE18、 GDELT</td>
<td>可以参考第二步，利用GNN建模时空信息</td>
</tr>
<tr class="even">
<td>TTAGN: Temporal Transaction Aggregation Graph Network for Ethereum Phishing Scams Detection (WWW 22)</td>
<td>Ethereum Phishing Scams Detection</td>
<td></td>
<td>GCN</td>
<td>作者从Etherscan上爬取的数据集</td>
<td>搞清楚GCN在时序信息建模中的作用</td>
</tr>
<tr class="odd">
<td>Time-aware Entity Alignment using Temporal Relational Attention. （WWW 22）</td>
<td>Entity Alignment</td>
<td>之前没有探究过在TKG和Open KG之间的实体对齐</td>
<td>GCN</td>
<td></td>
<td>GCN用于表示学习，在实体对齐任务下进行了评估</td>
</tr>
<tr class="even">
<td>TREND: TempoRal Event and Node Dynamics for Graph Representation Learning （WWW 22）</td>
<td>Temporal Graph Representation Learning</td>
<td>之前的方法没有探究过如何处理新出现的节点</td>
<td>GCN</td>
<td>CollegeMsg、cit-HepTh、Wikipedia、Taobao</td>
<td>GCN建模时序信息</td>
</tr>
<tr class="odd">
<td>Element-guided Temporal Graph Representation Learning for Temporal Sets Prediction （WWW 22）</td>
<td>Temporal Sets Prediction</td>
<td>之前的方法无法建模不同set之间的协同信息</td>
<td>GCN</td>
<td>DC、TaoBao、JingDong、TMS</td>
<td>GCN建模时序信息</td>
</tr>
<tr class="even">
<td>STAM: A Spatiotemporal Aggregation Method for Graph Neural Network-based Recommendation （WWW 22）</td>
<td>Recommendation</td>
<td>之前利用GNN进行推荐的方法没有考虑时序的信息</td>
<td>GNN</td>
<td>MovieLens、Amazon、Taobao</td>
<td>GCN获取的邻居信息加入了时序信息</td>
</tr>
<tr class="odd">
<td>HINTS: Citation Time Series Prediction for New Publications via Dynamic Heterogeneous Information Network Embedding （WWW 21）</td>
<td>作者提出了一个新的问题，预测一篇论文的引用时间序列</td>
<td>之前的方法大多是根据已有的leading citation value来进行预测，无法处理冷启动问题</td>
<td>GNN</td>
<td>AMiner、APS</td>
<td>HGNN结合时序信息，并且尝试解决冷启动问题</td>
</tr>
<tr class="even">
<td>TLogic: Temporal Logical Rules for Explainable Link Forecasting on Temporal Knowledge Graphs (AAAI 22)</td>
<td>Link Forecasting</td>
<td>作者认为之前对于时序知识图谱的建模缺少可解释性，也缺少对于推理逻辑链的研究</td>
<td>随机游走</td>
<td>ICEWS14、ICEWS18、ICEWS0515</td>
<td>一个可解释的建模方法</td>
</tr>
<tr class="odd">
<td>Temporal Knowledge Graph Completion using Box Embeddings （AAAI 22）</td>
<td>知识图谱补全</td>
<td>作者认为之前的方法都没有从时空推理的角度进行研究</td>
<td>在BoxE方法基础上进行了拓展</td>
<td>ICEWS14、 ICEWS05-15、 GDELT</td>
<td>作者对时空知识图谱的各种性质进行了验证</td>
</tr>
<tr class="even">
<td>Neural Latent Space Model for Dynamic Networks and Temporal Knowledge Graphs （AAAI 21）</td>
<td>Link Forecasting</td>
<td>作者这项工作能够同时处理同构网络和异构网络</td>
<td>GRU</td>
<td>UCI、Enron、Yelp、ML-10M、WIKI、YAGO</td>
<td>作者从概率分布的角度分析出发，逐步建立一个使用神经网络的模型</td>
</tr>
<tr class="odd">
<td>Dynamic Knowledge Graph Alignment （AAAI 21）</td>
<td>Knowledge Graph Alignment</td>
<td>之前在知识图谱对齐中的方法通常假设KG是静态的</td>
<td>GCN</td>
<td>DBP15K</td>
<td>GCN用于时序知识图谱对齐</td>
</tr>
<tr class="even">
<td>Learning from History: Modeling Temporal Knowledge Graphs with Sequential Copy-Generation Networks （AAAI 21）</td>
<td>知识图谱补全</td>
<td>之前对于时序知识图谱建模的方法忽略了在过去的fact当中常常存在重复性的知识</td>
<td>copy-generation</td>
<td>ICEWS18、ICEWS14、GDELT、WIKI、YAGO</td>
<td></td>
</tr>
<tr class="odd">
<td>ChronoR: Rotation Based Temporal Knowledge Graph Embedding （AAAI 21）</td>
<td>知识图谱补全</td>
<td></td>
<td>基于向量空间旋转操作的方法</td>
<td>ICEWS14、ICEWS05-15、YAGO15K</td>
<td></td>
</tr>
<tr class="even">
<td>Relational Learning to Capture the Dynamics and Sparsity of Knowledge Graphs （AAAI 21）</td>
<td>知识图谱补全</td>
<td>作者希望能够同时解决时序问题和数据稀疏的问题</td>
<td></td>
<td>-</td>
<td></td>
</tr>
<tr class="odd">
<td>Diachronic Embedding for Temporal Knowledge Graph Completion （AAAI 2020）</td>
<td>知识图谱补全</td>
<td>作者认为之前的方法大多会给TKG上的实体都赋予一个静态的表示，但是实体的特征会随着时间而改变。因此，作者提出将实体和对应的时间点作为特征，得到随时间改变的向量</td>
<td></td>
<td>ICEWS14、ICEWS05-15、GDELT</td>
<td>一个model agnostic的方法</td>
</tr>
<tr class="even">
<td>Learning to Walk across Time for Interpretable Temporal Knowledge Graph Completion (KDD 21)</td>
<td>Temporal Knowledge Graph Completion</td>
<td>作者认为之前出现的很多建模TKG的方法是将静态的KGE方法进行拓展，这导致它们往往无法：1. 考虑相关的邻居信息 2. 进行基于路径的解释</td>
<td>GNN</td>
<td>ICEWS14、ICEWS05-15、Wikidata11k</td>
<td>GNN建模时空知识图谱</td>
</tr>
<tr class="odd">
<td>Temporal Knowledge Graph Reasoning Based on Evolutional Representation Learning （SIGIR 21）</td>
<td>知识图谱补全</td>
<td>作者尝试对之前利用evolutional learning的方法进行改进，作者认为之前的方法无法处理出现在同一时间片下的facts</td>
<td>GNN</td>
<td>ICEWS18、ICEWS14、ICEWS05-15、WIKI、YAGO、GDELT</td>
<td>GNN建模时空知识图谱</td>
</tr>
<tr class="even">
<td>TIE: A Framework for Embedding-based Incremental Temporal Knowledge Graph Completion （SIGIR 21）</td>
<td>知识图谱补全</td>
<td>作者认为之前的方法存在以下问题：1. 信息的遗忘 2. 对于改变的fact的不稳定性 3. 训练效率低</td>
<td>Incremental learning</td>
<td>YAGO11k、Wikidata12k</td>
<td>增量学习，之前没有接触过</td>
</tr>
<tr class="odd">
<td>Time-dependent Entity Embedding is not All You Need: A Re-evaluation of Temporal Knowledge Graph Completion Models under a Unified Framework （EMNLP 21）</td>
<td>知识图谱补全</td>
<td>作者对于六种现存的TKG建模的方法的实现策略进行了全面的评估。作者发现通过实验，不同的方法在不同数据集上的性能并不和论文中声称的一致</td>
<td>-</td>
<td>ICEWS14、 ICEWS11-14、 GDELT-m10</td>
<td>对目前的KGE方法可能存在的问题进行了解，便于后续的学习</td>
</tr>
<tr class="even">
<td>TimeTraveler: Reinforcement Learning for Temporal Knowledge Graph Forecasting （EMNLP 21）</td>
<td>Knowledge Graph Forecasting</td>
<td>作者提出了首个利用强化学习实现知识图谱forecasting的方法</td>
<td>强化学习</td>
<td>ICEWS14、ICEWS18、WIKI、YAGO</td>
<td>强化学习用于时序知识图谱建模</td>
</tr>
<tr class="odd">
<td>Learning Neural Ordinary Equations for Forecasting Future Links on Temporal Knowledge Graphs （EMNLP 21）</td>
<td>Knowledge Graph Forecasting</td>
<td>作者认为之前的方法大多将TKG建模到离散的状态空间下。作者提出将GCN和神经微分方程（neural ordinary differential equations）结合。</td>
<td>GCN</td>
<td>ICEWS14、ICEWS18、 ICEWS05-15、YAGO、WIKI</td>
<td>GCN的拓展版本，用于建模时序知识图谱</td>
</tr>
<tr class="even">
<td>Time-aware Graph Neural Network for Entity Alignment between Temporal Knowledge Graphs （EMNLP 21）</td>
<td>TKG对齐</td>
<td>作者认为之前的知识图谱对齐方法忽略了很多知识图谱中存在的时序信息</td>
<td>GNN</td>
<td>DICEWS-1K、DICEWS-200、YAGO-WIKI50K-5K、YAGO-WIKI50K-1K、YAGO-WIKI20K</td>
<td>GCN用于TKG对齐</td>
</tr>
<tr class="odd">
<td>TeMP: Temporal Message Passing for Temporal Knowledge Graph Completion (EMNLP 20)</td>
<td>知识图谱补全</td>
<td>作者认为之前的方法没有能够显式地利用多跳的邻居信息</td>
<td>GNN</td>
<td>ICEWS、ICEWS05-15、GDELT</td>
<td>GNN建模时空知识图谱</td>
</tr>
<tr class="even">
<td>Recurrent Event Network: Autoregressive Structure Inference over Temporal Knowledge Graphs （EMNLP 20）</td>
<td>知识图谱补全</td>
<td>作者认为之前的方法只能够预测发生在过去的fact，而无法预测要连续发生在未来的fact（类似外推）</td>
<td>GNN</td>
<td>ICEWS18、GDELT、 ICEWS14、WIKI、 YAGO</td>
<td>GNN建模时空知识图谱，并且尝试进行外推</td>
</tr>
<tr class="odd">
<td>Temporal Knowledge Base Completion: New Algorithms and Evaluation Protocols (EMNLP 20)</td>
<td>知识图谱补全</td>
<td>作者提出了一个建模时序知识图谱的新方法 TIMEPLEX。更重要的是，作者发现了在之前的评估策略中可能出现的不正确的评估方式</td>
<td>复数域下的建模方法</td>
<td>WIKIDATA12k、YAGO11k、ICEWS14、ICEWS05-15</td>
<td>评估策略的影响很关键</td>
</tr>
<tr class="even">
<td>Domain Knowledge Empowered Structured Neural Net for End-to-End Event Temporal Relation Extraction （EMNLP 20）</td>
<td>关系抽取</td>
<td>作者在神经网络中加入了分布约束来避免之前的方法常估计的硬约束条件</td>
<td>Bert-based</td>
<td>TimeBank-Dense、I2B2-TEMPORAL</td>
<td></td>
</tr>
<tr class="odd">
<td>DyERNIE: Dynamic Evolution of Riemannian Manifold Embeddings for Temporal Knowledge Graph Completion (EMNLP 20)</td>
<td>知识图谱补全</td>
<td>作者认为之前的TKG建模方法大多是在欧式空间下的建模方法，而作者尝试基于黎曼流形对实体进行建模</td>
<td>基于语义匹配</td>
<td>ICEWS14、ICEWS05-15、GDELT</td>
<td>非欧式空间下的TKG建模方法</td>
</tr>
<tr class="even">
<td>HIP Network: Historical Information Passing Network for Extrapolation Reasoning on Temporal Knowledge Graph （IJCAI 21）</td>
<td>Extrapolation reasoning</td>
<td>作者认为之前的TKG方法，对于时序信息的利用不够充分。</td>
<td>GNN</td>
<td>ICEWS14、ICEWS18、GDELT、WIKI、YAGO</td>
<td>GNN用于时序知识图谱</td>
</tr>
<tr class="odd">
<td>Temporal Attribute Prediction via Joint Modeling of Multi-Relational Structure Evolution (IJCAI 20)</td>
<td>时序知识图谱推理</td>
<td>之前没有人把temporally evolving graphs和 time series prediction结合到一起</td>
<td>RNN</td>
<td>ATG、CAC、MTG、GDELT</td>
<td>-</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
]]></content>
      <categories>
        <category>Reading-list</category>
        <category>TKG</category>
      </categories>
      <tags>
        <tag>Collection</tag>
        <tag>TKG</tag>
        <tag>Reading-list</tag>
      </tags>
  </entry>
  <entry>
    <title>hypernetworks-collection</title>
    <url>/collection/hypernetworks-collection/</url>
    <content><![CDATA[<h1 id="collection-of-hypernetworks-papers">Collection of Hypernetworks Papers</h1>
<p>调研hypernetwork相关文章</p>
<ul>
<li>Dynamic Convlutional Layer（CVPR 2015）</li>
<li>SRCNN（ICCV 2015）</li>
<li>DFN（NIPS 2016）</li>
<li>HyperNetworks（ICLR 2017）</li>
<li>Nachmani et al. （arxiv 2020）</li>
<li>Hyper-CNN（arxiv 2021）</li>
<li>HyperSeg（CVPR 2021）</li>
<li>LGNN（IJCAI 2021）</li>
</ul>
<span id="more"></span>
<h2 id="dynamic-convlutional-layer">Dynamic Convlutional Layer</h2>
<p><strong>A Dynamic Convolutional Layer for Short Range Weather Prediction</strong> CVPR 2015</p>
<p>针对短时天气预测任务，这个任务会接收一个时序的图像数据，然后预测新的天气图像。构造了一个动态卷积层，对于当前的天气图像要使用的卷积参数，该参数由前面时序的图像输入生成。</p>
<p>模型的整体结构，DC表示动态卷积层，Network B就是用来生成卷积核的网络，同样是一个卷积网络。Network B的输出是两个，垂直卷积核V和水平卷积核H，经过softmax得到SV1和SH1。最后的CROP是一个裁剪层crop layer，只取DC2输出的中心patch。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210914164044854.png" alt="image-20210914164044854" /></p>
<p>产生卷积核的network B结构：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210914164712746.png" alt="image-20210914164712746" style="zoom:50%;" /></p>
<h2 id="srcnn">SRCNN</h2>
<p><strong>Conditioned Regression Models for Non-Blind Single Image Super-Resolution</strong> ICCV 2015</p>
<p>针对Single image super-resolution（SISR）任务，它接收一个低分率的图像<span class="math inline">\(l\)</span>，输出高分辨率图像<span class="math inline">\(h\)</span>。作者认为在还原为高分辨率的图片时，对于不同的图片，应该是有不同的blur kernel，而不是让blur kernel在训练和测试过程中一直固定。</p>
<p>作者为每个image都定义了一个额外的blur kernel，然后使用生成参数的方法生成新的blur 卷积核。SRCNN的示例图。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210914165618622.png" alt="image-20210914165618622" style="zoom:50%;" /></p>
<p>图片中的<span class="math inline">\(W_{1}(k,\theta)\)</span>就是生成的blur kernels，生成方式就是简单的全连接层。</p>
<h2 id="dfn">DFN</h2>
<p><a href="https://github.com/dbbert/dfn"><strong>Dynamic Filter Networks</strong></a> NIPS 2016</p>
<p>作者在video and stereo prediction任务上进行实验，使用一个ﬁlter-generating network生成参数，然后进行dynamic ﬁltering layer。作者除了让参数sample-specific，还产生了location-specific的参数。</p>
<p>概念图如下，其中input B依赖于之前的input A。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210914170121911.png" alt="image-20210914170121911" style="zoom:50%;" /></p>
<p>针对不同预测任务，作者设计了了不同的DFN。</p>
<p>在video prediction上，产生参数的网络是一个encoder-decoder的网络，输出是location-specific的卷积核。 <img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210914170342132.png" alt="image-20210914170342132" style="zoom:50%;" /> 学习steerable ﬁlters，产生参数的网络是MLP。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210914170640801.png" alt="image-20210914170640801" style="zoom:50%;" /> ## HyperNetworks</p>
<p><strong>HyperNetworks</strong> ICLR 2017</p>
<a href="#">Post not found: HyperNetworks[个人详细博客]</a>
<p>核心贡献是将Hypernetwork扩展到了convolutional networks和long recurrent networks，证明其在使用更少的参数情况下，在序列模型和卷积网络的多个预测任务下都达到了不错的训练结果。</p>
<p>CNN：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210725111153567.png"  style="zoom:50%;" /></p>
<p>RNN：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210725111631387.png" alt="image-20210725111631387" style="zoom: 33%;" /></p>
<h2 id="nachmani-et-al.">Nachmani et al.</h2>
<p><strong>Molecule Property Prediction and Classiﬁcation with Graph Hypernetworks</strong> arxiv 2020</p>
<p>在molecule property prediction and classiﬁcation任务上，将hypernetwork引入GNN提升模型效果。为了解决hypernetwork存在的不稳定问题，作者发现拼接current message和first message来作为hypernetwork的输入能够解决这一问题。</p>
<p>作者针对NMP-Edge network, Invariant Graph Network和Graph Isomorphism Network都引入了hypernetwork。下面重点关注对GIN的改进。</p>
<p>GIN原来的形式</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210915103148269.png" alt="image-20210915103148269" style="zoom:50%;" /></p>
<p>改进后：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210915103214168.png" alt="image-20210915103214168" style="zoom:50%;" /></p>
<p><span class="math inline">\(f\)</span>和<span class="math inline">\(g\)</span>是3层和2层使用tanh的MLP。可以看到，它使用current message和first message作为hypernetwork输入，hypernetwork同样是一个GNN的形式，以节点为中心。</p>
<h2 id="hyper-cnn">Hyper-CNN</h2>
<p><strong>Hyper-Convolution Networks for Biomedical Image Segmentation</strong> arxiv 2021</p>
<p>作者将filter kernel的二维坐标作为输入，经过hypernetwork产生对应的kernel value。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210916103216333.png" alt="image-20210916103216333" style="zoom: 33%;" /></p>
<p>作者的hypernetwork是一个多层（四层）的1x1卷积网络，在实现的时候，主网络的每一层都有一个对应的hyper-CNN作为补充。</p>
<p>hypernetwork输入是表示x和y轴的两个channel输入，然后不断经过1x1卷积，不改变channel维度，最后输出weight。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210916103510440.png" alt="image-20210916103510440" style="zoom:50%;" /></p>
<h2 id="hyperseg">HyperSeg</h2>
<p><a href="https://nirkin.com/hyperseg"><strong>HyperSeg: Patch-wise Hypernetwork for Real-time Semantic Segmentation</strong></a> CVPR 2021</p>
<p>这篇文章是针对Real-time Semantic Segmentation任务，不是很了解这个任务，看起来是针对实时拍摄的image进行scene understanding，划分图像边界。</p>
<p>作者引入了hypernetwork，动态产生卷积weight，进行patch-wise的卷积操作，最后输出预测。hypernetwork使用了U-Net的结构（同样不了解）。</p>
<p>整体结构：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210917180259574.png" alt="image-20210917180259574" style="zoom:50%;" /></p>
<p>这里hypernetwork就是context head，它接受backbone的输出。backbone会把原来的image划分为不同resolution的feature map，最后的一个feature map输出给hypernetwork。</p>
<p>hypernetwork的输出是一个大的signal map，提供给不同的meta-block使用，划分方式是作者设计了一个根据channel和不同meta-block需要的weights进行划分，划分方法在附录里，没有细看。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210917180815544.png" alt="image-20210917180815544" style="zoom:50%;" /></p>
<h2 id="lgnn">LGNN</h2>
<p><strong>Node-wise Localization of Graph Neural Networks</strong> IJCAI 2021</p>
<p>作者认为对于整个图学习同样的weight matrix，可能导致模型倾向于建模最常见的pattern，而不是针对不同node的不同的local context进行学习。作者让graph中不同node拥有不同的weight matrix。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210921195057584.png" alt="image-20210921195057584" style="zoom:50%;" /></p>
<p>具体有两个Node-level localization和Edge-level localization.</p>
<p><strong>Node-level localization</strong></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210921195215512.png" alt="image-20210921195215512" style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210921195243176.png" alt="image-20210921195243176" style="zoom:50%;" /></p>
<p>注意，这里没有给不同node都定义新的vector，而是直接从上一层的邻居直接mean聚合，然后进行转换，生成的向量<span class="math inline">\(a_v\)</span>和<span class="math inline">\(b_v\)</span>之后用于生成node <span class="math inline">\(v\)</span>的weight matrix。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210921195358969.png" alt="image-20210921195358969" style="zoom:50%;" /></p>
<p>注意这里，是把<span class="math inline">\(a_v\)</span>和<span class="math inline">\(b_v\)</span>作为一行，然后复制，最后作用到graph global matrix<span class="math inline">\(W_l\)</span>上。</p>
<p><strong>Edge-level localization</strong></p>
<p>作者对node <span class="math inline">\(v\)</span>的不同邻居edge进一步建模：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210921195557949.png" alt="image-20210921195557949" style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210921195613526.png" alt="image-20210921195613526" style="zoom:50%;" /></p>
<p>最后聚合：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210921195641977.png" alt="image-20210921195641977" style="zoom:50%;" /></p>
]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>Collection</tag>
      </tags>
  </entry>
  <entry>
    <title>good-paper-sentences-collection</title>
    <url>/collection/paper-sentences-collection/</url>
    <content><![CDATA[<p>好的句子收集</p>
<span id="more"></span>
<ol type="1">
<li>Our evaluation shows that our approach obtains better results than task-specific handcrafted representations across different tasks and programming languages （我们的评估结果显示....）</li>
<li>Leveraging machine learning models for predicting program properties(利用机器学习模型预测程序属性)</li>
<li>We present a novel program representation （我们提出了一个新的...）</li>
<li>In this paper, we demonstrate the power and generality of AST paths on the following tasks: （利用下面的任务说明了....）</li>
<li>Empirical studies have shown that...（实验性的研究表明...）</li>
<li>Raychev et al. （...等人）i.e. (也就是说) e.g. (比如)</li>
<li>Automatic generation may produce a prohibitively large number of paths.（产生出令人望而却步的代价...）</li>
<li>In Sections 3.1 and 3.2 we present CRFs and word2vec(在第几部分我们介绍了...) in this section(这一部分...)</li>
<li>neural-network based approaches have shown（基于神经网络的方法...）</li>
<li>we base the following definitions on pairwise paths between AST terminals.（base...on... 基于）</li>
<li>if and only if...(当且仅当)</li>
<li>This paper makes the following contributions.（本论文的贡献集中在以下方面）</li>
<li>Section 6 and 7 are dedicated to the discussion of our results and conclusions.（第6部分和第七部分集中于）</li>
<li>In the next section we describe（在下一部分我们描述了）</li>
<li>To summarize, our E.T.-RNN approach would possibly work better than（总的来说.....）</li>
<li>In this following,（在这之后）</li>
<li>Khashman tests NN classiﬁers with different training to validation data ratios （测试了不同训练集和测试集比...）</li>
<li>By employing different kernel functions, SVM technique can be applied to（通过应用不同的核函数...）</li>
<li>The process of boosting continues until the loss function reduction becomes limited.（直至损失函数收敛）</li>
<li>In accordance with the suggestion of Ala’raj and Abbod (2016b),（根据....的建议，不要总是使用according to）</li>
<li>Ranging from early matrix factorization to <strong>recently emerged</strong> deep learning based methods,（从早期的矩阵分解，到现在出现的深度学习方法）</li>
<li><strong>Distinct from HOP-Rec</strong>, we contribute a new technique to integrate high-order connectivities into the prediction model,(与......不同，没有用到different from)</li>
<li>This not only increases the model representation ability, but also <strong>boosts</strong> the performance for recommendation(提升了性能，使用了boost这个词，而不是improve)</li>
<li>Towards this end, we perform experiments over user groups of different sparsity levels. (为此，我们开展了实验.....)</li>
<li>For fair consideration, the latent dimensions of all compared baselines are set the same as in Table 2,（处于公平角度考虑....）</li>
<li>The results demonstrate the significant superiority of RippleNet over strong baselines（结果展示了模型的明显的提升效果）</li>
<li>Recently, many studies on extending deep learning approaches for graph data have emerged（最近出现了很多的研究.....）</li>
<li>Our paper makes notable contributions summarized as follows（我们论文的贡献总结如下：）</li>
<li>we refer the readers to [39]（我们推荐/建议读者参考...）</li>
<li>Attention mechanisms have become almost a de facto standard in many sequence-based tasks（注意力机制已经成为事实上的标准....）</li>
<li>In general, the modeling process boils down to extracting local or global connectivity patterns between entities（通常，建模过程归结为....）</li>
<li>we show marked performance gains in comparison to state-of-the-art methods on all datasets.（和目前最好的结果表现出了显著的结果）</li>
<li>To the best of our knowledge,（就我们目前所知）</li>
<li>aroused considerable research interest（引起了很大的研究兴趣）</li>
<li>we ﬁnd that COMPGCN outperforms all the existing methods in 4 out of 5 metrics on FB15k-237 and in 3 out of 5 metrics on WN18RR dataset.(数据集里的在4个里面的五个指标上取得了更好的结果)</li>
<li>We defer this as future work（我们推迟这个作为将来的工作）</li>
<li>a blowup in the number of parameters that need to be estimated.（大量需要估计的参数）</li>
<li>Another approach for graph embeddings is thus to leverage proven approaches for language embeddings.（使用已经被证明过的方法...）</li>
<li>we also discuss quality metrics that provide ways to measure quantitative aspects of these dimensions. （讨论定量的方面....）</li>
<li>GNNs are notorious for their poor scalability.（GNN因差可扩放性而臭名昭著）</li>
<li>We speculate that（我们推测...）</li>
<li>In this setting, we compare the（在这种设置下）</li>
<li>we leave these results out of our comparison table.（我们在对比结果中排除了....）</li>
<li>Three benchmark datasets (FB15k-237, WN18RR and FB15k-237-Attr) are utilized in this study. （在本文中使用了...数据集）</li>
<li>Our work is mainly related to two lines of research（我们的工作主要与两方面的研究有关）</li>
<li>Empirically, our model yields considerable performance improvements over existing embedding models,（我们的论文取得了很大的效果提升）</li>
<li>We empirically evaluate different choices of entity representations and relation representations under this framework on the canonical link prediction task（我们在典型的，标准的任务上评估了）</li>
<li>SEEK can achieve either state-of-the-art or highly competitive performance on a variety of benchmarks for KGE compared with existing methods.（和现在的方法相比，方法达到了有竞争力或者目前最优的解）</li>
<li>Numerous efforts have since continued to push the boundaries of recurrent language models（人们一直不断努力扩大模型的界限）</li>
<li>Our overarching interest is whether（我们首要的兴趣是...）</li>
<li>Our experimental study provides additional evidence for this ﬁnding.（我们的实验为之间的发现提供了额外的证明）</li>
<li>Similar remarks hold for RESCAL and DistMult as well as (albeit to a smaller extent) ConvE and TransE.（类似的说法对于....也成立）</li>
<li>RESCAL (Nickel et al., 2011), which constitutes one of the ﬁrst KGE models（Resscal被视作KGE的第一个工作之一）</li>
<li>predicting the properties of molecules and materials using machine learning (and especially deep learning) is still in its infancy.（使用机器学习预测化学分子或者材料的属性仍然处在初期阶段）</li>
<li>most research applying machine learning to chemistry tasks has revolved around feature engineering.（大多数的研究都是围绕...）</li>
<li>empowering HGT to maintain dedicated representation for different types of nodes and edges. (使HGT能够为不同的node和edge获得专门的表示)</li>
<li>Figure 1 depicts he macro-structure of Mixer.（图1描绘了整体结构）</li>
<li>Vinyals et al. [32] and Ravi and Larochelle [24] apply Matching Networks using cosine distance. However for both Prototypical Networks and Matching Networks any distance is permissible（对于模型来说...都是允许的）</li>
<li>For Protypical Networks, we conjecture this is primarily due to cosine distance not being a Bregman divergence（我们猜测某种现象/结果可能是因为...）</li>
<li>While suggestive as a research result, in terms of practical applications, the zero-shot performance of GPT-2 is still far from use-able.（作为实验结果有启发性）</li>
<li>We <strong>hold that</strong> the poor performance of the pre-trained multimodal model may be attributed to the fact that the pre-training datasets and objects have gaps in information extraction tasks. (我们认为/我们假设)</li>
</ol>
]]></content>
      <tags>
        <tag>Collection</tag>
      </tags>
  </entry>
  <entry>
    <title>CapsE</title>
    <url>/kge/CapsE/</url>
    <content><![CDATA[<h1 id="a-capsule-network-based-embedding-model-for-knowledge-graph-completion-and-search-personalization">A Capsule Network-based Embedding Model for Knowledge Graph Completion and Search Personalization</h1>
<p>2019-6-2s</p>
<h2 id="introduction">1 Introduction</h2>
<p>常用的KE模型，比如TransE，Complex，DISTMULT等模型，它们只捕获了三元实体之间的线性联系，没有捕获非线性的联系。</p>
<p>本论文的基础是在capsule networks（CapsNet）Dynamic routing between capsules的基础上，直接应用到knowledge graph triplet上。CPasNet原来是作用于图片上。</p>
<p>论文的理论是处在相同维度下的triplet，同一纬度下的embedding可以通过capsule（each capsule is a group of neurons） network捕获不同的变体。</p>
<span id="more"></span>
<h2 id="the-proposed-capse">2 The proposed CapsE</h2>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20200217182118840.png" style="zoom:50%;" /></p>
<p>特点：</p>
<ol type="1">
<li>三元组直接作为一个矩阵进行训练</li>
<li>最后的score function是向量的长度</li>
</ol>
<p>具体请看论文描述</p>
<h2 id="knowledge-graph-completion-evaluation">3 Knowledge graph completion evaluation</h2>
<p>数据集：</p>
<ul>
<li>WN18RR</li>
<li>FB15k-237</li>
</ul>
<p>评估指标：</p>
<ul>
<li>Mean rank（MR）</li>
<li>Mean reciprocal rank (MRR)</li>
<li>Hits@10</li>
</ul>
<p>Embedding 初始化：</p>
<ul>
<li>ConvKB和CapsE都使用了TransE训练好之后的embedding来初始化</li>
<li>对于TransE，在WN18RR数据集下，使用了100-dimensional Glove word embeddings初始化</li>
</ul>
<p>参数设置：</p>
<ul>
<li>初始的KE维度为100</li>
<li>过滤器filter数量的设置在{50，100, 200, 400}</li>
</ul>
<p>关于关系r的分类：</p>
<blockquote>
<p>Following Bordes et al. (2013), for each relation r in FB15k-237, we calculate the averaged number <span class="math inline">\(\eta_s\)</span> of head entities per tail entity and the averaged number <span class="math inline">\(\eta_o\)</span> of tail entities per head entity. If <span class="math inline">\(\eta_s\)</span> &lt;1.5 and <span class="math inline">\(\eta_o\)</span> &lt;1.5, r is categorized one-to-one (1-1). If <span class="math inline">\(\eta_s\)</span> &lt;1.5 and <span class="math inline">\(\eta_o\)</span> ≥ 1.5, r is categorized one-to-many (1-M). If <span class="math inline">\(\eta_s\)</span> ≥ 1.5 and <span class="math inline">\(\eta_o\)</span> &lt;1.5, r is categorized many-to-one (M-1). If <span class="math inline">\(\eta_s\)</span> ≥ 1.5 and <span class="math inline">\(\eta_o\)</span> ≥ 1.5, r is categorized many-to-many (M-M)</p>
</blockquote>
<p>最后得到的结果显示M-M的关系总是最多的</p>
<p>使用filtered设置进行训练</p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>KGE</category>
      </categories>
  </entry>
  <entry>
    <title>CoPER</title>
    <url>/kge/CoPER/</url>
    <content><![CDATA[<h1 id="coper">CoPER</h1>
<p>认为之前的KGE方法将entity embedding和relation embedding之间的交互都局限在additive上。 这篇文章使用relation embedding产生参数，转化entity embedding。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/Screen-Shot-2020-09-25-at-4.22.16-PM.png" /></p>
<span id="more"></span>
<p>首先它指出了之前很多方法对于entity embedding和relation embedding的学习是受限的，比如<span class="math inline">\(w[h;r]\)</span>，这样的方式，无法建模下面的例子。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/Screen-Shot-2020-09-25-at-4.22.09-PM.png" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/Screen-Shot-2020-09-25-at-4.16.00-PM.png" /></p>
<p>属于additive的方式，无法很好的区分<span class="math inline">\(e_0\)</span>，<span class="math inline">\(e_1\)</span>，<span class="math inline">\(e_2\)</span>，<span class="math inline">\(e_3\)</span></p>
<p>在实现的方法中，它设计了三个不同的产生参数的module，contextual parameter generator (CPG).</p>
<p>Parameter Lookup Table：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/Screen-Shot-2020-09-25-at-4.21.05-PM.png" /></p>
<p>Linear Projection.：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/Screen-Shot-2020-09-25-at-4.22.25-PM.png" /></p>
<p>Multi-Layer Perceptron. ：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/Screen-Shot-2020-09-25-at-4.21.33-PM.png" /></p>
<p>实验中发现，第一种方法效果s都不太好，容易过拟合，而且relation之间没有information sharing。第二中方法和第三种方法更合适，第二中方法适合于更大size的dataset，第三种更适合小一点的dataset。 在ConvE和MINERVA方法的基础上加入了CPG模块，具体看图</p>
<p>需要注意的一点是它使用了新的训练方式，导致ConvE和DISTMULT这些方法的效果远远好于原来的效果，以后可以详细看一下。 优点：和ParamE类似，都是将relation embedding转换到了parameter space中，但是ParamE是完全将模型参数都作为relation embedding，而CoPER是使用某种结构转换relation embedding到参数中，还需要选择在什么地方使用由relation embedding转换过来的parameter。 可以利用或改进的点：可以利用这样的思想，将relation embedding转换为参数，然后和entity embedding进行交互。</p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>KGE</category>
      </categories>
  </entry>
  <entry>
    <title>MMML-survey-list</title>
    <url>/collection/MMML-survey-list/</url>
    <content><![CDATA[<h1 id="multimodal-machine-learning">Multimodal Machine Learning</h1>
<p>对目前的多模态机器学习相关文献进行的调研list。</p>
<span id="more"></span>
<h2 id="tutorial-and-course">Tutorial and Course</h2>
<table>
<colgroup>
<col style="width: 20%" />
<col style="width: 19%" />
<col style="width: 29%" />
<col style="width: 30%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">地址</th>
<th style="text-align: center;">标题</th>
<th>内容</th>
<th>评价</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><a href="https://cmu-multicomp-lab.github.io/mmml-tutorial/schedule/">课程地址</a></td>
<td style="text-align: center;"><strong>T</strong>utorial on <strong>M</strong>ulti<strong>M</strong>odal <strong>M</strong>achine <strong>L</strong>earning</td>
<td>CMU 2022年最新的MMML tutorial，包括了slides和videos，主要介绍多模态机器学习相关概念和关键挑战；值得新人阅读。</td>
<td>内容较为全面，并且从专业的角度看到MMML的发展现状和挑战。作者说了很快会有一个600+引用文献的survey出现，值得期待。</td>
</tr>
<tr class="even">
<td style="text-align: center;"><a href="https://cmu-multicomp-lab.github.io/adv-mmml-course/spring2022/">课程地址</a></td>
<td style="text-align: center;"><strong>A</strong>dvanced <strong>T</strong>opics in <strong>M</strong>ulti<strong>M</strong>odal <strong>M</strong>achine <strong>L</strong>earning</td>
<td>CMU 2022春季的MMML各个领域最新的研究挑战；没有视频，但是提供了讲义。</td>
<td>还未阅读，目前看到讲义内容不多。</td>
</tr>
<tr class="odd">
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<h2 id="paper">Paper</h2>
<h3 id="acl-22">ACL 22</h3>
<ol type="1">
<li>Cross-Modal Discrete Representation Learning</li>
<li>Finding Structural Knowledge in Multimodal-BERT</li>
<li>Guided Attention Multimodal Multitask Financial Forecasting with Inter-Company Relationships and Global and Local News</li>
<li>Leveraging Unimodal Self-Supervised Learning for Multimodal Audio-Visual Speech Recognition</li>
<li>Leveraging Visual Knowledge in Language Tasks: An Empirical Study on Intermediate Pre-training for Cross-modal Knowledge Transfer</li>
<li>M3ED: Multi-modal Multi-scene Multi-label Emotional Dialogue Database</li>
<li>Modeling Temporal-Modal Entity Graph for Procedural Multimodal Machine Comprehension</li>
<li>MSCTD: A Multimodal Sentiment Chat Translation Dataset</li>
<li>Multi-Modal Sarcasm Detection via Cross-Modal Graph Convolutional Network</li>
<li>Multimodal Dialogue Response Generation</li>
<li>Multimodal fusion via cortical network inspired losses</li>
<li>Multimodal Sarcasm Target Identification in Tweets</li>
<li>On Vision Features in Multimodal Machine Translation</li>
<li>Phone-ing it in: Towards Flexible, Multi-Modal Language Model Training using Phonetic Representations of Data</li>
<li>Premise-based Multimodal Reasoning: Conditional Inference on Joint Textual and Visual Clues</li>
<li>RoCBert: Robust Chinese Bert with Multimodal Contrastive Pretraining</li>
<li>SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing</li>
<li>Understanding Multimodal Procedural Knowledge by Sequencing Multimodal Instructional Manuals</li>
<li>UniXcoder: Unified Cross-Modal Pre-training for Code Representation</li>
<li>When did you become so smart, oh wise one?! Sarcasm Explanation in Multi-modal Multi-party Dialogues</li>
<li>XDBERT: Distilling Visual Information to BERT from Cross-Modal Systems to Improve Language Understanding</li>
<li>M-SENA: An Integrated Platform for Multimodal Sentiment Analysis</li>
<li>MMCoQA: Conversational Question Answering over Text, Tables, and Images</li>
</ol>
<p>Findings of ACL 22</p>
<ol start="24" type="1">
<li>Reinforced Cross-modal Alignment for Radiology Report Generation</li>
<li>Cross-Modal Cloze Task: A New Task to Brain-to-Word Decoding</li>
<li>Sentiment Word Aware Multimodal Refinement for Multimodal Sentiment Analysis with ASR Errors</li>
<li>Modality-specific Learning Rates for Effective Multimodal Additive Late-fusion</li>
<li>Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation</li>
<li>Assessing Multilingual Fairness in Pre-trained Multimodal Representations</li>
<li>Modular and Parameter-Efficient Multimodal Fusion with Prompting</li>
<li>Comprehensive Multi-Modal Interactions for Referring Image Segmentation</li>
<li>Attention as Grounding: Exploring Textual and Cross-Modal Attention on Entities and Relations in Language-and-Vision Transformer</li>
<li>Improving Candidate Retrieval with Entity Profile Generation for Wikidata Entity Linking（把mention和Wikidata中的实体联系在一起，没有使用多模态信息）</li>
</ol>
<h3 id="acl-21">ACL 21</h3>
<ol type="1">
<li>Multimodal Sentiment Detection Based on Multi-channel Graph Neural Networks</li>
<li>Self-Supervised Multimodal Opinion Summarization</li>
<li>Learning Relation Alignment for Calibrated Cross-modal Retrieval</li>
<li>KM-BART: Knowledge Enhanced Multimodal BART for Visual Commonsense Generation</li>
<li>The Possible, the Plausible, and the Desirable: Event-Based Modality Detection for Language Processing</li>
<li>Factuality Assessment as Modal Dependency Parsing</li>
<li>Multi-stage Pre-training over Simplified Multimodal Pre-training Models</li>
<li>LayoutLMv2: Multi-modal Pre-training for Visually-rich Document Understanding</li>
<li>UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning</li>
<li>Missing Modality Imagination Network for Emotion Recognition with Uncertain Missing Modalities</li>
<li>Competence-based Multimodal Curriculum Learning for Medical Report Generation</li>
<li>MultiMET: A Multimodal Dataset for Metaphor Understanding</li>
<li>Constructing Multi-Modal Dialogue Dataset by Replacing Text with Semantically Relevant Images</li>
<li>Learning Language and Multimodal Privacy-Preserving Markers of Mood from Mobile Data</li>
<li>CTFN: Hierarchical Learning for Multimodal Sentiment Analysis Using Coupled-Translation Fusion Network</li>
<li>MMGCN: Multimodal Fusion via Deep Graph Convolution Network for Emotion Recognition in Conversation</li>
<li>Cross-modal Memory Networks for Radiology Report Generation</li>
<li>Multi-perspective Coherent Reasoning for Helpfulness Prediction of Multimodal Reviews</li>
<li>Good for Misconceived Reasons: An Empirical Revisiting on the Need for Visual Context in Multimodal Machine Translation</li>
<li>Multimodal Multi-Speaker Merger &amp; Acquisition Financial Modeling: A New Task, Dataset, and Neural Baselines</li>
<li>More than Text: Multi-modal Chinese Word Segmentation</li>
<li>Constructing Multi-Modal Dialogue Dataset by Replacing Text with Semantically Relevant Images</li>
</ol>
<p>Findings of ACL 21</p>
<ol start="23" type="1">
<li>Multimodal Incremental Transformer with Visual Grounding for Visual Dialogue Generation</li>
<li>GeoQA: A Geometric Question Answering Benchmark Towards Multimodal Numerical Reasoning</li>
<li>Read, Listen, and See: Leveraging Multimodal Information Helps Chinese Spell Checking</li>
<li>Deciphering Implicit Hate: Evaluating Automated Detection Algorithms for Multimodal Hate</li>
<li>Entheos: A Multimodal Dataset for Studying Enthusiasm</li>
<li>Multimodal Fusion with Co-Attention Networks for Fake News Detection</li>
<li>GEM: A General Evaluation Benchmark for Multimodal Tasks</li>
<li>Transformer-Exclusive Cross-Modal Representation for Vision and Language</li>
<li>Probing Multi-modal Machine Translation with Pre-trained Language Model</li>
<li>Multimodal Graph-based Transformer Framework for Biomedical Relation Extraction</li>
<li>A Text-Centered Shared-Private Framework via Cross-Modal Prediction for Multimodal Sentiment Analysis</li>
</ol>
<h3 id="acl-20">ACL 20</h3>
<ol type="1">
<li>Cross-modal Language Generation using Pivot Stabilization for Web-scale Language Coverage</li>
<li>Multimodal Quality Estimation for Machine Translation</li>
<li>MMPE: A Multi-Modal Interface for Post-Editing Machine Translation</li>
<li>Integrating Multimodal Information in Large Pretrained Transformers</li>
<li>MultiQT: Multimodal Learning for Real-Time Question Tracking in Speech</li>
<li>Multiresolution and Multimodal Speech Recognition with Transformers</li>
<li>A Novel Graph-based Multi-modal Fusion Encoder for Neural Machine Translation</li>
<li>Learning to Recover from Multi-Modality Errors for Non-Autoregressive Neural Machine Translation</li>
<li>Improving Multimodal Named Entity Recognition via Entity Span Detection with Unified Multimodal Transformer</li>
<li>CH-SIMS: A Chinese Multimodal Sentiment Analysis Dataset with Fine-grained Annotations of Modality</li>
<li>Reasoning with Multimodal Sarcastic Tweets via Modeling Cross-Modality Contrast and Semantic Association</li>
<li>Multimodal Transformer for Multimodal Machine Translation</li>
<li>Sentiment and Emotion help Sarcasm? A Multi-task Learning Framework for Multi-Modal Sarcasm, Sentiment and Emotion Analysis</li>
<li>Towards Emotion-aided Multi-modal Dialogue Act Classification</li>
<li>A Recipe for Creating Multimodal Aligned Datasets for Sequential Tasks</li>
<li>Benchmarking Multimodal Regex Synthesis with Complex Structures</li>
<li>Clue: Cross-modal Coherence Modeling for Caption Generation</li>
<li>Multimodal Neural Graph Memory Networks for Visual Question Answering</li>
<li>Cross-Modality Relevance for Reasoning on Language and Vision</li>
<li>Unsupervised Multimodal Neural Machine Translation with Pseudo Visual Pivoting ### IJCAI 22</li>
<li>Cross-modal Representation Learning and Relation Reasoning for Bidirectional Adaptive Manipulation</li>
<li>Unsupervised Misaligned Infrared and Visible Image Fusion via Cross-Modality Image Generation and Registration</li>
<li>SHAPE: An Unified Approach to Evaluate the Contribution and Cooperation of Individual Modalities</li>
<li>MMT: Multi-Way Multi-Modal Transformer for Multimodal Learning</li>
<li>AutoAlign: Pixel-Instance Feature Aggregation for Multi-Modal 3D Object Detection</li>
<li>Lightweight Bimodal Network for Single-Image Super-Resolution via Symmetric CNN and Recursive Transformer</li>
<li>Unsupervised Multi-Modal Medical Image Registration via Discriminator-Free Image-to-Image Translation</li>
<li>Targeted Multimodal Sentiment Classification based on Coarse-to-Fine Grained Image-Target Matching</li>
<li>Recipe2Vec: Multi-modal Recipe Representation Learning with Graph Neural Networks</li>
<li>PlaceNet: Neural Spatial Representation Learning with Multimodal Attention</li>
<li>Representation Learning for Compressed Video Action Recognition via Attentive Cross-modal Interaction with Motion Enhancement</li>
<li>MA-ViT: Modality-Agnostic Vision Transformers for Face Anti-Spoofing</li>
<li>Unsupervised Voice-Face Representation Learning by Cross-Modal Prototype Contrast</li>
<li>MFAN: Multi-modal Feature-enhanced Attention Networks for Rumor Detection</li>
</ol>
<h3 id="ijcai-21">IJCAI 21</h3>
<ol type="1">
<li>Dig into Multi-modal Cues for Video Retrieval with Hierarchical Alignment</li>
<li>Deep Unified Cross-Modality Hashing by Pairwise Data Alignment</li>
<li>Weakly Supervised Dense Video Captioning via Jointly Usage of Knowledge Distillation and Cross-modal Matching</li>
<li>MRD-Net: Multi-Modal Residual Knowledge Distillation for Spoken Question Answering</li>
<li>Rethinking Label-Wise Cross-Modal Retrieval from A Semantic Sharing Perspective</li>
<li>MDNN: A Multimodal Deep Neural Network for Predicting Drug-Drug Interaction Events</li>
<li>Modality-aware Style Adaptation for RGB-Infrared Person Re-Identification</li>
<li>Multimodal Transformer Network for Pedestrian Trajectory Prediction</li>
<li>SalientSleepNet: Multimodal Salient Wave Detection Network for Sleep Staging</li>
<li>UIBert: Learning Generic Multimodal Representations for UI Understanding</li>
</ol>
<h3 id="ijcai-20">IJCAI 20</h3>
<ol type="1">
<li>A Similarity Inference Metric for RGB-Infrared Cross-Modality Person Re-identification</li>
<li>Set and Rebase: Determining the Semantic Graph Connectivity for Unsupervised Cross-Modal Hashing</li>
<li>A Modal Logic for Joint Abilities under Strategy Commitments</li>
<li>Embodied Multimodal Multitask Learning</li>
<li>Triple-GAIL: A Multi-Modal Imitation Learning Framework with Generative Adversarial Nets</li>
<li>“The Squawk Bot”∗ : Joint Learning Of Time Series And Text Data Modalities For Automated Financial Information Filtering</li>
<li>Interpretable Multimodal Learning for Intelligent Regulation in Online Payment Systems</li>
</ol>
<h3 id="emnlp-21">EMNLP 21</h3>
<ol type="1">
<li>Improving Multimodal fusion via Mutual Dependency Maximisation</li>
<li>Text2Mol: Cross-Modal Molecule Retrieval with Natural Language Queries</li>
<li>Iconary: A Pictionary-Based Game for Testing Multimodal Communication with Drawings and Text</li>
<li>Multimodal Phased Transformer for Sentiment Analysis</li>
<li>CTAL: Pre-training Cross-modal Transformer for Audio-and-Language Representations</li>
<li>Vision Guided Generative Pre-trained Language Models for Multimodal Abstractive Summarization</li>
<li>How to Leverage Multimodal EHR Data for Better Medical Predictions?</li>
<li>Joint Multi-modal Aspect-Sentiment Analysis with Auxiliary Cross-modal Relation Detection</li>
<li>Multi-Modal Open-Domain Dialogue</li>
<li>SIMMC 2.0: A Task-oriented Dialog Dataset for Immersive Multimodal Conversations</li>
<li>Hitting your MARQ: Multimodal ARgument Quality Assessment in Long Debate Video</li>
<li>NewsCLIPpings: Automatic Generation of Out-of-Context Multimodal Media</li>
<li>Vision Matters When It Should: Sanity Checking Multimodal Machine Translation Models</li>
<li>Unimodal and Crossmodal Refinement Network for Multimodal Sequence Fusion</li>
<li>Improving Multimodal Fusion with Hierarchical Mutual Information Maximization for Multimodal Sentiment Analysis</li>
<li>On Pursuit of Designing Multi-modal Transformer for Video Grounding</li>
<li>Vision-and-Language or Vision-for-Language? On Cross-Modal Influence in Multimodal Transformers</li>
<li>M-Arg: Multimodal Argument Mining Dataset for Political Debates with Audio and Transcripts</li>
<li>VQA-MHUG: A Gaze Dataset to Study Multimodal Neural Attention in Visual Question Answering</li>
<li>Point-of-Interest Type Prediction using Text and Images（首个根据推特的文本和图像判断POI类型）</li>
</ol>
<p>Findings of EMNLP 21</p>
<ol start="20" type="1">
<li>Self-supervised Contrastive Cross-Modality Representation Learning for Spoken Question Answering</li>
<li>Cross-Modal Retrieval Augmentation for Multi-Modal Classification</li>
<li>What Does Your Smile Mean? Jointly Detecting Multi-Modal Sarcasm and Sentiment Using Quantum Probability</li>
<li>Entity-level Cross-modal Learning Improves Multi-modal Machine Translation</li>
<li>Which is Making the Contribution: Modulating Unimodal and Cross-modal Dynamics for Multimodal Sentiment Analysis</li>
<li>Optimal Neural Program Synthesis from Multimodal Specifications</li>
<li>MIRTT: Learning Multimodal Interaction Representations from Trilinear Transformers for Visual Question Answering</li>
<li>DialogueTRM: Exploring Multi-Modal Emotion Dynamics in Conversations</li>
<li>An animated picture says at least a thousand words: Selecting Gif-based Replies in Multimodal Dialog</li>
<li>MURAL: Multimodal, Multitask Retrieval Across Languages</li>
<li>MSD: Saliency-aware Knowledge Distillation for Multimodal Understanding</li>
<li>MOMENTA: A Multimodal Framework for Detecting Harmful Memes and Their Targets</li>
</ol>
<h3 id="emnlp-20">EMNLP 20</h3>
<ol type="1">
<li>X-LXMERT: Paint, Caption and Answer Questions with Multi-Modal Transformers</li>
<li>Generating Image Descriptions via Sequential Cross-Modal Alignment Guided by Human Gaze</li>
<li>Multimodal Routing: Improving Local and Global Interpretability of Multimodal Language Analysis</li>
<li>Multi-modal Multi-label Emotion Detection with Modality and Label Dependence</li>
<li>Multistage Fusion with Forget Gate for Multimodal Summarization in Open-Domain Videos</li>
<li>Detecting Cross-Modal Inconsistency to Defend Against Neural Fake News</li>
<li>VMSMO: Learning to Generate Multimodal Summary for Video-based News Articles</li>
<li>Does my multimodal model learn cross-modal interactions? It’s harder to tell than you might think!</li>
<li>CMU-MOSEAS: A Multimodal Language Dataset for Spanish, Portuguese, German and French</li>
<li>Cross-Media Keyphrase Prediction: A Unified Framework with Multi-Modality Multi-Head Attention and Image Wordings</li>
<li>Multimodal Joint Attribute Prediction and Value Extraction for E-commerce Product</li>
<li>Unsupervised Natural Language Inference via Decoupled Multimodal Contrastive Learning</li>
<li>MAF: Multimodal Alignment Framework for Weakly-Supervised Phrase Grounding</li>
</ol>
<p>Findings of EMNLP 20</p>
<ol start="15" type="1">
<li>Open-Ended Visual Question Answering by Multi-Modal Domain Adaptation</li>
<li>Dual Low-Rank Multimodal Fusion</li>
<li>DocStruct: A Multimodal Method to Extract Hierarchy Structure in Document for General Form Understanding</li>
<li>Modeling Intra and Inter-modality Incongruity for Multi-Modal Sarcasm Detection</li>
<li>MultiDM-GCN: Aspect-guided Response Generation in Multi-domain Multi-modal Dialogue System using Graph Convolutional Network</li>
<li>Fine-Grained Grounding for Multimodal Speech Recognition</li>
<li>MMFT-BERT: Multimodal Fusion Transformer with BERT Encodings for Visual Question Answering</li>
</ol>
<h3 id="naacl-22">NAACL 22</h3>
<ol type="1">
<li>Analyzing Modality Robustness in Multimodal Sentiment Analysis</li>
<li>Beyond Emotion: A Multi-Modal Dataset for Human Desire Understanding</li>
<li>Twitter-COMMs: Detecting Climate, COVID, and Military Multimodal Misinformation</li>
<li>A Study of Syntactic Multi-Modality in Non-Autoregressive Machine Translation</li>
<li>Modal Dependency Parsing via Language Model Priming</li>
<li>Multimodal Dialogue State Tracking</li>
<li>GMN: Generative Multi-modal Network for Practical Document Information Extraction</li>
<li>A Computational Acquisition Model for Multimodal Word Categorization</li>
<li>COGMEN: COntextualized GNN based Multimodal Emotion recognitioN</li>
<li>Cross-modal Contrastive Learning for Speech Translation</li>
<li>Visual Commonsense in Pretrained Unimodal and Multimodal Models</li>
<li>MCSE: Multimodal Contrastive Learning of Sentence Embeddings</li>
<li>JointLK: Joint Reasoning with Language Models and Knowledge Graphs for Commonsense Question Answering（作者把LM和KG看做是两个modality）</li>
<li>KAT: A Knowledge Augmented Transformer for Vision-and-Language</li>
</ol>
<p>Findings of NNACL 22</p>
<ol start="13" type="1">
<li>Multimodal Intent Discovery from Livestream Videos</li>
<li>Learning to Embed Multi-Modal Contexts for Situated Conversational Agents</li>
<li>MM-Claims: A Dataset for Multimodal Claim Detection in Social Media</li>
<li>Cross-Lingual Cross-Modal Consolidation for Effective Multilingual Video Corpus Moment Retrieval</li>
<li>CLMLF:A Contrastive Learning and Multi-Layer Fusion Method for Multimodal Sentiment Detection</li>
</ol>
<h3 id="naacl-21">NAACL 21</h3>
<ol type="1">
<li>MTAG: Modal-Temporal Attention Graph for Unaligned Human Multimodal Language Sequences</li>
<li>Improving Cross-Modal Alignment in Vision Language Navigation via Syntactic Information</li>
<li>Multilingual Multimodal Pre-training for Zero-Shot Cross-Lingual Transfer of Vision-Language Models</li>
<li>MUSER: MUltimodal Stress Detection using Emotion Recognition as an Auxiliary Task</li>
<li>Cross-lingual Cross-modal Pretraining for Multimodal Retrieval</li>
<li>An Empirical Investigation of Bias in the Multimodal Analysis of Financial Earnings Calls</li>
<li>Multimodal End-to-End Sparse Model for Emotion Recognition</li>
<li>MIMOQA: Multimodal Input Multimodal Output Question Answering</li>
<li>Towards Sentiment and Emotion aided Multi-modal Speech Act Classification in Twitter</li>
<li>MM-AVS: A Full-Scale Dataset for Multi-modal Summarization</li>
<li>Larger-Context Tagging: When and Why Does It Work?</li>
</ol>
<h3 id="naacl">NAACL</h3>
<p>Quantifying the visual concreteness of words and topics in multimodal datasets. NAACL 18</p>
<h3 id="www-22">WWW 22</h3>
<ol type="1">
<li>Multimodal Continual Graph Learning with Neural Architecture Search</li>
<li>Modality Matches Modality: Pretraining Modality-Disentangled Item Representations for Recommendation</li>
<li>Cross-modal Ambiguity Learning for Multimodal Fake News Detection</li>
<li>A Duo-generative Approach to Explainable Multimodal COVID-19 Misinformation Detection</li>
<li>On Explaining Multimodal Hateful Meme Detection Models</li>
</ol>
<h3 id="www-21">WWW 21</h3>
<ol type="1">
<li>High-Dimensional Sparse Cross-Modal Hashing with Fine-Grained Similarity Embedding</li>
</ol>
<h3 id="www-20">WWW 20</h3>
<ol type="1">
<li>Nowhere to Hide: Cross-modal Identity Leakage between Biometrics and Devices</li>
<li>Adversarial Multimodal Representation Learning for Click-Through Rate Prediction</li>
<li>Learning from Cross-Modal Behavior Dynamics with Graph-Regularized Neural Contextual Bandit</li>
<li>Learning to Respond with Stickers: A Framework of Unifying Multi-Modality in Multi-Turn Dialog</li>
<li>Domain Adaptive Multi-Modality Neural Attention Network for Financial Forecasting</li>
<li>A Multimodal Variational Encoder-Decoder Framework for Micro-video Popularity Prediction</li>
<li>Multimodal Post Attentive Profiling for Influencer Marketing</li>
<li>TransModality: An End2End Fusion Method with Transformer for Multimodal Sentiment Analysis</li>
</ol>
<h3 id="acm-mm-21">ACM MM 21</h3>
<ol type="1">
<li>Theophany: Multimodal Speech Augmentation in Instantaneous Privacy Channels</li>
<li>Multimodal Asymmetric Dual Learning for Unsupervised Eyeglasses Removal</li>
<li>HetEmotionNet: Two-Stream Heterogeneous Graph Recurrent Neural Network for Multi-modal Emotion Recognition</li>
<li>Graph Convolutional Multi-modal Hashing for Flexible Multimedia Retrieval</li>
<li>A Stepwise Matching Method for Multimodal Image based on Cascaded Network</li>
<li>Learning What and When to Drop: Adaptive Multimodal and Contextual Dynamics for Emotion Recognition in Conversation</li>
<li>Differentiated Learning for Multi-Modal Domain Adaptation</li>
<li>Database-adaptive Re-ranking for Enhancing Cross-modal Image Retrieval</li>
<li>Exploiting BERT For Multimodal Target Sentiment Classification Through Input Space Translation</li>
<li>Pre-training Graph Transformer with Multimodal Side Information for Recommendation</li>
<li>Simplifying Multimodal Emotion Recognition with Single Eye Movement Modality</li>
<li>M3TR: Multi-modal Multi-label Recognition with Transformer</li>
<li>Multimodal Dialog System: Relational Graph-based Context-aware Question Understanding</li>
<li>Towards a Unified Middle Modality Learning for Visible-Infrared Person Re-Identification</li>
<li>ROSITA: Enhancing Vision-and-Language Semantic Alignments via Cross- and Intra-modal Knowledge Integration</li>
<li>Joint-teaching: Learning to Refine Knowledge for Resource-constrained Unsupervised Cross-modal Retrieval</li>
<li>Cross-modal Consensus Network for Weakly Supervised Temporal Action Localization</li>
<li>Searching a Hierarchically Aggregated Fusion Architecture for Fast Multi-Modality Image Fusion</li>
<li>Deep Self-Supervised t-SNE for Multi-modal Subspace Clustering</li>
<li>Multimodal Video Summarization via Time-Aware Transformers</li>
<li>Local Graph Convolutional Networks for Cross-Modal Hashing</li>
<li>Cross-modality Discrepant Interaction Network for RGB-D Salient Object Detection</li>
<li>Conceptual and Syntactical Cross-modal Alignment with Cross-level Consistency for Image-Text Matching</li>
<li>Unsupervised Cross-Modal Distillation for Thermal Infrared Tracking</li>
<li>Understanding Chinese Video and Language via Contrastive Multimodal Pre-Training</li>
<li>Cross-Modal Retrieval and Synthesis (X-MRS): Closing the Modality Gap in Shared Representation Learning</li>
<li>Multimodal Compatibility Modeling via Exploring the Consistent and Complementary Correlations</li>
<li>Missing Data Imputation for Solar Yield Prediction using Temporal Multi-Modal Variational Auto-Encoder</li>
<li>Cross-Modal Recipe Embeddings by Disentangling Recipe Contents and Dish Styles</li>
<li>Cross-modal Self-Supervised Learning for Lip Reading: When Contrastive Learning meets Adversarial Training</li>
<li>Multi-Modal Multi-Instance Learning for Retinal Disease Recognition</li>
<li>Transformer-based Feature Reconstruction Network for Robust Multimodal Sentiment Analysis</li>
<li>Efficient Multi-Modal Fusion with Diversity Analysis</li>
<li>SINGA-Easy: An Easy-to-Use Framework for MultiModal Analysis</li>
<li>CoCo-BERT: Improving Video-Language Pre-training with Contrastive Cross-modal Matching and Denoising</li>
<li>DRDF: Determining the Importance of Different Multimodal Information with Dual-Router Dynamic Framework</li>
<li>MCCN: Multimodal Coordinated Clustering Network for Large-Scale Cross-modal Retrieval</li>
<li>Heterogeneous Feature Fusion and Cross-modal Alignment for Composed Image Retrieval</li>
<li>Exploring Graph-Structured Semantics for Cross-Modal Retrieval</li>
<li>Cross Modal Compression: Towards Human-comprehensible Semantic Compression</li>
<li>Sensor-Augmented Egocentric-Video Captioning with Dynamic Modal Attention</li>
<li>Cascade Cross-modal Attention Network for Video Actor and Action Segmentation from a Sentence</li>
<li>Cross-Modal Joint Prediction and Alignment for Composed Query Image Retrieval</li>
<li>MM-Flow: Multi-modal Flow Network for Point Cloud Completion</li>
<li>Meta Self-Paced Learning for Cross-Modal Matching</li>
<li>Learning Disentangled Factors from Paired Data in Cross-Modal Retrieval: An Implicit Identifiable VAE Approach</li>
<li>Multi-Modal Sarcasm Detection with Interactive In-Modal and Cross-Modal Graphs</li>
<li>Fine-grained Cross-modal Alignment Network for Text-Video Retrieval</li>
<li>Cross-Modal Generalization: Learning in Low Resource Modalities via Meta-Alignment</li>
<li>Hierarchical Multi-Task Learning for Diagram Question Answering with Multi-Modal Transformer</li>
<li>Product-oriented Machine Translation with Cross-modal Cross-lingual Pre-training</li>
<li>Graph Neural Networks for Knowledge Enhanced Visual Representation of Paintings</li>
</ol>
<h3 id="acm-mm-20">ACM MM 20</h3>
<ol type="1">
<li>MM-Hand: 3D-Aware Multi-Modal Guided Hand Generative Network for 3D Hand Pose Synthesis</li>
<li>Cross-Modal Omni Interaction Modeling for Phrase Grounding</li>
<li>VideoIC: A Video Interactive Comments Dataset and Multimodal Multitask Learning for Comments Generation</li>
<li>Multimodal Dialog Systems via Capturing Context-aware Dependencies of Semantic Elements</li>
<li>Semi-supervised Multi-modal Emotion Recognition with Cross-Modal Distribution Matching</li>
<li>Adaptive Multimodal Fusion for Facial Action Units Recognition</li>
<li>Crossing You in Style: Cross-modal Style Transfer from Music to Visual Arts</li>
<li>Incomplete Cross-modal Retrieval with Dual-Aligned Variational Autoencoders</li>
<li>Joint Attribute Manipulation and Modality Alignment Learning for Composing Text and Image to Image Retrieval</li>
<li>Supervised Hierarchical Deep Hashing for Cross-Modal Retrieval</li>
<li>Multi-modal Attentive Graph Pooling Model for Community Question Answer Matching</li>
<li>Towards Modality Transferable Visual Information Representation with Optimal Model Compression</li>
<li>Deep Multimodal Neural Architecture Search</li>
<li>Cross-domain Cross-modal Food Transfer</li>
<li>Finding Achilles’ Heel: Adversarial Attack on Multi-modal Action Recognition</li>
<li>Cross-Modal Relation-Aware Networks for Audio-Visual Event Localization</li>
<li>Learning Deep Multimodal Feature Representation with Asymmetric Multi-layer Fusion</li>
<li>Deep Multi-modality Soft-decoding of Very Low Bit-rate Face Videos</li>
<li>Jointly Cross- and Self-Modal Graph Attention Network for Query-Based Moment Localization</li>
<li>STRONG: Spatio-Temporal Reinforcement Learning for Cross-Modal Video Moment Localization</li>
<li>Improving Intra- and Inter-Modality Visual Relation for Image Captioning</li>
<li>Multimodal Attention with Image Text Spatial Relationship for OCR-Based Image Captioning</li>
<li>ADHD Intelligent Auxiliary Diagnosis System Based on Multimodal Information Fusion (Demo)</li>
<li>A Cross-modality and Progressive Person Search System (Demo)</li>
<li>Multimodal Deep Learning for Social Media Popularity Prediction With Attention Mechanism</li>
<li>Video Relation Detection with Trajectory-aware Multi-modal Features</li>
<li>XlanV Model with Adaptively Multi-Modality Feature Fusing for Video Captioning</li>
<li>A Quantitative Comparison of Different Machine Learning Approaches for Human Spermatozoa Quality Prediction Using Multimodal Datasets</li>
<li>Learning Self-Supervised Multimodal Representations of Human Behaviour</li>
<li>Cross-modal Non-linear Guided Attention and Temporal Coherence in Multi-modal Deep Video Models</li>
<li>Look, Read and Feel: Benchmarking Ads Understanding with Multimodal Multitask Learning</li>
<li>Down to the Last Detail: Virtual Try-on with Fine-grained Details</li>
<li>MEmoR: A Dataset for Multimodal Emotion Reasoning in Videos</li>
<li>Modeling both Intra- and Inter-modal Influence for Real-Time Emotion Detection in Conversations</li>
<li>Transformer-based Label Set Generation for Multi-modal Multi-label Emotion Detection</li>
<li>CM-BERT: Cross-Modal BERT for Text-Audio Sentiment Analysis</li>
<li>Label Embedding Online Hashing for Cross-Modal Retrieval</li>
<li>Class-Aware Modality Mix and Center-Guided Metric Learning for Visible-Thermal Person Re-Identification</li>
<li>RGB2LIDAR: Towards Solving Large-Scale Cross-Modal Visual Localization</li>
<li>MMFL: Multimodal Fusion Learning for Text-Guided Image Inpainting</li>
<li>MISA: Modality-Invariant and -Specific Representations for Multimodal Sentiment Analysis</li>
<li>Multi-modal Cooking Workflow Construction for Food Recipes</li>
<li>Adaptive Temporal Triplet-loss for Cross-modal Embedding Learning</li>
<li>Deep-Modal: Real-Time Impact Sound Synthesis for Arbitrary Shapes</li>
<li>K-armed Bandit based Multi-modal Network Architecture Search for Visual Question Answering</li>
<li>Dynamic Context-guided Capsule Network for Multimodal Machine Translation</li>
<li>KBGN: Knowledge-Bridge Graph Network for Adaptive Vision-Text Reasoning in Visual Dialogue</li>
<li>Learning Modality-Invariant Latent Representations for Generalized Zero-shot Learning</li>
<li>Boosting Continuous Sign Language Recognition via Cross Modality Augmentation</li>
<li>Towards More Explainability: Concept Knowledge Mining Network for Event Recognition</li>
<li>Memory-Based Network for Scene Graph with Unbalanced Relations</li>
</ol>
<h3 id="neurips-21">NeurIPS 21</h3>
<ol type="1">
<li>Exploring Cross-Video and Cross-Modality Signals for Weakly-Supervised Audio-Visual Video Parsing</li>
<li>Cross-Modal Domain Adaptation for Cost-Efficient Visual Reinforcement Learning</li>
<li>End-to-end Multi-modal Video Temporal Grounding</li>
<li>Multi-modal Dependency Tree for Video Captioning</li>
<li>Perceptual Score: What Data Modalities Does Your Model Perceive?</li>
<li>UFC-BERT: Unifying Multi-Modal Controls for Conditional Image Synthesis</li>
<li>Learning with Noisy Correspondence for Cross-modal Matching</li>
<li>Probing Inter-modality: Visual Parsing with Self-Attention for Vision-Language Pre-training</li>
<li>Modality-Agnostic Topology Aware Localization</li>
<li>Explainable Semantic Space by Grounding Language to Vision with Cross-Modal Contrastive Learning</li>
<li>Raw Nav-merge Seismic Data to Subsurface Properties with MLP based Multi-Modal Information Unscrambler</li>
<li>What Makes Multi-modal Learning Better than Single (Provably)</li>
</ol>
<h3 id="neurips-020">NeurIPS 020</h3>
<ol type="1">
<li>Labelling unlabelled videos from scratch with multi-modal self-supervision</li>
<li>Self-Supervised Learning by Cross-Modal Audio-Video Clustering</li>
<li>CodeCMR: Cross-Modal Retrieval For Function-Level Binary Source Code Matching</li>
<li>A Contour Stochastic Gradient Langevin Dynamics Algorithm for Simulations of Multi-modal Distributions</li>
<li>An implicit function learning approach for parametric modal regression</li>
<li>Removing Bias in Multi-modal Classifiers: Regularization by Maximizing Functional Entropies</li>
</ol>
<h3 id="aaai-22">AAAI 22</h3>
<ol type="1">
<li>Event-Image Fusion Stereo Using Cross-Modality Feature Propagation</li>
<li>Cross-Modal Mutual Learning for Audio-Visual Speech Recognition and Manipulation</li>
<li>Cross-Modal Federated Human Activity Recognition via Modality-Agnostic and Modality-Specific Representation Learning</li>
<li>Detecting Human-Object Interactions with Object-Guided Cross-Modal Calibrated Semantics</li>
<li>Show Your Faith: Cross-Modal Conﬁdence-Aware Network for Image-Text Matching</li>
<li>Modality-Adaptive Mixup and Invariant Decomposition for RGB-Infrared Person Re-identiﬁcation</li>
<li>MuMu: Cooperative Multitask Learning-Based Guided Multimodal Fusion</li>
<li>Cross-Modal Object Tracking: Modality-Aware Representations and A Uniﬁed Benchmark</li>
<li>You Only Infer Once: Cross-Modal Meta-Transfer for Referring Video Object Segmentation</li>
<li>Multi-Modal Perception Attention Network with Self-Supervised Learning for Audio-Visual Speaker Tracking</li>
<li>Visual Sound Localization in the Wild by Cross-Modal Interference Erasing</li>
<li>TVT: Three-Way Vision Transformer through Multi-Modal Hypersphere Learning for Zero-Shot Sketch-Based Image Retrieval</li>
<li>Interact, Embed, and EnlargE: Boosting Modality-Specific Representations for Multi-Modal Person Re-identification</li>
<li>MAGIC: Multimodal relAtional Graph adversarIal inferenCe for Diverse and Unpaired Text-Based Image Captioning</li>
<li>Promoting Single-Modal Optical Flow Network for Diverse Cross-Modal Flow Estimation</li>
<li>Event-Aware Multimodal Mobility Nowcasting</li>
<li>Online Enhanced Semantic Hashing: Towards Effective and Efficient Retrieval for Streaming Multi-Modal Data</li>
<li>AXM-Net: Implicit Cross-Modal Feature Alignment for Person Re-identiﬁcation</li>
<li>Monocular Camera-Based Point-Goal Navigation by Learning Depth Channel and Cross-Modality Pyramid Fusion</li>
<li>Multimodal Adversarially Learned Inference with Factorized Discriminators</li>
<li>Learning Aligned Cross-Modal Representation for Generalized Zero-Shot Classification</li>
<li>Regularized Modal Regression on Markov-Dependent Observations: A Theoretical Assessment</li>
<li>Multi-Head Modularization to Leverage Generalization Capability in Multi-Modal Networks</li>
<li>BM-NAS: Bilevel Multimodal Neural Architecture Search</li>
<li>Tailor Versatile Multi-Modal Learning for Multi-Label Emotion Recognition</li>
<li>Bi-CMR: Bidirectional Reinforcement Guided Hashing for Effective Cross-Modal Retrieval</li>
<li>Cross-Modal Coherence for Text-to-Image Retrieval</li>
<li>Nice Perfume. How Long Did You Marinate in It? Multimodal Sarcasm Explanation</li>
<li>Are Vision-Language Transformers Learning Multimodal Representations? A Probing Perspective</li>
<li>Hierarchical Cross-Modality Semantic Correlation Learning Model for Multimodal Summarization</li>
<li>UniMS: A Uniﬁed Framework for Multimodal Summarization with Knowledge Distillation</li>
<li>Evaluating Explainable AI on a Multi-Modal Medical Imaging Task: Can Existing Algorithms Fulfill Clinical Requirements?</li>
<li>Sentiment and Emotion-Aware Multi-Modal Complaint Identiﬁcation</li>
<li>D-vlog: Multimodal Vlog Dataset for Depression Detection</li>
<li>An End-to-End Traditional Chinese Medicine Constitution Assessment System Based on Multimodal Clinical Feature Representation and Fusion</li>
<li>ALLURE * : A Multi-Modal Guided Environment for Helping Children Learn to Solve a Rubik’s Cube with Automatic Solving and Interactive Explanations</li>
<li>A Multimodal Fusion-Based LNG Detection for Monitoring Energy Facilities (Student Abstract)</li>
<li>Using Multimodal Data and AI to Dynamically Map Flood Risk</li>
<li>College Student Retention Risk Analysis from Educational Database Using Multi-Task Multi-Modal Neural Fusion</li>
</ol>
<h3 id="aaai-21">AAAI 21</h3>
<ol type="1">
<li>Embracing Domain Differences in Fake News: Cross-domain Fake News Detection using Multi-modal Data</li>
<li>Dynamic Graph Representation Learning for Video Dialog via Multi-Modal Shufﬂed Transformers</li>
<li>SMIL: Multimodal Learning with Severely Missing Modality</li>
<li>CHEF: Cross-Modal Hierarchical Embeddings for Food Domain Retrieval</li>
<li>Dual Adversarial Graph Neural Networks for Multi-label Cross-modal Retrieval</li>
<li>Deep Probabilistic Imaging: Uncertainty Quantiﬁcation and Multi-modal Solution Characterization for Computational Imaging</li>
<li>Efficient Object-Level Visual Context Modeling for Multimodal Machine Translation: Masking Irrelevant Objects Helps Grounding</li>
<li>Conﬁdence-aware Non-repetitive Multimodal Transformers for TextCaps</li>
<li>Amodal Segmentation Based on Visible Region Segmentation and Shape Prior</li>
<li>Multimodal Fusion via Teacher-Student Network for Indoor Action Recognition</li>
<li>Demodalizing Face Recognition with Synthetic Samples</li>
<li>Joint Color-irrelevant Consistency Learning and Identity-aware Modality Adaptation for Visible-infrared Cross Modality Person Re-identiﬁcation</li>
<li>Robust Multi-Modality Person Re-identiﬁcation</li>
<li>Deep Graph-neighbor Coherence Preserving Network for Unsupervised Cross-modal Hashing</li>
<li>Learning Intuitive Physics with Multimodal Generative Models</li>
<li>VMLoc: Variational Fusion For Learning-Based Multimodal Camera Localization</li>
<li>Noise Estimation Using Density Estimation for Self-Supervised Multimodal Learning</li>
<li>Deep Mutual Information Maximin for Cross-Modal Clustering</li>
<li>MUFASA: Multimodal Fusion Architecture Search for Electronic Health Records</li>
<li>Enhanced Audio Tagging via Multi- to Single-Modal Teacher-Student Mutual Learning</li>
<li>Learning Modality-Speciﬁc Representations with Self-Supervised Multi-Task Learning for Multimodal Sentiment Analysis</li>
<li>Theoretical Analyses of Multi-Objective Evolutionary Algorithms on Multi-Modal Objectives</li>
<li>Humor Knowledge Enriched Transformer for Understanding Multimodal Humor</li>
<li>Audio-Oriented Multimodal Machine Comprehension via Dynamic Inter- and Intra-modality Attention</li>
<li>MELINDA: A Multimodal Dataset for Biomedical Experiment Method Classification</li>
<li>Multi-modal Multi-label Emotion Recognition with Heterogeneous Hierarchical Message Passing</li>
<li>LAMS: A Location-aware Approach for Multimodal Summarization (Student Abstract)</li>
<li>Fashion Focus: Multi-modal Retrieval System for Video Commodity Localization in E-commerce</li>
<li>Data-Driven Multimodal Patrol Planning for Anti-poaching</li>
<li>Screening for Depressed Individuals by Using Multimodal Social Media Data</li>
<li>Multi-modal User Intent Classiﬁcation Under the Scenario of Smart Factory (Student Abstract)</li>
</ol>
<h3 id="aaai-20">AAAI 20</h3>
<ol type="1">
<li>Infrared-Visible Cross-Modal Person Re-Identiﬁcation with an X Modality</li>
<li>MANYMODAL QA: Modality Disambiguation and QA over Diverse Inputs</li>
<li>Privacy Enhanced Multimodal Neural Representations for Emotion Recognition</li>
<li>Modality-Balanced Models for Visual Dialogue</li>
<li>Aspect-Aware Multimodal Summarization for Chinese E-Commerce Products</li>
<li>Semi-Supervised Multi-Modal Learning with Balanced Spectral Decomposition</li>
<li>Modality to Modality Translation: An Adversarial Representation Learning and Graph Fusion Network for Multimodal Fusion</li>
<li>Cross-Modal Attention Network for Temporal Inconsistent Audio-Visual Event Localization</li>
<li>Crisis-DIAS: Towards Multimodal Damage Analysis - Deployment, Challenges and Assessment</li>
<li>Towards Cross-Modality Medical Image Segmentation with Online Mutual Knowledge Distillation</li>
<li>Learning Multi-Modal Biomarker Representations via Globally Aligned Longitudinal Enrichments</li>
<li>Urban2Vec: Incorporating Street View Imagery and POIs for Multi-Modal Urban Neighborhood Embedding</li>
<li>M3ER: Multiplicative Multimodal Emotion Recognition using Facial, Textual, and Speech Cues</li>
<li>Cross-Modal Subspace Clustering via Deep Canonical Correlation Analysis</li>
<li>Learning Relationships between Text, Audio, and Video via Deep Canonical Correlation for Multimodal Language Analysis</li>
<li>Visual Agreement Regularized Training for Multi-Modal Machine Translation</li>
<li>Learning Long- and Short-Term User Literal-Preference with Multimodal Hierarchical Transformer Network for Personalized Image Caption</li>
<li>Multimodal Summarization with Guidance of Multimodal Reference</li>
<li>Factorized Inference in Deep Markov Models for Incomplete Multimodal Time Series</li>
<li>MULE: Multimodal Universal Language Embedding</li>
<li>Unicoder-VL: A Universal Encoder for Vision and Language by Cross-Modal Pre-Training</li>
<li>Attention-Based Multi-Modal Fusion Network for Semantic Scene Completion</li>
<li>Multimodal Structure-Consistent Image-to-Image Translation</li>
<li>Learning Cross-Aligned Latent Embeddings for Zero-Shot Cross-Modal Retrieval</li>
<li>Learning Cross-Modal Context Graph for Visual Grounding</li>
<li>Multimodal Interaction-Aware Trajectory Prediction in Crowded Space</li>
<li>Cross-Modality Paired-Images Generation for RGB-Infrared Person Re-Identification</li>
<li>Adaptive Cross-Modal Embeddings for Image-Text Alignment</li>
<li>Mining on Heterogeneous Manifolds for Zero-Shot Cross-Modal Image Retrieval</li>
<li>Cross-Modality Attention with Semantic Graph Embedding for Multi-Label Classification</li>
<li>Adaptive Unimodal Cost Volume Filtering for Deep Stereo Matching</li>
<li>Diana’s World: A Situated Multimodal Interactive Agent</li>
<li>Interpreting Multimodal Machine Learning Models Trained for Emotion Recognition to Address Robustness and Privacy Concerns</li>
<li>Trimodal Attention Module for Multimodal Sentiment Analysis (Student Abstract)</li>
<li>SpotFake+: A Multimodal Framework for Fake News Detection via Transfer Learning (Student Abstract)</li>
</ol>
<h3 id="sigir-22">SIGIR 22</h3>
<ol type="1">
<li>CRET: Cross-Modal Retrieval Transformer for Efficient Text-Video Retrieval</li>
<li>Multimodal Disentanglement Variational AutoEncoders for Zero-Shot Cross-Modal Retrieval</li>
<li>Bit-aware Semantic Transformer Hashing for Multi-modal Retrieval</li>
<li>V2P: Vision-to-Prompt based Multi-Modal Product Summary Generation</li>
<li>Progressive Learning for Image Retrieval with Hybrid-Modality Queries</li>
<li>Tag-assisted Multimodal Sentiment Analysis under Uncertain Missing Modalities</li>
<li>A Multitask Framework for Sentiment, Emotion and Sarcasm aware Cyberbullying Detection from Multi-modal Code-Mixed Memes</li>
<li>Multi-modal Graph Contrastive Learning for Micro-video Recommendation</li>
<li>Cross-Probe BERT for Fast Cross-Modal Search</li>
<li>MM-Rec: Visiolinguistic Model Empowered Multimodal News Recommendation</li>
<li>Modality-Balanced Embedding for Video Retrieval</li>
<li>An Efficient Fusion Mechanism for Multimodal Low-resource Setting</li>
<li>Next Point-of-Interest Recommendation with Auto-Correlation Enhanced Multi-Modal Transformer Network</li>
<li>MET-Meme: a Multimodal Meme Dataset Rich in Metaphors</li>
<li>MuMiN: A Large-Scale Multilingual Multimodal Fact-Checked Misinformation Social Network Dataset</li>
<li>Golden Retriever: A Real-Time Multi-Modal Text-Image Retrieval System with the Ability to Focus</li>
<li>An Intelligent Advertisement Short Video Production System via Multi-Modal Retrieval</li>
</ol>
<h3 id="sigir-21">SIGIR 21</h3>
<ol type="1">
<li>DepressionNet: A Novel Summarization Boosted Deep Framework for Depression Detection on Social Media</li>
<li>Hierarchical Multi-modal Contextual Attention Network for Fake News Detection</li>
<li>Hybrid Fusion with Intra- and Cross-Modality Attention for Image-Recipe Retrieval</li>
<li>Multimodal Activation: Awakening Dialog Robots without Wake Words</li>
<li>Privacy Protection in Deep Multi-modal Retrieval</li>
<li>MMConv: An Environment for Multimodal Conversational Search across Multiple Domains</li>
<li>Multi-Modal Supplementary-Complementary Summarization using Multi-Objective Optimization</li>
<li>Dynamic Modality Interaction Modeling for Image-Text Retrieval</li>
<li>Hierarchical Cross-Modal Graph Consistency Learning for Video-Text Retrieval</li>
<li>PAN: Prototype-based Adaptive Network for Robust Cross-modal Retrieval</li>
<li>Heterogeneous Attention Network for Effective and Efficient Cross-modal Retrieval</li>
<li>Towards Multi-Modal Conversational Information Seeking</li>
<li>FedCMR: Federated Cross-Modal Retrieval</li>
<li>Cross-Graph Attention Enhanced Multi-Modal Correlation Learning for Fine-Grained Image-Text Retrieval</li>
<li>Deep Music Retrieval for Fine-Grained Videos by Exploiting Cross-Modal-Encoded Voice-Overs</li>
<li>AliMe Avatar: Multi-modal Content Production and Presentation for Live-streaming E-commerce</li>
<li>QuTI! Quantifying Text-Image Consistency in Multimodal Documents</li>
</ol>
<h3 id="sigir-20">SIGIR 20</h3>
<ol type="1">
<li>Fashion Compatibility Modeling through a Multi-modal Try-on-guided Scheme</li>
<li>Tree-Augmented Cross-Modal Encoding for Complex-Query Video Retrieval</li>
<li>Nonlinear Robust Discrete Hashing for Cross-Modal Retrieval</li>
<li>Joint-modal Distribution-based Similarity Hashing for Large-scale Unsupervised Deep Cross-modal Retrieval</li>
<li>Web Table Retrieval using Multimodal Deep Learning</li>
<li>Correlated Features Synthesis and Alignment for Zero-shot Cross-modal Retrieval</li>
<li>MGNN: A Multimodal Graph Neural Network for Predicting the Survival of Cancer Patients</li>
<li>Multi-Modal Summary Generation using Multi-Objective Optimization</li>
<li>Multi-Level Multimodal Transformer Network for Multimodal Recipe Comprehension</li>
<li>MHM: Multi-modal Clinical Data based Hierarchical Multi-label Diagnosis Prediction</li>
<li>FashionBERT: Text and Image Matching with Adaptive Loss for Cross-modal Retrieval</li>
</ol>
<h3 id="cvpr-22">CVPR 22</h3>
<ol type="1">
<li>Cross-modal Map Learning for Vision and Language Navigation</li>
<li>PhoCaL: A Multi-Modal Dataset for Category-Level Object Pose Estimation with Photometrically Challenging Objects</li>
<li>Everything at Once – Multi-modal Fusion Transformer for Video Retrieval</li>
<li>CrossPoint: Self-Supervised Cross-Modal Contrastive Learning for 3D Point Cloud Understanding</li>
<li>Versatile Multi-Modal Pre-Training for Human-Centric Perception</li>
<li>Lite-MDETR: A Lightweight Multi-Modal Detector</li>
<li>Cross Modal Retrieval with Querybank Normalisation</li>
<li>Modeling Motion with Multi-Modal Features for Text-Based Video Segmentation</li>
<li>Tencent-MVSE: A Large-Scale Benchmark Dataset for Multi-Modal Video Similarity Evaluation</li>
<li>Open-Vocabulary Instance Segmentation via Robust Cross-Modal Pseudo-Labeling</li>
<li>RFNet: Unsupervised Network for Mutually Reinforcing Multi-modal Image Registration and Fusion</li>
<li>EI-CLIP: Entity-aware Interventional Contrastive Learning for E-commerce Cross-modal Retrieval</li>
<li>Learning Hierarchical Cross-Modal Association for Co-Speech Gesture Generation</li>
<li>X-Trans2Cap: Cross-Modal Knowledge Transfer using Transformer for 3D Dense Captioning</li>
<li>MM-TTA: Multi-Modal Test-Time Adaptation for 3D Semantic Segmentation</li>
<li>Interact before Align: Leveraging Cross-Modal Knowledge for Domain Adaptive Action Recognition</li>
<li>Cross-Modal Transferable Adversarial Attacks from Images to Videos</li>
<li>Open-Domain, Content-based, Multi-modal Fact-checking of Out-of-Context Images via Online Resources</li>
<li>Cross-Modal Perceptionist: Can Face Geometry be Gleaned from Voices?</li>
<li>Cross-modal Representation Learning for Zero-shot Action Recognition</li>
<li>Audio-visual Generalised Zero-shot Learning with Cross-modal Attention and Language</li>
<li>Robust Cross-Modal Representation Learning with Progressive Self-Distillation</li>
<li>Multi-modal Alignment using Representation Codebook</li>
<li>Wnet: Audio-Guided Video Object Segmentation via Wavelet-Based Cross-Modal Denoising Networkss</li>
<li>Text2Pos: Text-to-Point-Cloud Cross-Modal Localization</li>
<li>Reading to Listen at the Cocktail Party: Multi-Modal Speech Separation</li>
<li>Cross-modal Background Suppression for Audio-Visual Event Localization</li>
<li>Mutual Quantization for Cross-Modal Search with Noisy Labels</li>
<li>Learning Modal-Invariant and Temporal-Memory for Video-based Visible-Infrared Person Re-Identification</li>
<li>Multi-modal Extreme Classification</li>
<li>COTS: Collaborative Two-Stream Vision-Language Pre-Training Model for Cross-Modal Retrieval</li>
<li>ViSTA: Vision and Scene Text Aggregation for Cross-Modal Retrieval</li>
<li>CroMo: Cross-Modal Learning for Monocular Depth Estimation</li>
<li>CAT-Det: Contrastively Augmented Transformer for Multi-modal 3D Object Detection</li>
<li>Beyond a Pre-Trained Object Detector: Cross-Modal Textual and Visual Context for Image Captioning</li>
<li>X-Pool: Cross-Modal Language-Video Attention for Text-Video Retrieval</li>
<li>UMT: Unified Multi-modal Transformers for Joint Video Moment Retrieval and Highlight Detection</li>
<li>Multi-Modal Dynamic Graph Transformer for Visual Grounding</li>
<li>DeepFusion: Lidar-Camera Deep Fusion for Multi-Modal 3D Object Detection</li>
<li>M3L: Language-based Video Editing via Multi-Modal Multi-Level Transformers</li>
<li>Generalizable Cross-modality Medical Image Segmentation via Style Augmentation and Dual Normalization</li>
<li>Weakly Paired Associative Learning for Sound and Image Representations via Bimodal Associative Memory</li>
<li>ADAPT: Vision-Language Navigation with Modality-Aligned Action Prompts</li>
<li>Dual-Key Multimodal Backdoors for Visual Question Answering</li>
<li>CrossLoc: Scalable Aerial Localization Assisted by Multimodal Synthetic Data</li>
<li>Learning based Multi-modality Image and Video Compression</li>
<li>Modality-Agnostic Learning for Radar-Lidar Fusion in Vehicle Detection</li>
<li>WALT: Watch And Learn 2D amodal representation from Time-lapse imagery</li>
<li>Towards Multimodal Depth Estimation from Light Fields</li>
<li>End-to-End Referring Video Object Segmentation with Multimodal Transformers</li>
<li>FMCNet: Feature-Level Modality Compensation for Visible-Infrared Person Re-Identification</li>
<li>Show Me What and Tell Me How: Video Synthesis via Multimodal Conditioning</li>
<li>Amodal Segmentation through Out-of-Task and Out-of-Distribution Generalization with a Bayesian Model</li>
<li>Unimodal-Concentrated Loss: Fully Adaptive Label Distribution Learning for Ordinal Regression</li>
<li>Expanding Large Pre-trained Unimodal Models with Multimodal Information Injection for Image-Text Multimodal Classification</li>
<li>End-to-end Generative Pretraining for Multimodal Video Captioning</li>
<li>XMP-Font: Self-Supervised Cross-Modality Pre-training for Few-Shot Font Generation</li>
<li>Multimodal Dynamics: Dynamical Fusion for Trustworthy Multimodal Classification</li>
<li>The Auto Arborist Dataset: A Large-Scale Benchmark for Multiview Urban Forest Monitoring Under Domain Shift</li>
<li>MNSRNet: Multimodal Transformer Network for 3D Surface Super-Resolution</li>
<li>OMNIVORE: A Single Model for Many Visual Modalities</li>
<li>Motron: Multimodal Probabilistic Human Motion Forecasting</li>
<li>Amodal Panoptic Segmentation</li>
<li>Multimodal Colored Point Cloud to Image Alignment</li>
<li>Boosting 3D Object Detection by Simulating Multimodality on Point Clouds</li>
<li>ContIG: Self-supervised Multimodal Contrastive Learning for Medical Imaging with Genetics</li>
<li>Egocentric Scene Understanding via Multimodal Spatial Rectifier</li>
<li>Are Multimodal Transformers Robust to Missing Modality?</li>
<li>A Simple Multi-Modality Transfer Learning Baseline for Sign Language Translation</li>
<li>Multimodal Material Segmentation</li>
<li>Target-aware Dual Adversarial Learning and a Multi-scenario Multi-Modality Benchmark to Fuse Infrared and Visible for Object Detection</li>
<li>Balanced Multimodal Learning via On-the-fly Gradient Modulation</li>
<li>Multimodal Token Fusion for Vision Transformers</li>
<li>Learnable Irrelevant Modality Dropout for Multimodal Action Recognition on Modality-Specific Annotated Videos</li>
<li>XYLayoutLM: Towards Layout-Aware Multimodal Networks For Visually-Rich Document Understanding</li>
<li>STCrowd: A Multimodal Dataset for Pedestrian Perception in Crowded Scenes</li>
<li>3MASSIV: Multilingual, Multimodal and Multi-Aspect dataset of Social Media Short Videos</li>
<li>M5Product: Self-harmonized Contrastive Learning for E-commercial Multi-modal Pretraining. CVPR 22. <a href="https://xiaodongsuper.github.io/M5Product_dataset/">代码</a>
<ul>
<li>作者创建了一个包括了音频、图像、属性、视频等信息的大规模多模态数据集。虽然从全文来看没有声明是一个MMKG，但是如果从广义的角度看，也是属于KG</li>
</ul></li>
<li>VisualHow: Multimodal Problem Solving</li>
<li>WebQA: Multihop and Multimodal QA</li>
<li>Query and Attention Augmentation for Knowledge-Based Explainable Reasoning</li>
<li>Open-Vocabulary One-Stage Detection with Hierarchical Visual-Language Knowledge Distillation</li>
</ol>
<h3 id="cvpr-21">CVPR 21</h3>
<ol type="1">
<li>Shared Cross-Modal Trajectory Prediction for Autonomous Driving</li>
<li>Robust Multimodal Vehicle Detection in Foggy Weather Using Complementary Lidar and Radar Signals</li>
<li>EvDistill: Asynchronous Events to End-task Learning via Bidirectional Reconstruction-guided Cross-modal Knowledge Distillation</li>
<li>Cross-Modal Contrastive Learning for Text-to-Image Generation</li>
<li>Looking into Your Speech: Learning Cross-modal Affinity for Audio-visual Speech Separation</li>
<li>Deep RGB-D Saliency Detection with Depth-Sensitive Attention and Automatic Multi-Modal Fusion</li>
<li>Farewell to Mutual Information: Variational Distillation for Cross-Modal Person Re-Identification</li>
<li>Multi-Modal Relational Graph for Cross-Modal Video Moment Retrievals</li>
<li>Progressive Modality Reinforcement for Human Multimodal Emotion Recognition from Unaligned Multimodal Sequences</li>
<li>ABMDRNet: Adaptive-weighted Bi-directional Modality Difference Reduction Network for RGB-T Semantic Segmentation</li>
<li>How2Sign: A Large-scale Multimodal Dataset for Continuous American Sign Language</li>
<li>Cross-Modal Center Loss for 3D Cross-Modal Retrieval</li>
<li>Defending Multimodal Fusion Models against Single-Source Adversaries</li>
<li>StEP: Style-based Encoder Pre-training for Multi-modal Image Synthesiss</li>
<li>M3P: Learning Universal Representations via Multitask Multilingual Multimodal Pre-training</li>
<li>UC2 : Universal Cross-lingual Cross-modal Vision-and-Language Pre-training</li>
<li>Discover Cross-Modality Nuances for Visible-Infrared Person Re-Identification</li>
<li>Cross-Modal Collaborative Representation Learning and a Large-Scale RGBT Benchmark for Crowd Counting</li>
<li>Learning Cross-Modal Retrieval with Noisy Labels</li>
<li>Can audio-visual integration strengthen robustness under multimodal attacks?</li>
<li>Single Pair Cross-Modality Super Resolution</li>
<li>Multimodal Contrastive Training for Visual Representation Learning</li>
<li>VX2TEXT: End-to-End Learning of Video-Based Text Generation From Multimodal Inputs</li>
<li>Multi-Modal Fusion Transformer for End-to-End Autonomous Driving</li>
<li>Multimodal Motion Prediction with Stacked Transformers</li>
<li>Cross Modal Focal Loss for RGBD Face Anti-Spoofing</li>
<li>Probabilistic Embeddings for Cross-Modal Retrieval</li>
<li>There is More than Meets the Eye: Self-Supervised Multi-Object Detection and Tracking with Sound by Distilling Multimodal Knowledge</li>
<li>Audio-Visual Instance Discrimination with Cross-Modal Agreement</li>
<li>Learning from the Master: Distilling Cross-modal Advanced Knowledge for Lip Reading</li>
<li>LaPred: Lane-Aware Prediction of Multi-Modal Future Trajectories of Dynamic Agents</li>
<li>Adaptive Cross-Modal Prototypes for Cross-Domain Visual-Language Retrieval</li>
<li>Revamping Cross-Modal Recipe Retrieval with Hierarchical Transformers and Self-supervised Learning</li>
<li>VISUALVOICE: Audio-Visual Speech Separation with Cross-Modal Consistency</li>
<li>Deep Lucas-Kanade Homography for Multimodal Image Alignment</li>
<li>Distilling Audio-Visual Knowledge by Compositional Contrastive Learning</li>
<li>Improving Weakly Supervised Visual Grounding by Contrastive Knowledge Distillation</li>
<li>Amalgamating Knowledge from Heterogeneous Graph Neural Networks</li>
</ol>
<h3 id="cvpr-20">CVPR 20</h3>
<ol type="1">
<li>Multi-Modal Domain Adaptation for Fine-Grained Action Recognition</li>
<li>Creating Something from Nothing: Unsupervised Knowledge Distillation for Cross-Modal Hashing</li>
<li>Multimodal Future Localization and Emergence Prediction for Objects in Egocentric View With a Reachability Prior</li>
<li>Cross-modal Deep Face Normals with Deactivable Skip Connections</li>
<li>Monocular Real-time Hand Shape and Motion Capture using Multi-modal Data</li>
<li>Semantically Multi-modal Image Synthesis</li>
<li>Knowledge as Priors: Cross-Modal Knowledge Generalization for Datasets without Superior Knowledge</li>
<li>Cross-Modal Pattern-Propagation for RGB-T Tracking</li>
<li>Iterative Answer Prediction with Pointer-Augmented Multimodal Transformers for TextVQA</li>
<li>Modality Shifting Attention Network for Multi-modal Video Question Answering</li>
<li>A Local-to-Global Approach to Multi-modal Movie Scene Segmentation</li>
<li>Hi-CMD: Hierarchical Cross-Modality Disentanglement for Visible-Infrared Person Re-Identification</li>
<li>Speech2Action: Cross-modal Supervision for Action Recognition</li>
<li>Solving Mixed-modal Jigsaw Puzzle for Fine-Grained Sketch-Based Image Retrieval</li>
<li>Referring Image Segmentation via Cross-Modal Progressive Comprehension</li>
<li>Cross-Modal Cross-Domain Moment Alignment Network for Person Search</li>
<li>Vision-Dialog Navigation by Exploring Cross-modal Memory</li>
<li>A Real-Time Cross-modality Correlation Filtering Method for Referring Expression Comprehension</li>
<li>Multi-Modality Cross Attention Network for Image and Sentence Matchings</li>
<li>nuScenes: A multimodal dataset for autonomous driving</li>
<li>Seeing Through Fog Without Seeing Fog: Deep Multimodal Sensor Fusion in Unseen Adverse Weather</li>
<li>End-to-End Adversarial-Attention Network for Multi-Modal Clustering</li>
<li>xMUDA: Cross-Modal Unsupervised Domain Adaptation for 3D Semantic Segmentation</li>
<li>IMRAM: Iterative Matching with Recurrent Attention Memory for Cross-Modal Image-Text Retrieval∗</li>
<li>What Makes Training Multi-modal Classification Networks Hard?</li>
<li>Multi-Modal Graph Neural Network for Joint Reasoning on Vision and Scene Texts</li>
<li>Universal Weighting Metric Learning for Cross-Modal Matching</li>
<li>MMTM: Multimodal Transfer Module for CNN Fusion</li>
<li>Cross-Modality Person Re-Identification With Shared-Specific Feature Transfer</li>
<li>Unsupervised Multi-Modal Image Registration via Geometry Preserving Image-to-Image Translation</li>
<li>CoverNet: Multimodal Behavior Prediction using Trajectory Sets</li>
<li>EmotiCon: Context-Aware Multimodal Emotion Recognition using Frege’s Principle</li>
<li>Discriminative Multi-Modality Speech Recognition</li>
<li>MCEN: Bridging Cross-Modal Gap between Cooking Recipes and Dish Images with Latent Variable Model</li>
<li>Hypergraph Attention Networks for Multimodal Learning</li>
<li>Multimodal Categorization of Crisis Events in Social Media</li>
<li>Transform and Tell: Entity-Aware News Image Captioning</li>
</ol>
<h3 id="kdd-22">KDD 22</h3>
<ol type="1">
<li>Graph-Flashback Network for Next Location Recommendation</li>
<li>ERNIE-GeoL: A Geography-and-Language Pre-trained Model and its Applications in Baidu Maps</li>
<li>Graph Neural Networks for Multimodal Single-Cell Data Integration</li>
<li>External Knowledge Infusion for Tabular Pre-training Models with Dual-adapters</li>
</ol>
<h3 id="kdd-21">KDD 21</h3>
<ol type="1">
<li>Web-Scale Generic Object Detection at Microsoft Bing</li>
<li>Cross-Network Learning with Partially Aligned Graph Convolutional Networks</li>
<li>Triplet Attention: Rethinking the similarity in Transformers</li>
</ol>
<h3 id="iccv">ICCV</h3>
<ol type="1">
<li>VLG-Net: Video-Language Graph Matching Network for Video Grounding. ICCV 21</li>
<li>Visual-Textual Attentive Semantic Consistency for Medical Report Generation. ICCV 21</li>
<li>Public Life in Public Space (PLPS): A multi-task, multi-group video dataset for public life research. ICCV 21</li>
<li>Self-Motivated Communication Agent for Real-World Vision-Dialog Navigation. ICCV 21</li>
<li>VrR-VG: Refocusing Visually-Relevant Relationships. ICCV 19</li>
<li>Concept Generalization in Visual Representation Learning. ICCV 21</li>
<li>DocFormer: End-to-End Transformer for Document Understanding. ICCV 21</li>
<li>Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models. ICCV 15</li>
<li>Virtual Multi-Modality Self-Supervised Foreground Matting for Human-Object Interaction. ICCV 21</li>
<li>Smile, Be Happy :) Emoji Embedding for Visual Sentiment Analysis. ICCV Workshop 19</li>
<li>Explain Me the Painting: Multi-Topic Knowledgeable Art Description Generation. ICCV 19</li>
</ol>
<h3 id="eccv">ECCV</h3>
<ol type="1">
<li>VisualCOMET: Reasoning About the Dynamic Context of a Still Image. ECCV 20</li>
<li>Fashionpedia: Ontology, Segmentation, and an Attribute Localization Dataset. ECCV 20</li>
<li>Learning Type-Aware Embeddings for Fashion Compatibility. ECCV 18</li>
<li>A Dataset and Baselines for Visual Question Answering on Art. ECCV 20 Workshop</li>
<li>Learning to Scale Multilingual Representations for Vision-Language Tasks. ECCV 20</li>
<li>MaxViT: Multi-Axis Vision Transformer. ECCV 22</li>
</ol>
<h3 id="cikm">CIKM</h3>
<ol type="1">
<li>SciClops: Detecting and Contextualizing Scientific Claims for Assisting Manual Fact-Checking. CIKM 21</li>
<li>WebKE: Knowledge Extraction from Semi-structured Web with Pre-trained Markup Language Model. CIKM 21</li>
<li>MLM: A Benchmark Dataset for Multitask Learning with Multiple Languages and Modalities. CIKM 20</li>
<li>Multi-modal Dictionary BERT for Cross-modal Video Search in Baidu Advertising. CIKM 20</li>
<li>IMAS++: An Intelligent Medical Analysis System Enhanced with Deep Graph Neural Networks. CIKM 21</li>
<li>Recipe Representation Learning with Networks. CIKM 21</li>
<li>Student Can Also be a Good Teacher: Extracting Knowledge from Vision-and-Language Model for Cross-Modal Retrieval. CIKM 21</li>
<li>Improving Chinese Character Representation with Formation Graph Attention Network. CIKM 21</li>
<li>VidLife: A Dataset for Life Event Extraction from Videos. CIKM 21</li>
<li>Learning Chinese Word Embeddings from Stroke, Structure and Pinyin of Characters. CIKM 19</li>
</ol>
<h3 id="wsdm">WSDM</h3>
<ol type="1">
<li>Representation Interpretation with Spatial Encoding and Multimodal Analytics. WSDM 19</li>
<li>Beyond Statistical Relations: Integrating Knowledge Relations into Style Correlations for Multi-Label Music Style Classification. WSDM 20</li>
<li>Speaker and Time-aware Joint Contextual Learning for Dialogue-act Classification in Counselling Conversations. WSDM 22.</li>
<li>VISIR: Visual and Semantic Image Label Refinement. WSDM 18</li>
<li>Product Knowledge Graph Embedding for E-commerce. WSDM 20</li>
</ol>
<h3 id="icmr">ICMR</h3>
<ol type="1">
<li>Know Yourself and Know Others: Efficient Common Representation Learning for Few-shot Cross-modal Retrieval. ICMR 21</li>
<li>Personal Knowledge Base Construction from Multimodal Data. ICMR 21</li>
<li>Image Emotion Distribution Learning with Graph Convolutional Network. ICMR 19</li>
<li>HSGMP: Heterogeneous Scene Graph Message Passing for Cross-modal Retrieval. ICMR 21</li>
<li>HLVU : A New Challenge to Test Deep Understanding of Movies the Way Humans do. ICMR 20</li>
<li>Context-Aware Embeddings for Automatic Art Analysis. ICMR 19</li>
<li>Semantic Gated Network for Efficient News Representation. ICMR 20</li>
<li>Ten Questions in Lifelog Mining and Information Recall. ICMR 21</li>
<li>Video2Subtitle: Matching Weakly-Synchronized Sequences via Dynamic Temporal Alignment. ICMR 22</li>
<li>Improve Image Captioning by Modeling Dynamic Scene Graph Extension. ICMR 22</li>
<li>SenseMood: Depression Detection on Social Media. ICMR 20</li>
<li>Incorporating Semantic Knowledge for Visual Lifelog Activity Recognition. ICMR 20</li>
</ol>
<h3 id="coling">COLING</h3>
<ol type="1">
<li>MEISD: A Multimodal Multi-Label Emotion, Intensity and Sentiment Dialogue Dataset for Emotion Recognition and Sentiment Analysis in Conversations</li>
<li>Are Visual-Linguistic Models Commonsense Knowledge Bases? COLING 22</li>
<li>Extracting a Knowledge Base of COVID-19 Events from Social Media. COLING 22</li>
<li>Read Extensively, Focus Smartly: A Cross-document Semantic Enhancement Method for Visual Documents NER. COLING 22</li>
<li>Decoupling Mixture-of-Graphs: Unseen Relational Learning for Knowledge Graph Completion by Fusing Ontology and Textual Experts. COLING 22</li>
<li>A Relation Extraction Dataset for Knowledge Extraction from Web Tables. COLING 22</li>
<li>Virtual Knowledge Graph Construction for Zero-Shot Domain-Specific Document Retrieval. COLING 22</li>
<li>KC-ISA: An Implicit Sentiment Analysis Model Combining Knowledge Enhancement and Context Features. COLING 22</li>
<li>Learning from Adjective-Noun Pairs: A Knowledge-enhanced Framework for Target-Oriented Multimodal Sentiment Classification. COLING 22</li>
<li>Extracting a Knowledge Base of COVID-19 Events from Social Media. COLING 22</li>
<li>Mind the Gap! Injecting Commonsense Knowledge for Abstractive Dialogue Summarization. CLOING 22</li>
<li>Multilingual and Multimodal Topic Modelling with Pretrained Embeddings. COLING 22</li>
<li>Knowledge-injected Prompt Tuning for Event Detection. COLING 22</li>
<li>Section-Aware Commonsense Knowledge-Grounded Dialogue Generation with Pre-trained Language Model. COLING 22</li>
<li>Enhancing Clinical BERT Embedding using a Biomedical Knowledge Base. COLING 20]</li>
<li></li>
</ol>
]]></content>
      <categories>
        <category>Reading-list</category>
        <category>MMKG</category>
      </categories>
      <tags>
        <tag>Collection</tag>
        <tag>Reading-list</tag>
        <tag>MMKG</tag>
      </tags>
  </entry>
  <entry>
    <title>CompGCN</title>
    <url>/kge/CompGCN/</url>
    <content><![CDATA[<h1 id="composition-based-multi-relational-graph-convolutional-networks">COMPOSITION-BASED MULTI-RELATIONAL GRAPH CONVOLUTIONAL NETWORKS</h1>
<h2 id="introduction">1 INTRODUCTION</h2>
<p>原来的CNN, RNN等方法不能直接应用到graph上，因此最近GCN被提出来了。</p>
<p>但是初始的GCN方法主要集中与无向图，最近的针对有向图的方法类如R-GCN，存在over-parameterization问题。</p>
<blockquote>
<p>there is a need for a framework which can utilize KG embedding techniques for learning task-speciﬁc node and relation embeddings.</p>
<p>COMPGCN addresses the shortcomings of previously proposed GCN models by jointly learning vector representations for both nodes and relations in the graph</p>
</blockquote>
<span id="more"></span>
<h2 id="related-work">2 RELATED WORK</h2>
<p>两个方面叙述</p>
<ul>
<li>GCN: 原始的GCN，之后的各种拓展，都在MPNN框架下，本论文提出的也是这样，但是专门为relational data设计过</li>
<li>KE: translational、semantic matching based、neural network based</li>
</ul>
<h2 id="background">3 BACKGROUND</h2>
<h3 id="gcn-on-undirected-graph">GCN on undirected graph</h3>
<p>图的表示形式： <span class="math display">\[
G=(\cal{V},\cal{E},\cal{X})
\]</span> 其中<span class="math inline">\(\cal{X}\)</span>表示所有entity的初始feature，<span class="math inline">\(\cal{X}\in \cal{R}^{ |\cal{V}|\times d_0 }\)</span>。</p>
<p>获取归一化后的self-connection邻接矩阵： <span class="math display">\[
\hat{A}=\tilde{D}^{-\frac{1}{2}}(A+I)\tilde{D}^{-\frac{1}{2}} \\
\tilde{D}_{ii}=\sum_j{(A+I)_{ij}}
\]</span> 某一层的GCN： <span class="math display">\[
H^{k+1}=f(\hat{A}H^kW^k) \\
H^0=\cal{X}
\]</span></p>
<h3 id="gcn-on-directed-graph">GCN on directed graph</h3>
<p>图的表示形式： <span class="math display">\[
G=(\cal{V},\cal{R},\cal{E},\cal{X})
\]</span> <span class="math inline">\(\cal{R}\)</span>表示relation的集合。</p>
<p>这种情况下对于关系数据的处理就存在区别了，基于Encoding sentences with graph convolutional networks for semantic role labeling 中提出的假设，</p>
<blockquote>
<p>information in a directed edge ﬂows along both directions</p>
</blockquote>
<p>因此构造出反向关系inverse relation: <span class="math display">\[
(u,v,r)\in \cal{E}\ \ and\ \ (v,u,r^{-1})\in \cal{E^{-1}}
\]</span> 此时的GCN： <span class="math display">\[
H^{k+1}=f(\hat{A} H^k W^k_r) \\
H^0=\cal{X}
\]</span></p>
<h2 id="compgcn-details">4 CompGCN DETAILS</h2>
<p>图： <span class="math display">\[
G=(\cal{V},\cal{R},\cal{E},\cal{X},\cal{Z}) \\
\cal{X}\in \cal{R}^{ |\cal{V}|\times d_0 } \\
\cal{Z}\in \cal{R}^{ |\cal{R}|\times d_0 }
\]</span> 构造关系： <span class="math display">\[
\cal{E^{&#39;}} = \cal{E}\ \cup\ \{ (v,u,r^{-1}) | (u,v,r)\in \cal{E}\ \}\ \cup\ \{ (u,u,T) | u\in \cal{V} \}
\]</span> embedding更新方式： <span class="math display">\[
h_v^{k+1}=f(\sum_{(u,r)\in \cal{N}_v} W_{\lambda(x)^k}\phi(x_u^k, z_r^k))
\]</span> 其中<span class="math inline">\(\phi\)</span>函数，为了减少参数，可以为下面的三种方式，当然可以拓展为更多的方式： <span class="math display">\[
Sub: \ \phi(x_u, z_r) = x_u - z_r \\
Mult:\ \phi(x_u, z_r) = x_u * z_r \\
Circular-correlation:\ \phi(x_u, z_r) = x_u \star z_r
\]</span> 其中的关系权值矩阵： <span class="math display">\[
W_{dir(r)}= \begin{cases} W_o,\ r\in \cal{R} \\ W_i,\ r\in \cal{R}_{inv} \\ W_S\ r\in \cal{T}(self-loop) \end{cases}
\]</span> 对于关系relation的处理，与KBGAT一样： <span class="math display">\[
z_r^{k+1} = W_{rel}z_r^k \\
W_{rel}\in R^{d_1\times d_0}
\]</span> 在第一层初始的时候，对于relation的定义是bias-vector。 <span class="math display">\[
Z_r = \sum_b^B \alpha_{br}\bold{v}_b \\
\{ \bold{v}_1, \bold{v}_2,\cdots \bold{v}_B \}
\]</span></p>
<h2 id="experimental-setup">5 EXPERIMENTAL SETUP</h2>
<p>进行了下面三个任务：</p>
<ul>
<li>Link Prediction：FB15k-237，WN18RR</li>
<li>Node Classiﬁcation：MUTAG (Node) ， AM</li>
<li>Graph Classiﬁcation：bioinformatics dataset：MUTAG (Graph) ， PTC</li>
</ul>
<h2 id="results">6 RESULTS</h2>
<p>研究了下面四个方面的问题：</p>
<ol type="1">
<li>在link prediction上的效果</li>
<li>选择不同的composite operation效果</li>
<li>模型对于不同数量的relation的数据集的效果</li>
<li>在node和graph classiﬁcation的效果</li>
</ol>
<blockquote>
<p>We ﬁnd that with DistMult score function, multiplication operator (Mult) gives the best performance while with ConvE, circular-correlation surpasses all other operators.</p>
</blockquote>
<p>具体结果略</p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>GNN</category>
      </categories>
  </entry>
  <entry>
    <title>ConvE</title>
    <url>/kge/ConvE/</url>
    <content><![CDATA[<h1 id="convolutional-2d-knowledge-graph-embeddings">Convolutional 2D Knowledge Graph Embeddings</h1>
<p>2018-7-4</p>
<h2 id="introduction">1 Introduction</h2>
<p>第一个利用CNN学习KGE的方法。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20200314161529853.png" /></p>
<span id="more"></span>
<p>现在的knowledge graph会存在很多missing links，比如在Freebase和DBpedia中，超过66%的person实体没有到出生地的link。由于在知识图谱当中存在上百万的facts，所以模型的效率和计算代价就需要特别的考虑。</p>
<p>CNN具有能够快速计算的特性，因此可以应用与knowledge graph embedding。</p>
<h2 id="convolutional-2d-knowledge-graphs-embeddings">2 Convolutional 2D Knowledge Graphs Embeddings</h2>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20200314161529853.png" /></p>
<p>首先对<span class="math inline">\(e_1\in R^k\)</span>和<span class="math inline">\(r_r\in R^k\)</span>的一维的embedding进行转换为2维的形式： <span class="math display">\[
[\bar{e_s}; \bar{r_r}] \\
\bar{e_s},\ \bar{r_r}\in R^{k_w\times k_h} \\
k=k_w\times k_h
\]</span> 即： <span class="math display">\[
\begin{pmatrix} 
a &amp; a &amp; a\\ 
b &amp; b &amp; b\\
a &amp; a &amp; a\\
b &amp; b &amp; b\\
\end{pmatrix}
\]</span> 改变为这种两个embedding相间的格式。</p>
<p>之后进行卷积操作： <span class="math display">\[
relu([\bar{e_s}; \bar{r_r}] \star \cal{w})
\]</span> 然后变回一维矩阵 <span class="math display">\[
vec(relu([\bar{e_s}; \bar{r_r}] \star \cal{w}))
\]</span> 过一个全连接层， <span class="math display">\[
relu(vec(relu([\bar{e_s}; \bar{r_r}] \star \cal{w}))W)
\]</span> 最后和目标embedding相乘，就得到了score。 <span class="math display">\[
\psi_r(e_s,e_o)=relu(vec(relu([\bar{e_s}; \bar{r_r}] \star \cal{w})) W)e_o
\]</span> 训练loss $$</p>
<p>$$</p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>KGE</category>
      </categories>
  </entry>
  <entry>
    <title>ConvKB</title>
    <url>/kge/ConvKB/</url>
    <content><![CDATA[<h1 id="a-novel-embedding-model-for-knowledge-base-completion-based-on-convolutional-neural-network">A Novel Embedding Model for Knowledge Base Completion Based on Convolutional Neural Network</h1>
<p>2018-3-13 ConvKB</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20200219181643488.png" /></p>
<span id="more"></span>
<h2 id="introduction">1 Introduction</h2>
<p>知识图谱可以为knowledge base/knowledge graph</p>
<p>知识图谱在很多领域都有重要的作用，但是在open world assumion下，知识图谱本身是不完整的，不能包含所有实际存在的三元组。</p>
<p>在这种情况下，预测(h,r,t)是否valid就有意义了，这就是link prediction或者叫做knowledge completion。</p>
<p>之前有人提出过ConvE（2018），首个将CNN应用到knowledge completion中的model。</p>
<p>ConvKB就是在其基础上发展起来的。</p>
<p>CNN的意义：</p>
<blockquote>
<p>CNN learns non-linear features to capture complex relationships with a remarkably less number of parameters compared to fully connected neural networks.</p>
</blockquote>
<p>总结一下CNN相比fully connected neural network优势：</p>
<ul>
<li>更少的参数</li>
<li>能够捕获复杂的，非线性的关系</li>
</ul>
<h2 id="proposed-convkb-model">2 Proposed ConvKB model</h2>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20200219181643488.png" /></p>
<p>首先三元组组成矩阵 <span class="math display">\[
A=[v_h,v_r,v_t]\in R^{k\times 3}
\]</span> 之后经过一个简单的CNN层， <span class="math display">\[
v_i=g(w\cdot A_{i;}+b) \\
w\in R^{1\times k}
\]</span> 一个filter <span class="math inline">\(w\)</span>得到一个<span class="math inline">\(k\times 1\)</span>维的feature map</p>
<p>共有<span class="math inline">\(\Gamma\)</span>个filter，通过concat得到<span class="math inline">\(R^{\Gamma k\times 1}\)</span>维的向量，</p>
<p>最后与一个权值矩阵<span class="math inline">\(W\in R^{\Gamma k\times 1}\)</span>相乘，得到最终的结果score。score function衡量不相似的程度，越小越相似。</p>
<p>训练时候的损失函数是log-likehood损失函数。</p>
<h2 id="experiments">3 Experiments</h2>
<p>实验数据集：</p>
<ul>
<li>WN18RR</li>
<li>FB15k237</li>
</ul>
<p>都是对于WN18和FB15k去除可逆关系之后的结果</p>
<p>sample corrupt triplets的时候采用了Bernoulli trick</p>
<p>embedding初始化使用TransE训练出来的结果</p>
<p>filter情况下的数据集进行训练</p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>KGE</category>
      </categories>
  </entry>
  <entry>
    <title>ConvR</title>
    <url>/kge/ConvR/</url>
    <content><![CDATA[<h1 id="adaptive-convolution-for-multi-relational-learning">Adaptive Convolution for Multi-Relational Learning</h1>
<p>2019-6</p>
<h2 id="introduction">1 Introduction</h2>
<blockquote>
<p>Learning with multi-relational data plays a pivotal role in many application domains, ranging from social networks or recommender systems to large-scale knowledge bases (KBs)</p>
</blockquote>
<p>ConvR的思想是从relation中构造filter，然后卷积于subject embedding，最后投影，与object embedding做点积。</p>
<p>这样的做法就导致了ConvR的另一个优势，减少了参数的数量。</p>
<span id="more"></span>
<h2 id="adaptive-convolution-on-multi-relational-data">3 Adaptive Convolution on Multi-relational Data</h2>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20200330213550962.png" /></p>
<p>对于三元组<span class="math inline">\((s, r, o)\)</span>，首先将<span class="math inline">\(e_s\)</span> reshape为2D矩阵， <span class="math display">\[
e_s\in R^{d_e}\Rightarrow S\in R^{d_e^h,\ d_e^w}
\]</span> 对于关系<span class="math inline">\(r\)</span>，先分割为<span class="math inline">\(c\)</span>段： <span class="math display">\[
r^{(1)},\cdots,r^{(c)}
\]</span> 然后每个<span class="math inline">\(r^{(l)}\)</span> reshape为2D的矩阵作为filter <span class="math display">\[
R^{l}\in R^{h,\ w}
\]</span> 对于<span class="math inline">\(c\)</span>个filter，在<span class="math inline">\(S\)</span>上卷积，得到<span class="math inline">\(c\)</span>个feature map。</p>
<p>将<span class="math inline">\(c\)</span>个feature map先展开为一维，然后stack到一起，得到单向量<span class="math inline">\(e_c\)</span>。</p>
<p>最后过一个全连接层，和尾结点计算点积 <span class="math display">\[
\psi(s,r,o)=f(We_c+b)e_o
\]</span> 训练方式与ConvE保持一致。</p>
<p>比起ConvE的好处就是结果更好，参数更少，空间复杂度降低。</p>
<p>ConvR使用三个dropout防过拟合：</p>
<ul>
<li>在reshape subject representation时</li>
<li>在卷积得到feature map之后</li>
<li>在经过全连接之后</li>
</ul>
<h2 id="experiments">4 Experiments</h2>
<p>使用了四个数据集：</p>
<ul>
<li>FB15k</li>
<li>WN18</li>
<li>FB15K-237</li>
<li>WN18RR</li>
</ul>
<p>实现的超参：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20200330213445418.png" /></p>
]]></content>
  </entry>
  <entry>
    <title>CrossE</title>
    <url>/kge/CrossE/</url>
    <content><![CDATA[<h1 id="interaction-embeddings-for-prediction-and-explanation-in-knowledge-graphs">Interaction Embeddings for Prediction and Explanation in Knowledge Graphs</h1>
<p>2019-3-12日发表</p>
<p>设计了一种有效的，浅层的KGE方法CrossE，能够让entity embedding和relation embedding进行更多的交互。</p>
<span id="more"></span>
<h2 id="abstract">Abstract</h2>
<p>在知识图谱embedding的现有技术当中，Crossover interactions信息没有被利用过，本文提出的CrossE就是利用这种信息，并且进行了link prediction和prediction explanation的实验。</p>
<h2 id="introduction">1. INTRODUCTION</h2>
<p>几个出名的知识图谱Yago，WordNet，Freebase都是以<span class="math inline">\((h, r, t)\)</span>表示的三元组。</p>
<blockquote>
<p>Knowledge graph embedding (KGE) learns distributed representations [11] for entities and relations, called entity embeddings and relation embeddings.</p>
</blockquote>
<p>三种知识图谱embedding类型，</p>
<ul>
<li>tensor factorization based RESCAL ,</li>
<li>translation-based TransE ,</li>
<li>neural tensor network NTN</li>
</ul>
<p>但是这些模型都没有使用过Crossover interactions。</p>
<figure>
<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20200213103537464.png" alt="image-20200213103537464" /><figcaption>image-20200213103537464</figcaption>
</figure>
<p>Crossover interactions包括interaction from relations to entities和interactions from entities to relations.</p>
<p>例如在上图中预测(X, isFatherOf, ? )</p>
<p>那么，与关系isFatherOf有关的实体只有Y，S这些与X是家庭关系的实体，与Q，T实体无关，这就叫做interaction from relations to entities，根据关系选择实体</p>
<p>同时，关系isFatherOf有两条途径，但头结点实体是X，所以只能选择X作为头结点的关系，这叫做interactions from entities to relations.</p>
<p>因此提出了CrossE：</p>
<ol type="1">
<li>generate interaction embeddings <span class="math inline">\(h_I\)</span> for head entity <span class="math inline">\(h\)</span></li>
<li>generate interaction embeddings <span class="math inline">\(r_I\)</span> for relation <span class="math inline">\(r\)</span></li>
<li>combine interaction embeddings <span class="math inline">\(h_I\)</span> and <span class="math inline">\(r_I\)</span> together</li>
<li>compare the similarity of combined embedding with tail entity embedding<span class="math inline">\(t\)</span></li>
</ol>
<h2 id="related-work">2. Related work</h2>
<p>不考虑利用了额外信息进行embedding的模型，剩下的模型从entity是否表示为统一的形式划分，</p>
<ul>
<li><p>KGEs with general embeddings：</p>
<blockquote>
<p>Existing embedding methods with general embeddings all represent entities as low-dimensional vectors and relations as operations that combine the representation of head entity and tail entity.</p>
</blockquote>
<p>典型模型包括：TransE, RESCAL，DistMult，ComplEx</p>
<p>这些模型都没有考虑在不同情况下embedding应该是不同的，没有考虑crossover interaction</p></li>
<li><p>KGEs with multiple embeddings：</p>
<blockquote>
<p>Some KGEs learn multiple embeddings for entities or relations under various considerations.</p>
</blockquote>
<p>比如：Structured Embedding (SE)（每个关系有两个矩阵），ORC（每个entity有head embedding和tail embedding），TransH，TransR等</p>
<p>这些模型，relation的embedding是general的，只考虑了relation-&gt;entity的interaction</p></li>
</ul>
<h2 id="crosse-model-description">3. CrossE: MODEL DESCRIPTION</h2>
<p>CrossE最大的创新就在于考虑了新的embedding内容，同时没有增加过多的参数。</p>
<figure>
<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20200213173128411.png" alt="image-20200213173128411" /><figcaption>image-20200213173128411</figcaption>
</figure>
<p>初始定义：</p>
<ul>
<li><span class="math inline">\(E \in R^{n_e \times d}, R \in R^{n_r \times d}, C \in R^{n_r \times d}\)</span>分别表示所有实体，关系和interaction matrix</li>
<li><span class="math inline">\(x_h, x_r, x_t\)</span>分别表示(h, r, t)的one-hot形式</li>
</ul>
<p>对于一个三元组(h, r, t)进行预测的顺序如下：</p>
<h3 id="获取general-embedding">3.1 获取general embedding</h3>
<p><span class="math display">\[
h=x_h^T E,\ r=x_r^T R,\ t=x_t^T E
\]</span></p>
<h3 id="interaction-embedding-for-entities.">3.2 Interaction Embedding for Entities.</h3>
<p>对于头结点h， <span class="math display">\[
h_I = c_r \circ h \\
c_r = x_r^TC
\]</span> 其中的<span class="math inline">\(\circ\)</span>是Hadamard product, an element-wise operator</p>
<h3 id="interaction-embedding-for-relations">3.3 Interaction Embedding for Relations</h3>
<p>对于关系r，模型来自头结点的信息， <span class="math display">\[
r_I = h_I \circ r
\]</span></p>
<h3 id="combination-operator">3.4 Combination Operator</h3>
<p>将前面的两个基于interaction的embedding联合起来， <span class="math display">\[
q_{hr}=tanh(h_I+r_I+b)
\]</span></p>
<h3 id="similarity-operator">3.5 Similarity Operator</h3>
<p>计算最终的相似度， <span class="math display">\[
f(h, r, t)=\sigma(q_{hr}t^T)=\sigma(tanh(c_r\circ h+c_r\circ h \circ r+b )t^T)
\]</span> 最终的loss function使用交叉熵。</p>
<h2 id="explanations-for-predictions">4 EXPLANATIONS FOR PREDICTIONS</h2>
<p>在知识图谱的explanation：</p>
<ul>
<li><p>在知识图谱中对于(h, r, t)可以成立的explanation是指从h-&gt;t的路径</p></li>
<li><p>对于一个explanation，应该有对应的一个或多个support，在这篇论文当中，explanation的support就是现存的知识图谱中相似结构，如下图所示。</p>
<figure>
<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20200213203729321.png" alt="image-20200213203729321" /><figcaption>image-20200213203729321</figcaption>
</figure></li>
</ul>
<p>寻找explanation的步骤：</p>
<figure>
<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20200213204130795.png" alt="image-20200213204130795" /><figcaption>image-20200213204130795</figcaption>
</figure>
<p>详细的请参考论文</p>
<h2 id="experimental-evaluation">5 EXPERIMENTAL EVALUATION</h2>
<p>略</p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>KGE</category>
      </categories>
  </entry>
  <entry>
    <title>GRL</title>
    <url>/kge/GRL/</url>
    <content><![CDATA[<h1 id="generalized-relation-learning-with-semantic-correlation-awareness-for-link-prediction">Generalized Relation Learning with Semantic Correlation Awareness for Link Prediction</h1>
<p>AAAI 2021</p>
<p>作者提出了一种能够捕获KG中relation的semantic correlations的方法，叫做GRL（Generalized Relation Learning）。这个方法在一般的embedding方法之后，利用输出的embedding评估relation之间的相似程度。</p>
<span id="more"></span>
<h2 id="introduction">Introduction</h2>
<p><strong>motivation</strong>：作者认为目前用于link prediction的基于embedding方法存在两个问题：</p>
<ol type="1">
<li>忽略了对于few-shot relation的学习，大多数方法假设不同relation有足够的实例进行学习</li>
<li>无法学习zero-shot relation</li>
</ol>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210629162808164.png" style="zoom:50%;" /></p>
<p>根据调研的情况来看，大多数的relation是few-shot relation。</p>
<p><strong>method</strong>：作者利用many-shot relation来为相似的few-shot和zero-shot relation提供信息。主要做法是提出GRL，学习relation之间的相关性correlation。</p>
<h2 id="method">Method</h2>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210629163143120-20210629200856777.png" style="zoom:50%;" /></p>
<p>首先，通过一个base model获取embedding，比如利用ConvE或者DistMult。</p>
<p>GRL详细的说有三个module，</p>
<p>在Attention module中，首先捕获头实体与尾实体之间可能存在的潜在relation信息，即学习一个头尾实体的联合表示joint vector <span class="math inline">\(\mathbf{j}\)</span></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210629163754371.png" style="zoom:50%;" /></p>
<p>然后，作者使用了一个Relation Memory Block保存所有的relation信息，<span class="math inline">\(K\)</span>就是所有relation的数量。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210629163911233.png" style="zoom:50%;" /></p>
<p>之后，作者希望从这个Relation Memory Block导出能够丰富<span class="math inline">\(\mathbf{j}\)</span>的信息，</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210629164240642.png" style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210629164355894.png" style="zoom:50%;" /></p>
<p>这一步实际就是捕获了不同relation之间的correlation，需要注意的是对于在<span class="math inline">\(\mathbf{M}\)</span>中的预测目标<span class="math inline">\(\mathbf{r}\)</span>，会被mask为0。<span class="math inline">\(\alpha_{sim}\)</span>就是joint vector <span class="math inline">\(\mathbf{j}\)</span>和不同relation之间的相似程度。这样，利用<span class="math inline">\(\alpha_{sim}\)</span>，在遭遇zero-shot relation时，可以选择最相似的relation来替代zero-shot relation。</p>
<p>在Fusion module中，为了确定如何自适应的混合<span class="math inline">\(\mathbf{j}\)</span>和<span class="math inline">\(\mathbf{rk}\)</span>，使用了一个类似GRU的方法，计算一个weight scalar。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210629164658959.png" style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210629164715132.png" style="zoom:50%;" /></p>
<p>最后，在classifier module，预测真实的relation：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210629165004731.png" style="zoom:50%;" /></p>
<p>其中，<span class="math inline">\(W_c\in \mathbb{R}^{dim\times K}\)</span>，计算loss</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210629165133409.png" style="zoom:50%;" /></p>
<p>最终，这个loss和base model的loss混合到一起</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210629165254620.png" style="zoom:50%;" /></p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>KRL</category>
      </categories>
      <tags>
        <tag>KG</tag>
      </tags>
  </entry>
  <entry>
    <title>HAKE</title>
    <url>/kge/HAKE/</url>
    <content><![CDATA[<h1 id="learning-hierarchy-aware-knowledge-graph-embeddings-for-link-prediction">Learning Hierarchy-Aware Knowledge Graph Embeddings for Link Prediction</h1>
<p>2019-12-15 AAAI 2020</p>
<p>Hierarchy-Aware Knowledge Graph Embeddings（HAKE）就是不增加额外的信息，利用知识图谱的语义层级建模。</p>
<p>HAKE为了区分所有的实体，将实体嵌入分为两部分：</p>
<ul>
<li>不同的语义层级下的实体，使用极坐标的模长/极径（modulus）表示</li>
<li>同一语义层级下的不同实体，使用极坐标的相位/极角（phase）表示</li>
</ul>
<span id="more"></span>
<h2 id="introduction">1 Introduction</h2>
<p>问题：目前的模型大多没有对于语义层级关系（semantic hierarchy）进行建模</p>
<p>解决方案：引入极坐标系（polar coordinate system），</p>
<ul>
<li>具有更高语义层级的实体具有更小的半径</li>
<li>同一语义层级的实体具有不同的角度</li>
</ul>
<p>知识图谱在是一系列的事实的集合，是语义网络的拓展。</p>
<p>现在的知识图谱可以包含数以亿计的事实（fact），但是知识图谱不可能包含所有实际中存在的事实。因此，链路预测（link prediction）/知识图谱补全（knowledge base completion）成为了研究的一个方向。即如何根据已有的事实，预测可能存在的事实。</p>
<p>受到词嵌入的启发，知识图谱嵌入（knowgraph graph embedding）——将知识图谱映射到离散的表示形式，就成为了研究热点。</p>
<blockquote>
<ul>
<li>知识图谱嵌入的应用方向很多，不只是链路预测，还包括实体分类等等。</li>
<li>知识图谱嵌入也只是图嵌入的一个方向</li>
<li>链路预测对于其它的图（社交网络等），同样成立</li>
</ul>
</blockquote>
<p>之前的知识图嵌入的工作主要集中在建模关系的特性：</p>
<ul>
<li>对称/不对称</li>
<li>可逆/不可逆</li>
<li>组合</li>
</ul>
<p>在知识图谱当中存在语义的层级，比如在wordnet知识图谱里，[arbor/cassia/palm, hypernym, tree]，tree的语义层级要高于[hypernym, tree]。对于如何利用知识图谱的语义特性的工作较少，并且很多要求要增加额外的信息，例如额外的文本描述，来建模知识图谱的层级关系。</p>
<p>Hierarchy-Aware Knowledge Graph Embeddings（HAKE）就是不增加额外的信息，利用知识图谱的语义层级建模。</p>
<p>HAKE为了区分所有的实体，将实体嵌入分为两部分：</p>
<ul>
<li>不同的语义层级下的实体，使用极坐标的模长/极径（modulus）表示</li>
<li>同一语义层级下的不同实体，使用极坐标的相位/极角（phase）表示</li>
</ul>
<h2 id="the-proposed-hake">2 The Proposed HAKE</h2>
<p>HAKE的模型图：</p>
<figure>
<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20200614173832367.png" alt="image-20200614173832367" /><figcaption>image-20200614173832367</figcaption>
</figure>
<h3 id="the-modulus-part">2.1 The modulus part</h3>
<p>极坐标的极径建模实体的语义层级。</p>
<p>知识图谱中的实体可以组成一棵树，越往上的实体语义层级越高，越往下的语义层级越低。使用模量/极径表示实体在语义树中的深度，具有越高的语义层级的实体有更小的深度，更小的模量。</p>
<p>使用<span class="math inline">\(e_m\)</span>表示嵌入的模量部分modulus part，则有： <span class="math display">\[
h_m\circ r_m=t_m,\  where\ h_m,\ t_m\in R^k,\ r_m \in R^k_{+}
\]</span> 距离函数为： <span class="math display">\[
d_{r,m}(h_m, t_m)=||h_m\circ r_m - t_m||_2
\]</span> 要注意这里限制了<span class="math inline">\(r_m\)</span>必须在正数域下，这是因为正数的<span class="math inline">\(r_m\)</span>不会改变<span class="math inline">\(h_m\)</span>的符号，这是因为对于正样本<span class="math inline">\((h,r,t)\)</span>，<span class="math inline">\(h_m\)</span>与<span class="math inline">\(t_m\)</span>倾向于有相同的符号，<span class="math display">\[d_{r,m}(h_m,t_m)\]</span>更小，而负样本<span class="math inline">\((h,r,t^{&#39;})\)</span>更难保证同一纬度下的<span class="math inline">\(h_m\)</span>与<span class="math inline">\(t_m\)</span>倾向有相同的符号，导致<span class="math inline">\(d_{r,m}(h_m,t_m^{&#39;})\)</span>更大。</p>
<p>这样的<span class="math inline">\(r_m\)</span>成为了一个缩放操作，对于<span class="math inline">\((h,r,t)\)</span>，</p>
<ol type="1">
<li>如果h的层级比t更大，r倾向于&gt;1</li>
<li>如果h的层级与t一样，r倾向于=1</li>
<li>如果h的层级比t更小，r倾向于&lt;1</li>
</ol>
<h3 id="the-phase-part">2.2 The phase part</h3>
<p>进一步区分同一层级下的不同实体。</p>
<p>使用<span class="math inline">\(e_p\)</span>表示相位部分， <span class="math display">\[
(h_p+r_p)\ mod\ 2\pi = t_p,\ where\ h_p,t_p,r_p\in [0, 2\pi)^k
\]</span> 距离函数： <span class="math display">\[
d_{r,p}(h_p,t_p)=|| \sin{((h_p + r_p - t_p)/2)} ||_1
\]</span> 除以2是保证<span class="math inline">\((h_p + r_p - t_p)/2\in [0, 2\pi)^k\)</span>，上面的式子和pRotatE中的一样。</p>
<h3 id="loss-function">2.3 Loss Function</h3>
<p>经过上面的两部分，获得总的嵌入： <span class="math display">\[
e=[e_m;e_p]
\]</span> 之后计算<span class="math inline">\((h,r,t)\)</span>存在概率的得分： <span class="math display">\[
f_r(h,t)=-(d_{r,m}(h,t)+\lambda d_{r,p}(h,t))
\]</span> 使用负采样的损失函数： <span class="math display">\[
L=-log\sigma(\gamma-f_r(h,t))-\sum_{i=1}^n p(h^{&#39;}_i, r, t^{&#39;}_i) log\sigma(f_r(h^{&#39;}_i,t^{&#39;}_i)-\gamma) \\
p(h^{&#39;}_j, r, t^{&#39;}_j) =\frac{exp\alpha f_r(h^{&#39;}_j, t^{&#39;}_j)}{\sum_i f_r(h^{&#39;}_i, t^{&#39;}_i)}
\]</span></p>
<h2 id="experiments-and-analysis">3 Experiments and Analysis</h2>
<h3 id="main-results">3.1 Main Results</h3>
<figure>
<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20200614173938559.png" alt="image-20200614173938559" /><figcaption>image-20200614173938559</figcaption>
</figure>
<p>三个数据集：WN18RR, FB15k-237, YAGO3-10</p>
<p>为了说明phase part部分的作用，只保留modulus part，作为模型<strong>ModE</strong>： <span class="math display">\[
d_{r,m}(h_m, t_m)=||h_m\circ r_m - t_m||_2\  where\ h_m,\ t_m\ r_m \in R^k
\]</span></p>
<h3 id="analysis-on-relation-embeddings">3.2 Analysis on Relation Embeddings</h3>
<p>首先，说明HAKE能否捕获不同语义层级的信息。</p>
<p>只使用modulus part，下图是表示不同语义层级的关系embedding的直方图，横轴是大小，纵轴是密度</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20200614174230314.png" /></p>
<p>其中，</p>
<ul>
<li>a和b是尾实体比头实体的关系语义层级高，结果显示关系embedding大部分元素&lt;1</li>
<li>c和d是尾实体比头实体的关系语义层级一样，结果显示关系embedding大部分元素=1</li>
<li>e和f是尾实体比头实体的关系语义层级低，结果显示关系embedding大部分元素&gt;1</li>
</ul>
<p>同样可以看出HAKE比ModE的方差更小，说明HAKE的建模更准确。</p>
<p>之后，说明phase part的作用，比较c和d的关系embedding的phase part</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20200614175049088.png" /></p>
<p>上述结果说明有很多关系嵌入的phase part元素分布在<span class="math inline">\(\pi\)</span>，导致<span class="math inline">\(h_p\)</span>和<span class="math inline">\(t_p\)</span>不一样，可以区分同一语义层级的不同实体。</p>
<h3 id="analysis-on-entity-embeddings">3.3 Analysis on Entity Embeddings</h3>
<p>因为是使用极坐标来表示语义层级，可以把实体embedding在极坐标中可视化。</p>
<p>实体embedding大小为1000，选500个维度画在二维极坐标中，对原始的极径使用对数函数，来更好的展示结果。由于所有模的值都小于1，因此在图中，更大的直径表示更小的模值，即更高的语义层级。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20200614181125392.png" /></p>
<p>图中显示的结果说明HAKE比RotatE能够更好的捕获层级关系。</p>
<h3 id="ablation-studies">3.4 Ablation Studies</h3>
<figure>
<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20200614180811928.png" alt="image-20200614181125392" /><figcaption>image-20200614181125392</figcaption>
</figure>
]]></content>
      <categories>
        <category>Paper</category>
        <category>KGE</category>
      </categories>
  </entry>
  <entry>
    <title>HoLE</title>
    <url>/kge/HoLE/</url>
    <content><![CDATA[<h1 id="hole-holographic-embeddings-of-knowledge-graphs">HoLE: Holographic Embeddings of Knowledge Graphs</h1>
<p>AAAI 2016</p>
<p>这篇文章提出了holographic embeddings (HOLE)，来学习KG的compositional vector space representations。</p>
<span id="more"></span>
<p><strong>motivation</strong>：However, existing embedding models that can capture rich interactions in relational data are often limited in their scalability. Vice versa, models that can be computed efﬁciently are often considerably less expressive.</p>
<p><strong>methods</strong>：直接从subject entity embedding和object entity embedding中，使用circular correlation获得新的embedding，称作holograph embedding，然后使用这个holograph embedding与relation embedding做点积，得到预测概率。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210418181121701.png" style="zoom:50%;" /></p>
<p><strong>理解circular correlation</strong>：</p>
<p>它是一种捕获feature interaction的方法，首先我们来看以下几个不同的捕获特征交互的方法。</p>
<ol type="1">
<li>Tensor Product</li>
</ol>
<p><span class="math display">\[
[\mathbf{a}\ \otimes\ \mathbf{b}]_{ij} = \mathbf{a}_{i}\mathbf{b}_j \in \mathbb{R}^{d^2}
\]</span></p>
<p>形成了一个矩阵。这样捕获的feature的特点是获得了所有的pairwise multiplicative interactions between the features of <span class="math inline">\(\mathbf{a}\)</span> and <span class="math inline">\(\mathbf{b}\)</span>。</p>
<p>从直观上来看，如果是来自<span class="math inline">\(\mathbf{a}\)</span> 和<span class="math inline">\(\mathbf{b}\)</span>的同时起作用时，这样的方法比较好。它能够用来捕获<em>通用，共有</em>的特征，例如a和b是自由人和自由党，<em>liberal persons are typically members of liberal parties</em>，这样的事实。</p>
<blockquote>
<p>Intuitively, a feature in the tuple representation a ⊗ b is “on” (has a high absolute magnitude), if and only if the corresponding features of both entities are “on”</p>
</blockquote>
<p>这样的方法在RESCAL和NTN，DistMult中都得到了使用。</p>
<p>缺点在于（1）计算量相对较大（2）无法捕获独立的特征</p>
<ol start="2" type="1">
<li>Concatenation, Projection, and Non-Linearity</li>
</ol>
<p>这是最常见的方法，对于向量输入<span class="math inline">\(\mathbf{a}\)</span> 和<span class="math inline">\(\mathbf{b}\)</span>，先拼接，然后linear projection，最后经过一层non-linearity function。 <span class="math display">\[
f(W[\mathbf{a};\mathbf{b}])
\]</span> 这种方法捕获的特征是如果有特征至少在<span class="math inline">\(\mathbf{a}\)</span> 和<span class="math inline">\(\mathbf{b}\)</span>中起到作用。</p>
<blockquote>
<p>Intuitively, a feature in the tuple representation W(a ⊕ b) is “on” if at least one of the corresponding features is “on”.</p>
</blockquote>
<p>缺点是对于<span class="math inline">\(\mathbf{a}\)</span> 和<span class="math inline">\(\mathbf{b}\)</span>没有直接的交互。</p>
<ol start="3" type="1">
<li>Circular Convolution</li>
</ol>
<p><span class="math display">\[
[\mathbf{a}\ *\ \mathbf{b}]_{k} = \sum_{i=0}^{d-1} a_i b_{k-i\ mod\ d}
\]</span></p>
<p>将<span class="math inline">\(\mathbf{b}\)</span>反转，然后与<span class="math inline">\(\mathbf{a}\)</span>进行卷积。</p>
<ol start="4" type="1">
<li>Circular Correlation</li>
</ol>
<p><span class="math display">\[
[\mathbf{a}\ \star\ \mathbf{b}]_{k} = \sum_{i=0}^{d-1} a_i b_{k+i\ mod\ d}
\]</span></p>
<p><span class="math inline">\(\mathbf{b}\)</span>不需要反转，然后与<span class="math inline">\(\mathbf{a}\)</span>进行卷积。</p>
<p>一个图示：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210418184909978.png" style="zoom:50%;" /></p>
<p>从这个图能够看出来，Circular Correlation可以看做是tensor dot的一种压缩方式，它的输出结果的每一维都是tensor dot结果的一部分。</p>
<p>它与Circular Convolution的区别：</p>
<ul>
<li>Non Commutative：对于Circular Convolution，<span class="math inline">\(\mathbf{a}\ *\ \mathbf{b} = \mathbf{b}\ *\ \mathbf{a}\)</span>，但是对于Circular Correlation，<span class="math inline">\(\mathbf{a}\ \star\ \mathbf{b} \not= \mathbf{b}\ \star\ \mathbf{a}\)</span>。</li>
<li>Similiarity Component：在计算Circular Correlation的0维输出的时候，实际是在计算<span class="math inline">\(\mathbf{a}\)</span> 和<span class="math inline">\(\mathbf{b}\)</span>的相似程度。</li>
</ul>
<p>它与Circular Convolution的联系： <span class="math display">\[
\mathbf{a}\ \star\ \mathbf{b} = \tilde{\mathbf{a}}\ *\ \mathbf{b}
\]</span> 其中，<span class="math inline">\(\tilde{\mathbf{a}}\)</span>是<span class="math inline">\(\mathbf{a}\)</span>的involution，<span class="math inline">\(\tilde{\mathbf{a}}_i=\mathbf{a}_{-i\ mod\ d}\)</span></p>
<p>为什么会想到使用Circular Correlation？</p>
<p>这个问题需要回归到题目 Holographic，作者受到基于Associative Memory的holographic models的启发。</p>
<p>在holographic reduced representations方法中，使用circular convolution来store <span class="math inline">\(\mathbf{a}\)</span> 和<span class="math inline">\(\mathbf{b}\)</span>的关联信息： <span class="math display">\[
\mathbf{m} = \mathbf{a}\ *\ \mathbf{b}
\]</span> <span class="math inline">\(\mathbf{m}\)</span>保存了memory，然后，使用circular correlation来retrieve和 <span class="math inline">\(\mathbf{a}\)</span> 相关的信息： <span class="math display">\[
\mathbf{b}^\prime = \mathbf{a}\ \star\ \mathbf{m} = \mathbf{a}\ \star\  (\mathbf{a}\ *\ \mathbf{b} )= \mathbf{b} * (\mathbf{a}\ \star\ \mathbf{a})
\]</span> 使用<span class="math inline">\(\mathbf{b}^\prime\)</span>可以与所有的候选<span class="math inline">\(\mathbf{b}\)</span>求相似度。</p>
<p>因此，这个问题作者类比到了KGE，<span class="math inline">\(\mathbf{m}\)</span>类比到<span class="math inline">\(\mathbf{e}_o\)</span>，<span class="math inline">\(\mathbf{a}\)</span>类比到<span class="math inline">\(\mathbf{e}_s\)</span>，<span class="math inline">\(\mathbf{b}\)</span>类比到<span class="math inline">\(\mathbf{r}_p\)</span>。</p>
<p>对于HoLE，Circular Correlation就是用来retrieve stored in <span class="math inline">\(\mathbf{e}_o\)</span>，然后与所有的候选<span class="math inline">\(\mathbf{r}_p\)</span>求相似度。</p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>KGE</category>
      </categories>
      <tags>
        <tag>KGE</tag>
      </tags>
  </entry>
  <entry>
    <title>KBGAT</title>
    <url>/kge/KBGAT/</url>
    <content><![CDATA[<h1 id="learning-attention-based-embeddings-for-relation-prediction-in-knowledge-graphs">Learning Attention-based Embeddings for Relation Prediction in Knowledge Graphs</h1>
<p>2019-6-4</p>
<p>将GAT应用到KG上。</p>
<span id="more"></span>
<h2 id="introduction">1 Introduction</h2>
<blockquote>
<p>Our architecture is an encoder-decoder model where our generalized graph attention model and ConvKB (Nguyen et al., 2018) play the roles of an encoder and decoder, respectively.</p>
</blockquote>
<p>CNN-based和translational-based的模型单独的处理triplet，没有考虑到KG当中某个entity附近的丰富的语义信息。</p>
<p>本论文在GAT的基础上改进。</p>
<blockquote>
<p>To the best of our knowledge, we are the ﬁrst to learn new graph attention based embeddings that speciﬁcally target relation prediction on KGs.</p>
</blockquote>
<h2 id="our-approach">3 Our Approach</h2>
<p>和之前的GAT模型比较起来，用于知识图谱的话需要考虑relation。</p>
<p>假设每一层的输入包括两个矩阵，entity matrix和relation matrix <span class="math display">\[
H\in N_e\times T \\
G\in N_r\times P
\]</span> 每一层的输出为： <span class="math display">\[
H^{&#39;}\in N_e\times T^{&#39;} \\
G^{&#39;}\in N_r\times P^{&#39;}
\]</span></p>
<h3 id="attention">3.1 Attention</h3>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20200220112116226.png" style="zoom:50%;" /></p>
<p>对于一个以<span class="math inline">\(e_i\)</span>为顶点的edge (i, j, k)，应该能够传播给<span class="math inline">\(e_i\)</span>一个embedding： <span class="math display">\[
c_{ijk}=W_1[h_i][h_k][g_j]
\]</span> 接下来计算对应的attention value。 <span class="math display">\[
b_{ijk}=LeakRelu(W_2c_{ijk}) \\
\alpha_{ijk}=softmax(b_{ijk})
\]</span> 之后进行weighted sum就可以了，neighbour指的是某个的状态 <span class="math display">\[
h_i^{&#39;}=\sigma(\sum_{j\in N_i}\sum_{k\in R_{ij}}\alpha_{ijk}c_{ijk})
\]</span></p>
<p>但类似与GAT中，使用multihead attention机制，在实现的时候作者并不是使用了<span class="math inline">\(\sigma\)</span>，而是使用<span class="math inline">\(elu\)</span>函数。 <span class="math display">\[
h_i^{&#39;}=\lVert_{m=1}^{M} \sigma(\sum_{j\in N_i}\sum_{k\in R_{ij}}\alpha_{ijk}^{m} c_{ijk}^{m})
\]</span> 在这一层传入下一层的时候，作者实现中加了一个dropout层，防止过拟合。</p>
<p>但是在最后一层，就不使用concate操作了， <span class="math display">\[
h_i^{&#39;}=\sigma(\frac{1}{M} \sum_{m=1}^M \sum_{j\in N_i} \sum_{k\in R_{ij}} \alpha_{ijk}^{m} c_{ijk}^{m})
\]</span> 最后一层的输出记作<span class="math inline">\(H^f \in N_e \times T^{f}\)</span></p>
<h3 id="对于关系的处理">3.2 对于关系的处理</h3>
<p>对于输入的<span class="math inline">\(G\)</span>，前面获得的<span class="math inline">\(h_i^{&#39;}\)</span>只是针对实体i的，所以关系<span class="math inline">\(G\)</span>的变换是直接进行线性转换。 <span class="math display">\[
G^{&#39;}=GW^R \\
W^R\in R^{P\times P^{&#39;}}
\]</span></p>
<h3 id="保留原来的entity-embedding">3.3 保留原来的entity embedding</h3>
<p>在最后一层，加上原来的entity embedding。 <span class="math display">\[
H^{&#39;&#39;}=W^EH + H^f
\]</span></p>
<h3 id="training-objective">3.4 Training Objective</h3>
<p>hinge-loss： <span class="math display">\[
L(\Omega)=\sum_{t_{ij\in S}}\sum_{t_{ij}^{&#39;}\in S^{&#39;}} max(d_{ij}-d_{ij}^{&#39;}+\gamma,\ 0)
\]</span></p>
<h3 id="decoder">3.5 Decoder</h3>
<p>使用ConVKB作为decoder</p>
<h2 id="experiments-and-results">4 Experiments and Results</h2>
<p>数据集：</p>
<ul>
<li>WN18RR (Dettmers et al., 2018),</li>
<li>FB15k-237 (Toutanova et al., 2015),</li>
<li>NELL-995 (Xiong et al., 2017),</li>
<li>Uniﬁed Medical Language Systems (UMLS) (Kok and Domingos, 2007)<br />
</li>
<li>Alyawarra Kinship (Lin et al., 2018).</li>
</ul>
]]></content>
      <categories>
        <category>Paper</category>
        <category>GNN</category>
      </categories>
  </entry>
  <entry>
    <title>KE-GCN</title>
    <url>/kge/KE-GCN/</url>
    <content><![CDATA[<h1 id="knowledge-embedding-based-graph-convolutional-network">Knowledge Embedding Based Graph Convolutional Network</h1>
<p>WWW 2021, <a href="https://github.com/PlusRoss/KE-GCN">KE-GCN</a>，提出了一个泛化的框架，将GCN和KGE的传统方法结合起来，认为在GCN中的信息传播过程是传播计算edge是否存在的得分函数<span class="math inline">\(f(u,r,v)\)</span>对<span class="math inline">\(v\)</span>的梯度，并且提出了对于relation的传播过程，在knowledge graph alignment和entity classification上进行了实验。</p>
<span id="more"></span>
<blockquote>
<p>Recently, a considerable literature has grown up around the theme of Graph Convolutional Network (GCN). How to effectively leverage the rich structural information in complex graphs, such as knowledge graphs with heterogeneous types of entities and relations, is a primary open challenge in the field. Most GCN methods are either restricted to graphs with a homogeneous type of edges (e.g., citation links only), or focusing on representation learning for nodes only instead of jointly propagating and updating the embeddings of both nodes and edges for target-driven objectives. This paper addresses these limitations by proposing a novel framework, namely the Knowledge Embedding based Graph Convolutional Network (KE-GCN), which combines the power of GCNs in graphbased belief propagation and the strengths of advanced knowledge embedding (a.k.a. knowledge graph embedding) methods, and goes beyond. Our theoretical analysis shows that KE-GCN offers an elegant unification of several well-known GCN methods as specific cases, with a new perspective of graph convolution. Experimental results on benchmark datasets show the advantageous performance of KE-GCN over strong baseline methods in the tasks of knowledge graph alignment and entity classification .</p>
</blockquote>
<h2 id="introduction">1 Introduction</h2>
<p><strong>motivation</strong>：</p>
<ul>
<li>传统的GCN方法主要假设在同质图上进行学习，忽略了KG中的relation蕴含的丰富的信息。</li>
<li>传统的KGE方法没有考虑graph的结构信息</li>
<li>将GCN和KGE结合的方法比如VR-GCN，COMPGCN等，在学习relation embedding的时候没有考虑entity embedding对relation embedding的影响</li>
</ul>
<p><strong>method</strong>：</p>
<p>为了解决上面的问题，提出了KE-GCN（Knowledge Embedding based Graph Convolution Network），能够结合KGE的方法，基于图卷积操作同时学习entity和relation embedding。</p>
<h2 id="method">2 Method</h2>
<h3 id="reformulation-of-vanilla-gcn">2.1 Reformulation of Vanilla GCN</h3>
<p>作者首先从新的角度看原始GCN的公式：</p>
<p>原来GCN的公式：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210517192759415.png" style="zoom:50%;" /></p>
<p>通过引入一个得分函数，重新定义GCN，假设引入得分函数<span class="math inline">\(f\)</span>，该得分函数计算edge存在的score，对于已经存在的edge输出较大的值；对于不存在的边输出较小的值。假设<span class="math inline">\(f\)</span>为求内积： <span class="math display">\[
f(h_u,h_v)=h_u^T h_v
\]</span> 那么计算的消息<span class="math inline">\(h_u\)</span>能够看做是<span class="math inline">\(f\)</span>对<span class="math inline">\(v\)</span>的梯度，那么所有的<span class="math inline">\(h_u\)</span>加起来就成为下面的形式</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210517191155431.png" style="zoom:50%;" /></p>
<p>此时对于<span class="math inline">\(h_v+m_v\)</span>看做是learning rate为1，对<span class="math inline">\(h_v\)</span>的梯度提升；目的是使scoring function<span class="math inline">\(f\)</span>的值最大。</p>
<p>通过修改为上面的形式，能够看到，它从新的角度说明了GCN做了什么，邻居信息是如何提供给中心节点的，是如何帮助中心节点获得更好的表示的。</p>
<blockquote>
<p>The above reformulation provides an explicit view about what the vanilla GCN is optimizing, instead of how the updates are executed procedurally.</p>
</blockquote>
<h3 id="the-new-framework">2.2 The New Framework</h3>
<p>新的framework有两个核心部分，更新实体表示以及更新关系表示：</p>
<p>更新实体表示：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210517195223629.png" style="zoom:50%;" /></p>
<p>更新关系表示：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210517192820729.png" style="zoom:50%;" /></p>
<p>更新的关系表示实际提供了一种global的view。</p>
<p>整体结构：</p>
<p>消息传递过程：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210517190645145.png" style="zoom:50%;" /></p>
<p>消息聚合过程：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210517190907479.png" style="zoom:50%;" /></p>
<p>它能够泛化COMPGCN、R-GCN以及W-GCN。实际上COMPGCN本身就已经泛化了R-GCN和W-GCN，这部分泛化参考论文原文。</p>
<h2 id="experiments">3 EXPERIMENTS</h2>
<p>在实验的时候，对于KE-GCN，主要是引入不同的得分函数<span class="math inline">\(f\)</span>，该得分函数使用不同的KGE方法，并且为了简化模型，对于in，out，self-loop都使用了相同的<span class="math inline">\(W\)</span>和相同的得分函数<span class="math inline">\(f\)</span>。</p>
<p>使用了一系列的KGE方法：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210517195011713.png" style="zoom:50%;" /></p>
<h3 id="knowledge-graph-alignment">3.1 Knowledge Graph Alignment</h3>
<p>匹配不同KG中的实体和关系，在实验中直接计算embedding之间的L1-distance，</p>
<p>在这种情况下的loss function</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210517200018220.png" style="zoom:50%;" /></p>
<p>数据集，</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210517200444629.png" style="zoom:50%;" /></p>
<p>结果，这里只贴了KE-GCN的比较：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210517200123514.png" style="zoom:50%;" /></p>
<h3 id="knowledge-graph-entity-classification">3.2 Knowledge Graph Entity Classification</h3>
<p>在这种情况下的loss function</p>
<p>multi-class classification</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210517200315774.png" style="zoom:50%;" /></p>
<p>multi-label classification</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210517200151721.png" style="zoom:50%;" /></p>
<p>数据集，AM和WN是multi-class，FB15K是multi-label</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210517200250204.png" style="zoom:50%;" /></p>
<p>结果：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210517200606660.png" style="zoom:50%;" /></p>
<div class="pdf-container" data-target="KE-GCN.pdf" data-height="1000px"></div>
]]></content>
      <categories>
        <category>Paper</category>
        <category>KGE</category>
      </categories>
      <tags>
        <tag>GNN</tag>
        <tag>KGE</tag>
      </tags>
  </entry>
  <entry>
    <title>Knowledge-Graphs</title>
    <url>/kge/Knowledge-Graphs/</url>
    <content><![CDATA[<h1 id="knowledge-graphs">Knowledge Graphs</h1>
<p>Introduction of Knowledge Graphs.</p>
<span id="more"></span>
<blockquote>
<ol type="1">
<li>HOGAN A, BLOMQVIST E, COCHEZ M, et.al. Knowledge Graphs[J]. arXiv:2003.02320 [cs], 2020.</li>
<li>Knowledge Graph Embedding: A Survey of Approaches and Applications</li>
<li>东南大学知识图谱课程笔记，<a href="https://github.com/npubird/KnowledgeGraphCourse">原项目地址</a>。</li>
<li><a href="https://mp.weixin.qq.com/s/RK0bymmcXloxzCBYOPk6wg">从知识图谱到认知图谱： 历史、发展与展望</a></li>
<li><a href="https://web.stanford.edu/class/cs520/">StandFord的课程《knowledge graph》2020</a></li>
</ol>
</blockquote>
<h2 id="introduction">1. INTRODUCTION</h2>
<p>Knowledge graph这个词的现代含义是从2012年谷歌知识图谱（Google knowledge graph）提出后才具备的。谷歌使用知识图谱增强搜索引擎检索效果。</p>
<p>首先需要搞清楚几个重要的概念。</p>
<p>一、什么是知识？</p>
<p>有很多的定义</p>
<blockquote>
<ol type="1">
<li>Knowledge is a familiarity, awareness, or understanding of someone or something, such as facts, information, descriptions, or skills, which is acquired through experience or education by perceiving, discovering, or learning.——维基百科</li>
<li>知识包括两部分：已知的和未知的，实际对于所有涉及知识都是这两方面的研究</li>
</ol>
</blockquote>
<p>简单理解知识的例子：110，110本身是没有意义的，它只是计算机表示的符号。但是我们知道，认识到110可以是一百一十，可以是2进制，可以是中国的报警电话。它在不同的情景下有不同的具体含义，表达了除去110符号之外的现实含义。这就是知识。</p>
<p>二、什么是语义？</p>
<p>根据它原本的定义，语义就是语言包含的意义。语义可以简单地看作是数据所对应的现实世界中的事物所代表的概念的含义，以及这些含义之间的关系，是数据在某个领域上的解释和逻辑表示。</p>
<p>三、什么是本体？</p>
<p><strong>Ontology</strong>的定义，据维基百科</p>
<blockquote>
<p>In computer science and information science, an ontology is a <strong>formal naming and definition of the types, properties, and interrelationships of the entities</strong> that really or fundamentally exist for <strong>a particular domain</strong> of discourse. It is thus a practical application of philosophical ontology, with a taxonomy.</p>
</blockquote>
<p>具体的理解，引用知乎的<a href="https://www.zhihu.com/question/34835422/answer/60367501">答案</a></p>
<blockquote>
<p>我浅显的理解，本体就是这个知识库本身的<strong>存在，</strong>也就是知识库中对知识的一个定义，定义这个知识库中具体的每一个知识到底是什么。【本体论（英语：Ontology），又译存在论、存有论，它是形而上学的一个基本分支，本体论主要探讨存有本身，即一切现实事物的基本特征。】就好像有一匹马叫赤兔，那么马这个概念才是本体，赤兔红兔什么的无所谓；有一个美女叫貂蝉，那么美女这个概念才是本体，貂蝉西施啊什么的也无所谓。</p>
</blockquote>
<p>一句话概括本体，形式化、正式的概念定义。</p>
<p>四、本体、知识库（knowledge base）、知识图谱（knowledge graph）之间的关系？</p>
<p>还是引用知乎的<a href="https://www.zhihu.com/question/34835422/answer/144387604">答案</a></p>
<blockquote>
<p>从抽象层面看，本体最抽象，其次是知识库，最后才是知识图谱。举个例子，如果我们要做图书领域的知识库或者知识图谱，首先要对图书进行分类，这个分类就是本体，比如说，图书分为计算机类和电子类，计算机类有分为网络、人工智能；有了这个分类后，我们就可以把图书都分到每个类别，比如说《Zero to One》是一本进口原版书，然后这本书有各种属性－属性值，比如说书的作者是Peter Thiel，这些数据就构成了一个图书知识图谱（前面讲的分类可以认为不是这个知识图谱的一部分），而这里分类和知识图谱一起可以看成是一个图书知识库。也就是说，<strong>本体是强调概念关系</strong>，<strong>知识图谱强调实体关系和实体属性值</strong>，<strong>知识库则是所有知识的集合。但是知识库不局限于分类和图谱，知识库可以包括规则，包括过程性知识等</strong>。而本体也可以定义得很抽象，任何概念的内涵和外延可以定义本体。</p>
</blockquote>
<p>知识图谱获得了很多的研究关注，核心是在于使用图表示数据，进而使用图表示知识的思想。使用图表示知识的好处，和关系型的数据结构比较起来，有以下的优点：</p>
<ul>
<li>图能够提供很多领域下一种准确、直观的抽象。使用图描述数据的时候，可以一开始不确立一个准确的定义，而是随着发展慢慢的定义。</li>
<li>在图上也有查询语言，不仅能够支持传统的查询操作（join、unions、 projections），也能够在图上递归的查询。</li>
<li>标准的知识表示形式，比如ontologies和rule，都可以被用来描述和推理图中的节点</li>
</ul>
<p>knowledge graph的一个归纳的定义：</p>
<blockquote>
<p>A graph of data intended to accumulate and convey knowledge of the real world, whose nodes represent entities of interest and whose edges represent relations between these entities.</p>
</blockquote>
<p>在认识论里，对于知识已经有了很多的讨论。这里粗略的理解knowledge就是something that is known。知识图谱中的知识可以从外部累积，也可以从内部导出。使用Deductive methods/Inductive methods都可以导出新的知识。</p>
<blockquote>
<p>演绎Deductive是从一般到个别</p>
<p>归纳Inductive是从个别到一般</p>
</blockquote>
<p>知识图谱的构造可以用到很多数据来源，这些数据来源的结构、粒度各不相同。因此，构造知识图谱需要定义三方面的内容。</p>
<ol type="1">
<li>schema：知识图谱的高层次的结构定义</li>
<li>identity：确定知识图谱或不同来源的描述对象是否指向现实世界中相同的实体</li>
<li>context：context may indicate a specific setting in which some unit of knowledge is held true</li>
</ol>
<p>知识图谱不断成长和提升需要涉及的方面有：</p>
<ul>
<li>extraction</li>
<li>enrichment</li>
<li>quality assessment</li>
<li>refinement</li>
</ul>
<p>实践当中，按照知识图谱的开放程度，分为open knowledge graph和enterprise knowledge graphs两类。前者对公众开放，比如：DBpedia, Freebase, Wikidata, YAGO。后者主要是公司内部使用，Bing, Google, Airbnb, Amazon, eBay</p>
<p>实际上本质的，知识图谱获得成功要得益于它的特点——弱语义，多实例</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20200826100711774.png" style="zoom:50%;" /></p>
<p>不需要多强的语义，因为现实世界是如此的复杂，以至于我们不可能概括一个通用的、泛化性强的规则/语义去描述全部的现实世界。但语义又是必须的，没有语义的话，就没有知识，知识图谱的本身就不成立了。所以，知识图谱着眼于弱语义，同时看重实例的堆积，反而取得了成功，并且在快速的发展。</p>
<p>知识图谱的研究热点逐渐出现重数量轻结构化的倾向，</p>
<p>知识图谱怎么用？</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20200826153137057.png" style="zoom:50%;" /></p>
<p>宏观角度看知识图谱</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20200829195712010.png" style="zoom:50%;" /></p>
<p>知识图谱的缺点：</p>
<p>知识图谱的缺点本质上都是“⼆元⼀阶谓词逻辑”作为知识表示本身的缺陷带来的。词逻辑”作为知识表示本身的缺陷带来的。⻓久以来，知识 表示是研究者孜孜不倦追寻探索的话题，完全依靠（头实体，关系，尾实体）这样的命题，尽管能表示⼤部分简单事件或实体属性，但对于复杂知识却束手无策。</p>
<p>比如“克隆⽺的各项属性都与本体相同”，这个知识就无法被现有的知识图谱结构很好的记录。</p>
<p>另一方面，在构建知识图谱的过程中，从原始的语音、图像等的信息约减带来的实体链接困难。知识的最终来源是我们所处的世界，从原始的语⾳、图像等数据到⽂本再到知识图谱，信息量不断被约减，只有最核⼼的内容被保留下来。然⽽，忽略了原始⽂本中⼈物的具体经历等信息后，会导致同名⼈物难以消歧。</p>
<p>因此，现在有出现认知图谱，动力在于自然语言处理的进步，核心在于保持知识图谱的图结构带来的可解释性与精准稳定的推理能力的同时，带来推理能力的改变。</p>
<h2 id="data-graphs">2 DATA GRAPHS</h2>
<p>在这个部分我们讨论几种常见的知识图谱类型。</p>
<h3 id="directed-edge-labelled-graphs">2.1 Directed edge-labelled graphs</h3>
<p>是最常见，也是我们默认讨论知识图谱时具备的结构。它由节点集合与有向边集合组成。关系都是二元关系。一般使用RDF定义，可以进行各种查询操作。</p>
<h3 id="graph-dataset">2.2 Graph dataset</h3>
<p>A graph dataset then consists of a set of named graphs and a default graph.</p>
<p>即多个知识图的集合。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20200826105530023.png" style="zoom:50%;" /></p>
<h3 id="property-graphs">2.3 Property graphs</h3>
<p>属性图谱是在一般的知识图谱基础上改进的，A property graph allows a set of property–value pairs and a label to be associated with both nodes and edges.在边上添加了属性和标签。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20200829204546266.png" style="zoom:50%;" /></p>
<h3 id="other-graph-data-models">2.4 Other graph data models</h3>
<p>例如hypergraph：complex edges that connect sets rather than pairs of nodes</p>
<h2 id="deductive-knowledge">3 DEDUCTIVE KNOWLEDGE</h2>
<p>给予一些先验的知识，一些规则等，就可以在知识图谱上进行对于新知识的演绎。一些通用的规则，很多人都了解的知识被称为<em>commonsense knowledge</em>，相反的，只被相关领域的专家了解的知识叫做<em>domain knowledge</em>。</p>
<p>为了能够准确的构建知识图谱，我们必须首先确定涉及的term的准确定义，这就是本体<em>Ontologies</em>。描述本体的形式化语言是OWL（Web Ontology Language）。</p>
<p>在知识图谱中，普遍存在四种假设</p>
<ul>
<li>Closed World Assumption (CWA)：未知的知识一定不成立</li>
<li>Open World Assumption (OWA)：未知的知识有可能成立</li>
<li>Unique Name Assumption (UNA)：不同的实体一定对应现实世界的不同事物</li>
<li>No Unique Name Assumption (NUNA)：不同的实体可以对应现实世界的同一事物</li>
</ul>
<p>OWL是基于OWA，NUMA假设的。</p>
<p>演绎知识主要集中在推理方面，比如利用构建知识图谱时用到的本体，本体中的各种概念会构成一张图。利用预先知道的规则/逻辑，演绎推理出新的知识。这样得到的知识是很精确的。集中的表现是预先知道规则rule/描述逻辑Description Logics (DLs)，然后在graph中寻找能够匹配的现实。rule的一般表现是if-then类型的类似路径的序列。DL一般采用一阶逻辑FOL。</p>
<blockquote>
<p>A rule is composed of a body (if) and a head (then). Both the body and head are given as graph patterns. A rule indicates that if we can replace the variables of the body with terms from the data graph and form a subgraph of a given data graph, then using the same replacement of variables in the head</p>
</blockquote>
<blockquote>
<p>Description Logics (DLs) were initially introduced as a way to formalise the meaning of frames and semantic networks. Initially, DLs were restricted fragments of First Order Logic (FOL) that permit decidable reasoning tasks.</p>
</blockquote>
<h2 id="inductive-knowledge">4 INDUCTIVE KNOWLEDGE</h2>
<blockquote>
<p>inductively acquiring knowledge involves generalising patterns from a given set of input observations, which can then be used to generate novel but potentially imprecise predictions. Graph analytics is then the application of analytical processes to (typically large) graph data.</p>
</blockquote>
<p>由于一般归纳的知识再用于预测无法得到绝对正确的预测，因此一般提供<em>confidence</em>。</p>
<h3 id="graph-analytics">4.1 Graph analytics</h3>
<blockquote>
<p>Analytics is the process of discovering, interpreting, and communicating meaningful patterns inherent to (typically large) data collections</p>
</blockquote>
<p>各种的图分析技术：</p>
<ol type="1">
<li>Centrality：衡量节点的重要程度。to identify the most important (aka central) nodes or edges of a graph.</li>
<li>Community detection：探测节点之间的聚类。to identify communities in a graph, i.e., sub-graphs that are more densely connected internally than to the rest of the graph.</li>
<li>Connectivity：评估图的连通性。to estimate how well-connected the graph is, revealing, for instance, the resilience and (un)reachability of elements of the graph.</li>
<li>Node similarity：发现节点的相似程度。to find nodes that are similar to other nodes by virtue of how they are connected within their neighbourhood.</li>
<li>Path finding: 一般是发现两个节点之间的可能路径。to find paths in a graph, typically between pairs of nodes given as input.</li>
</ol>
<p>上述的图分析技术已经在图领域当中进行了深入的研究，存在很多的图框架适合于分析图，比如Apache Spark (GraphX), GraphLab, Pregel, Signal–Collect, Shark。</p>
<p>但是还不能直接把上面的图分析技术应用到知识图谱上，因为知识图谱是有向带标签的图。因此，有几种不同的方案处理这一问题。</p>
<ol type="1">
<li>Projection：移除边的类型得到一个无向或者有向图</li>
<li>Weighting：设计某些方法，将边的元数据转化为数字</li>
<li>Transformation：将原来的图进行转换——lossy/lossless，前者表示转化后的图无法复原原来的图；后者表示可以复原原来的图，比如把原来的边的标签表示为新的节点，新的边变为无标签的</li>
<li>Customisation：改变之前的图分析技术，加入对边的考虑</li>
</ol>
<p>具体选择哪种方法没有特定的方式，依赖于具体的技术。</p>
<p>一个可能的研究方向是结合语义和图分析技术，研究<em>semantically-invariant analytics</em>，因为之前的图分析技术无法利用具体的语义信息，比如逆关系/不可逆关系。</p>
<h3 id="knowledge-graph-embeddingskrl">4.2 Knowledge Graph Embeddings/KRL</h3>
<p>知识图谱嵌入的核心目的是创建出knowledge graph在低维、连续空间下的稠密表示。</p>
<p>它实际上是知识图谱表示学习目前的主流思想，都是把表示转化为embedding。基本可以划等号。</p>
<p><strong>表示学习</strong>：将研究对象的语义信息表示为稠密低维的实值向量，举例：文字、图片、语音</p>
<p><strong>知识表示学习</strong>：将知识库/知识图谱中的实体和关系表示为稠密低维的实值向量</p>
<p><strong>知识</strong>：知识图谱中的知识通常就是三元组（head, relation, tail）</p>
<p>知识表示中存在的问题，设想怎么样能够表示三元组？</p>
<ol type="1">
<li><p>0-1的onehot编码，每个实体/关系有唯一的编码，信息基本全丢失，不可用</p></li>
<li><p>直接使用图结构的图算法复杂度高</p></li>
<li><p>数据稀疏问题：长尾/重尾分布（大量的实体处在长尾上）</p></li>
</ol>
<p>研究知识图谱嵌入的意义：</p>
<ol type="1">
<li><p>低维向量提高计算效率</p></li>
<li><p>稠密的向量缓解数据稀疏问题</p></li>
<li><p>多源的异质信息表示形式统一，便于迁移和融合</p></li>
</ol>
<p>一般的评测任务</p>
<ul>
<li>链路预测</li>
<li>实体分类</li>
<li>图分类</li>
</ul>
<p>知识图谱嵌入一般的应用：</p>
<ul>
<li>知识融合，如Cross-lingual Entity Alignment via Joint Attribute-Preserving Embedding</li>
<li>人机交互，如Commonsense Knowledge Aware Conversation Generation with Graph Attention</li>
</ul>
<p>根据经典的survey，中所有的KGE从利用信息的角度分为两类：KG embedding with facts alone（仅使用facts）和Incoprorating additional information（多源信息融合）。从使用的模型核心方法的角度讲，根据[1]又可以划分为Translational models、Tensor decomposition models、Neural models。实际上人们也经常根据得分函数scoring function，直接分为Translational Model和Semantic Matching Models，前者的scoring计算entity embedding之间基于关系的距离，后者的scoring function基于相似度计算事实成立的得分。</p>
<blockquote>
<p><em>Translational distance models</em> exploit distance-based scoring functions. They measure the plausibility of a fact as the distance between the two entities, usually after a translation carried out by the relation.</p>
<p><em>Semantic matching models</em> exploit similarity-based scoring functions. They measure plausibility of facts by matching latent semantics of entities and relations embodied in their vector space representations.</p>
</blockquote>
<p>在这里，我们主要关心方法本身，根据模型进行分类。</p>
<h4 id="基于翻译的模型">4.2.1 基于翻译的模型</h4>
<p>起始于TransE，</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20200826152742454.png" style="zoom:50%;" /></p>
<p>TransE的不足：</p>
<ul>
<li>首先，TransE严格要求有效的三元组满足头实体加关系在向量空间中与尾实体足够靠近，可以很好地处理一对一关系，但在处理多映射属性关系时，存在多个实体竞争一个点的现象．例如在一对多关系中，同一个头实体<span class="math inline">\(h\)</span>与多个尾实体<span class="math inline">\(\{t_1,t_2\cdots\}\)</span>存在关系<span class="math inline">\(r\)</span>，由于<span class="math inline">\(h+r=t_i,f=\{1,2,\cdots\}\)</span>的约束，这些尾实体将竞争空间中的同一个点，即使它们语义差别很大，因而会造成向量空间的拥挤和误差；</li>
<li>其次，TransE只专注于满足知识图谱中的三元组约束，然而知识图谱中存在大量层级关系，例如在知识图谱WordNet的子集WN18中，大约有50%的层级关系，孩子关系就是一种层级关系，对应的实体和关系形成树结构．Li等人也指出，考虑层级结构有助于推理：</li>
<li>然后，TransE没有考虑丰富的语义信息，缺乏对空间中向量分布位置的进一步调整；</li>
<li>再者，TransE在单个知识图谱上进行学习推理，而单个知识图谱知识量有限；</li>
<li>此外，TransE参数的选取与知识图谱数据独立，不能反映数据的特点；</li>
<li>同时，TransE未考虑知识的时间约束。</li>
</ul>
<p>因此，诞生了一系列的Trans系列的方法。TransH, TransR, CTransR, TransD, TranSparse, TransM, ManiFoldE, TransF, TransA, KG2E, TransG。</p>
<h4 id="张量分解模型">4.2.2 张量分解模型</h4>
<p>这一方法的核心是将整个图表示为一个张量<span class="math inline">\(G\)</span>（tensor，就是多阶向量），0/1表示某个边是否存在，之后将所有的实体和关系表示为<span class="math inline">\(d\)</span>维的embedding，每个维度都代表某种能够对预测结果产生影响的latent factor，这样理由d个latent factor去尝试重建表示图的张量<span class="math inline">\(G\)</span>。</p>
<blockquote>
<p>A tensor is a multidimensional numeric field that generalises scalars (0-order tensors), vectors (1-order tensors) and matrices (2-order tensors) towards arbitrary dimension/order.</p>
</blockquote>
<p>下图是关于CP d-rank分解的实例，图中的<span class="math inline">\(x_i,y_i, z_i\)</span>表示的是在<span class="math inline">\(x,y,z\)</span>三个方面的第<span class="math inline">\(i\)</span>种latent factor。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20200826105942543.png" style="zoom:50%;" /></p>
<p>这一类型的方法有LFM， RESCAL，DistMult，ComplEx，ANALOGY，HolE。</p>
<h4 id="神经网络模型">4.2.3 神经网络模型</h4>
<p>上面的这些方法都是linear或者billear的scoring function，之后就有人使用non-linear的神经网络方法学习embedding。</p>
<p>一般的神经网络方法包括SME、NTN、MLP。但它们的问题在于容易过拟合，如果想要效果较好就需要较大的embedding维度，但这毫无疑问是不可接受的（由于数量巨大的实例数量）。</p>
<p>之后就出现了很多其它网络模型，包括使用CNN、RNN、GNN等。</p>
<p>CNN在知识图谱上的应用源于ConvE，之后发展出一系列的方法，ConvR、ConvKB、HypER、InteractE等。</p>
<p>使用RNN实际上是受到了word embedding的启发。例如基于word2vec和random work找寻路径的RDF2Vec，融合GloVe和PageRank的KGloVe。</p>
<p>基于GNN是得益于最近图神经网络的突飞猛进，模型有R-GCN，VR-GCN，COMPGCN，KBGAT等。这个方向还有很多可以研究的内容。</p>
<p>对于GNN目前没有统一全面的认识，简单说明，主要有两类GNN，一类是Recursive graph neural networks，一类是Convolutional graph neural networks。</p>
<p>Recursive graph neural networks的思想起源于The Graph Neural Network Model（2009），核心是对于graph中的每一个node/edge都有一个feature vector和一个hidden state vector，state vector不断aggregate邻居节点的信息获得新的state vector，使用transition function和邻居节点的feature vector构造要传递的信息，之后使用output function输入新的state vector得到最终的预测结果。至于什么时候停止要看是否能够达到fixpoint。</p>
<p>Convolutional graph neural networks是模拟了CNN处理图像的思想，但是与图像的区别在于无法很好的定义neighbour。这个方向又可以分为基于spectral和基于spatial的方法。</p>
<p>两类GNN方法的区别：</p>
<blockquote>
<p>There are two main differences between RecGNNs and ConvGNNs.</p>
<ul>
<li><p>First, RecGNNs aggregate information from neighbours recursively up to a fixpoint, whereas ConvGNNs typically apply a fixed number of convolutional layers.</p></li>
<li><p>Second, RecGNNs typically use the same function/parameters in uniform steps, while different convolutional layers of a ConvGNN can apply different kernels/weights at each distinct step.</p></li>
</ul>
</blockquote>
<h4 id="融合多元信息的模型">4.2.5 融合多元信息的模型</h4>
<p>这里的多源是指除单纯的三元组之外的信息</p>
<ol type="1">
<li>实体类别：同一类别的实体在表示空间中应该距离较近
<ul>
<li>SSE、TKRL</li>
</ul></li>
<li>关系路径：三元组与三元组可以拼接为路径
<ul>
<li>pTransE</li>
</ul></li>
<li>文本描述：每个实体/关系都有自己名字
<ul>
<li>NTN：先用新闻语料库对词汇进行表示，然后用词汇表示初始化实体的表示</li>
</ul></li>
<li>逻辑规则：融合逻辑规则，学习知识表示
<ul>
<li>ILP、KALE</li>
</ul></li>
<li>实体属性：大部分KGE将实体属性也看做关系处理，但是有时并不合理。例如（Obama，gender，male）</li>
<li>时序信息：在三元组的基础上添加时间维度</li>
</ol>
<h3 id="symbolic-learning">4.4 Symbolic Learning</h3>
<p>之前谈到的演绎的知识，它的规则或者DL都是可以通过已有的知识图谱中归纳出来的。如果我们能够挖掘出这样的规则，那么就可以为我们预测未知的链接是否存在提供可解释的依据。只不过是这样归纳的规则不一定是正确的。</p>
<p>上面自动归纳规则的方法就是Symbolic Learning：</p>
<blockquote>
<p>An alternative (sometimes complementary) approach is to adopt symbolic learning in order to learn hypotheses in a symbolic (logical) language that “explain” a given set of positive and negative edges.</p>
</blockquote>
<p>主要分为两大类，Rule mining和Axiom mining。</p>
<p>Rule mining的主要目标：</p>
<blockquote>
<p>The goal of rule mining is to identify new rules that entail a high ratio of positive edges from other positive edges, but entail a low ratio of negative edges from positive edges.</p>
</blockquote>
<p>从Inductive Logic Programming (ILP)开始已经有很多对于规则挖掘的探讨，但是由于知识图谱的规模大以及不完备性，又有很多新的专用方法出现。</p>
<p>一个非常重要的方法是AMIE，它是利用启发式的方法不断构造新的规则，然后判断这些规则在已有的facts中是否成立，confidence是否够高。之后还有很多的改进方法比如AMIE+，Gad-Elrab，RuLES，CARL等。</p>
<p>另外一种流派是认为矩阵乘法可以表示规则推导，叫做differentiable rule mining，The core idea is that the joins in rule bodies can be represented as matrix multiplication。这一类的方法有NeuralLP，DRUM等。</p>
<p>Axiom mining是自动挖掘DL的方法。</p>
<h2 id="quality-assessment">5 QUALITY ASSESSMENT</h2>
<p>在知识图谱创建或者从不同的来源获得更新之后，需要对知识图谱的质量进行评估。</p>
<p>四个被广泛使用的评估维度</p>
<h3 id="accuracy">5.1 Accuracy</h3>
<p>准确度是指要求知识图谱中的实体和关系能够准确的反映现实世界中的事物。</p>
<blockquote>
<p>Accuracy refers to the extent to which entities and relations – encoded by nodes and edges in the graph – correctly represent real-life phenomena.</p>
</blockquote>
<ul>
<li>Syntactic accuracy：语法的准确性是指知识图谱中的数据是不是准确符合定义的语法。一种度量指标就是错误的实例的比例。</li>
<li>Semantic accuracy：语义准确性是指数据是否正确的对应了现实，比如由于不恰当的知识抽取导致出现了错误的实例。一种度量的方法是拿知识图谱和多个对应的数据来源自动进行对比。</li>
<li>Timeliness：时间线是指知识图谱能够及时得到更新的度量。可以用多久进行一次更新进行度量。</li>
</ul>
<h3 id="coverage">5.2 Coverage</h3>
<p>覆盖范围/覆盖程度主要是指与领域相关的信息丢失的程度。</p>
<blockquote>
<p>Coverage refers to avoiding the omission of domain-relevant elements, which otherwise may yield incomplete query results or entailments, biased models, etc</p>
</blockquote>
<ul>
<li>Completeness：完备性是指知识图谱包含所有信息的程度。包括预先定义的schema是否完备（schema completeness）、属性对应的值是否完备（property completeness）、现实当中所有的实体和关系是否完备（population completeness）、已有实体之间的联系是否完备（linkability completeness）。</li>
<li>Representativeness：代表性是关注当前的知识图谱的biases。比如人口方面的知识图谱偏向反映某个地区/人种的信息。一种度量方式是拿知识图谱与当前已知的统计分布进行对比。</li>
</ul>
<h3 id="coherency">5.3 Coherency</h3>
<p>一致性是指知识图谱中的实例在语义/约束是否一致。</p>
<blockquote>
<p>Coherency refers to how well the knowledge graph conforms to – or is coherent with – the formal semantics and constraints defined at the schema-level.</p>
</blockquote>
<ul>
<li>Consistency：一致性是指知识图谱内部信息是否互相矛盾。</li>
<li>Validity：有效性是指知识图谱是否与预先的约束冲突。</li>
</ul>
<h3 id="succinctness">5.4 Succinctness</h3>
<p>简洁性是指要求知识图谱只包含准确，足够的内容，避免冗余信息。</p>
<blockquote>
<p>Succinctness refers to the inclusion only of relevant content (avoiding “information overload”) that is represented in a concise and intelligible manner.</p>
</blockquote>
<ul>
<li>Conciseness：简洁性是指知识图谱的schema和已有的数据是否存在于领域无关的情况。</li>
<li>Representational-conciseness：代表的简洁性是要求知识图谱已有数据是紧密联系的，避免出现同一个概念不同的实体/关系。</li>
<li>Understandability：可理解性是指知识图谱的内容能够无歧义的被人类理解。</li>
</ul>
<h2 id="review-previous-work">Review previous work</h2>
<p>现在回顾之前的工作，用的方法是神经网络模型，主要利用GNN做encoder，CNN做decoder，最后输出semantic socres。预测任务是link prediction，设想的场景是knowledge graph completion，该场景属于对于已有知识图谱质量中完备性的提升（refinement），方法是补全已有的实体和关系之间的联系。核心是学习knowledge graph embedding，学习到的是inductive knowledge，没有涉及多元信息的融合。</p>
<p>可以改进的方向：</p>
<ol type="1">
<li>从GNN的角度入手，目前可以想到的是改进注意力机制</li>
<li>从知识图谱角度入手，比如考虑在聚合一阶邻居的时候区分出属性和一般的关系、还可以考虑加入多步路径</li>
</ol>
]]></content>
      <categories>
        <category>Paper</category>
        <category>KG</category>
      </categories>
  </entry>
  <entry>
    <title>MR-GCN</title>
    <url>/kge/MR-GCN/</url>
    <content><![CDATA[<h1 id="mr-gcn-multi-relational-graph-convolutional-networks-based-on-generalized-tensor-product">MR-GCN: Multi-Relational Graph Convolutional Networks based on Generalized Tensor Product</h1>
<p>IJCAI 2020</p>
<p>2020-7</p>
<p>作者在定义了在multi-relational graph中的卷积操作，命名为MR-GCO（multirelational graph convolution operators），提出了一个可以用来做node classification的网络MR-GCN。</p>
<span id="more"></span>
<p><strong>motivation</strong>：现在的几个解决multi relation的GCN模型，倾向于在不同relation的graph中执行GCN卷积操作，然后混合（blending）结果。作者认为这种做法忽略了relation之间的correlation。</p>
<p>现有的处理multi-relational graph的方法主要有两种思路：</p>
<ol type="1">
<li>每个relation graph下进行GCN，然后融合，比如R-GCN，mGCN，Megan等，在这种情况下relation之间的correlation无法被显式的捕获。</li>
</ol>
<blockquote>
<p>The ﬁrst line conducts GCN on each single relation and then integrates the results with multi-view learning.</p>
</blockquote>
<ol start="2" type="1">
<li>另一种思路是把multi-relational graph转换为一个同质图homogeneous graph。比如Multi-GCN等。这一类方法存在的问题是可能存在信息的损失等。</li>
</ol>
<blockquote>
<p>Another line is aggregating the multi-relational graph into a homogeneous graph.</p>
</blockquote>
<p><strong>methods</strong>：作者在定义了在multi-relational graph中的卷积操作，命名为MR-GCO（multirelational graph convolution operators），提出了一个可以用来做node classification的网络MR-GCN。</p>
<p><strong>contribution</strong>：</p>
<ul>
<li>首个通过tensor eigen-decomposition，从GCNs spectral graph theory发展到multi-relational graphs下的convolution operator。</li>
<li>MR-GCN是通过定义generalized tensor product的tensor eigen-decomposition进行的，因此除了可以基于快速傅里叶变换进行外，还可以融合Haar, Discrete Cosine transform (DCT)等。</li>
</ul>
<p>MR-GCO是作者论文的核心，主要思想从推导过程来看和一般情况下的图卷积是一致的。</p>
<p>由于对泛化的张量积的核心思想不了解，因此无法确切的理解作者的操作。</p>
<p>MR-GCO的定义，对于图信号 <span class="math inline">\(x\in \mathbb{R}^{N\times R}\)</span>和过滤器<span class="math inline">\(g\in \mathbb{R}^{N\times R}\)</span>，作者定义为：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210626164638896.png" style="zoom:50%;" /></p>
<p>其中<span class="math inline">\(U\)</span>是对于多关系拉普拉斯矩阵进行张量特征分解后的结果：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210626164341117.png" style="zoom:50%;" /></p>
<p>其中的特殊运算 <span class="math inline">\(\diamondsuit_\Phi\)</span>就是泛化的张量积<span class="math inline">\(\Phi\)</span>-product，<span class="math inline">\(\Phi\)</span>是转化矩阵。</p>
<p>其中的拉普拉斯矩阵是多关系的三阶张量，每个relation有一个不同的拉普拉斯矩阵</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210626165323451.png" style="zoom:50%;" /></p>
<p>直接看作者结果</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210626164015015.png" style="zoom:50%;" /></p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>GNN</category>
      </categories>
      <tags>
        <tag>GNN</tag>
      </tags>
  </entry>
  <entry>
    <title>NSCaching</title>
    <url>/kge/NSCaching/</url>
    <content><![CDATA[<h1 id="nscaching-simple-and-efﬁcient-negative-sampling-for-knowledge-graph-embedding">NSCaching: Simple and Efﬁcient Negative Sampling for Knowledge Graph Embedding</h1>
<p>ICDE 2019</p>
<p>提出了一种针对KGE的动态负采样方法<a href="https://github.com/yzhangee/NSCaching">NSCaching</a>，核心思想是得分高的负样本很重要但是数量少，因此，作者直接使用cache来保存得分高的负样本，同时随着训练动态更新cache，可以看做是基于GAN的负采样方法的distilled版本。</p>
<span id="more"></span>
<h2 id="introduction">Introduction</h2>
<p><strong>motivation</strong>：在训练KGE的时候，负样本的质量很重要，也就是说那些越难与正样本区分的负样本可能越重要。<em>high-quality negative triplets should have large scores</em>，因为基于embedding的model实际上对于大多数负样本不敏感，给出的都是比较低的打分。如果使用random采样，采样得到的负样本，激活函数如果是sigmoid函数，那么如果负样本得分在&lt;&lt;0的区间内，那么梯度会很小，造成梯度消失的问题。</p>
<p>下面的图分析了负样本得分与正样本得分差距的情况。红线右侧这一部分是值得训练的负样本。越大的margin表示负样本与正样本越相等，越有训练的价值，随着训练的进行，这一部分的负样本越来越少。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210722174158272.png" alt="image-20210722174158272" style="zoom:50%;" /></p>
<p>目前有方法比如KBGAN等，尝试使用GAN解决上面的问题，但是GAN首先会引入额外的训练参数；同时GAN的训练存在instability和degeneracy的问题，并且它们可能有更高的variance，导致训练结果更不稳定。</p>
<p><strong>method</strong>：高质量的负样本数量并不多，分布上来看是一个很skew的曲线，因此可以使用cache来保存高质量的负样本，同时随着训练，不断更新这些负样本。</p>
<h2 id="method">Method</h2>
<p>方法很直观，为每个head和tail保存高质量负样本cache，负样本的质量用上一步训练的模型对它的预测结果进行衡量。从cache中随机选择head或者tail entity构造负样本。然后用于KGE model进行训练。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210722173128966.png" style="zoom:50%;" /></p>
<p>step 6和step 8是重点。</p>
<p><strong>Uniform sampling strategy from the cache</strong></p>
<p>由于负样本cache中的实体都能够用来构造高质量负样本，同时因为最大得分的负样本也可能是假阴性样本，因此不应该总是采样最大得分负样本，直接使用Uniform sampling来控制false negative triplets。</p>
<p><strong>Importance sampling strategy to update the cache</strong></p>
<p>对于head cache和tail cache都是使用一样的更新过程。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210722173851214.png" alt="image-20210722173851214" style="zoom:50%;" /></p>
<p>从所有实体中选择<span class="math inline">\(N_2\)</span>个实体作为更新候选项并入cache中，然后基于相对重要性采样<span class="math inline">\(N_1\)</span>个实体作为更新后的cache。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210722173831073.png" alt="image-20210722173831073" style="zoom:50%;" /></p>
<p>这一步没有总是保留最大得分的<span class="math inline">\(N_1\)</span>​个实体也是为了控制假阴性样本。因为假阴性负样本的存在，总是使用top N最大得分的负样本也不合适。</p>
<blockquote>
<p>NSCaching will learn from easy samples ﬁrst, but then gradually focus on hard ones, which is exactly the principle of self-paced learning</p>
</blockquote>
<p>在作者实验中，采用<span class="math inline">\(N_1=N_2=50\)</span>，负样本数量为1。</p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>KGE</category>
      </categories>
      <tags>
        <tag>KGE</tag>
      </tags>
  </entry>
  <entry>
    <title>PATHCON</title>
    <url>/kge/PATHCON/</url>
    <content><![CDATA[<h1 id="relational-message-passing-for-knowledge-graph-completion">Relational Message Passing for Knowledge Graph Completion</h1>
<p>在这篇论文中，作者只考虑了KG中的relation embedding，没有学习entity embedding。更具体的说，学习两个方面的结构信息，relational context和relation paths。前者是头/尾实体的邻居relation，后者是头尾实体在KG中相连的relational path。提出了<a href="https://github.com/hwwang55/PathCon">PATHCON</a></p>
<p>作者预测的是relation prediction，<span class="math inline">\(&lt;h,?,t&gt;\)</span>，区别于常见的head/tail prediction，这样情况下relation prediction的候选项是所有的relation，会少很多候选项。这篇文章，作者还提出了一个新的数据集，DDB14，基于医药和疾病的一个知识图谱。</p>
<span id="more"></span>
<h2 id="introduction">1 Introduction</h2>
<p><strong>motivation</strong>：作者认为实体的周围关系有很丰富的信息，可以使用GNN来学习这样的邻居结果。但是一般的KG上的GNN是迭代的将消息从实体传递到另外的实体。作者认为KG中的relation应该起到更大的作用。</p>
<p><strong>method</strong>：作者提出了一种relational message passing的方法，只考虑relation embedding，然后让messages在relation之间传播。同时，为了降低计算复杂度，作者提出了改进版alternate relational message passing，让relation先传递给entity，再传递给relation。</p>
<p>作者认为重点建模relation而不是entity有三方面的好处：</p>
<blockquote>
<ol type="1">
<li><p>it is inductive, since it can handle entities that do not appear in the training data during inference stage;</p></li>
<li><p>it is storage-efficient, since it does not calculate embeddings of entities; and</p></li>
<li><p>it is explainable, since it is able to provide explainability for predicted results by modeling the correlation strength among relation types.</p></li>
</ol>
</blockquote>
<p>这篇文章，作者还提出了一个新的数据集，DDB14。</p>
<h2 id="method">2 Method</h2>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210807105609410.png" alt="image-20210807105609410" style="zoom:50%;" /></p>
<p><strong>Alternate relational message passing</strong></p>
<p>为了降低以edge为底的聚合方法的计算复杂度（作者提供了计算复杂度的计算公式，没有细看），提出了交替的关系消息传递函数。让relation先传递给entity，再传递给relation。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210807103941361.png" alt="image-20210807103941361" style="zoom:50%;" /></p>
<p>注意，这个公式里面的，<span class="math inline">\(e\)</span>和<span class="math inline">\(v\)</span>表示边relation和节点entity。<span class="math inline">\(N(v)\)</span>不是表示实体<span class="math inline">\(v\)</span>的邻居实体，而是实体<span class="math inline">\(v\)</span>的邻居关系。<span class="math inline">\(A_1,\ A_2\)</span>是两个构造函数。</p>
<p>接下来解释作者使用到的两个KG上的关系信息</p>
<h3 id="relational-context">Relational Context</h3>
<p>头尾实体的周围关系。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210807104339704.png" alt="image-20210807104339704" style="zoom:50%;" /></p>
<h3 id="relational-paths">Relational Paths</h3>
<p>头尾实体在KG上可以相连的路径，注意，为了限制路径数量，该路径被限制为各个节点的实体在这个路径上是唯一的，这样避免出现循环圈这样的情况。</p>
<p>对于一个路径<span class="math inline">\(p\)</span>​​，作者只保留中间的各个relation，然后给这样的relation赋予一个独立的embedding <span class="math inline">\(s_p\)</span>​，而不是去用某种方式产生。</p>
<p>虽然这种做法看起来会导致参数量爆炸，实际上作者发现出现的relational path数量并不多，单独赋予embedding，是可以接受的。</p>
<h3 id="combining-relational-context-and-paths">Combining Relational Context and Paths</h3>
<p>对于要预测的三元组<span class="math inline">\(&lt;h,?,t&gt;\)</span>，首先产生一个relation context</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210807105123264.png" alt="image-20210807105123264" style="zoom:50%;" /></p>
<p>之后，基于注意力聚合Relational Paths</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210807105237434.png" alt="image-20210807105237434" style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210807105253024.png" alt="image-20210807105253024" style="zoom:50%;" /></p>
<p>最终的预测输出</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210807105324839.png" alt="image-20210807105324839" style="zoom:50%;" /></p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>GNN</category>
      </categories>
      <tags>
        <tag>GNN</tag>
        <tag>KG</tag>
      </tags>
  </entry>
  <entry>
    <title>ParamE</title>
    <url>/kge/ParamE/</url>
    <content><![CDATA[<h1 id="parame">ParamE</h1>
<p>这篇文章提出了新的KGE方法叫做ParamE，既能够建模关系的翻译属性，又能够利用非线性的联系。</p>
<p>最大的创新点在于<strong>将神经网络的参数看做是relation embedding</strong>，在实验中，为了验证，设计了三个方法，ParamE-MLP，ParanE-CNN，ParamE-Gate</p>
<p>实际是为所有的relation都设计了单独的神经网络。</p>
<span id="more"></span>
<p>最后得到的表示投影到tail entity embedding space中，类似于ConvE，与所有tail entity embedding相乘，经过sigmoid得到预测得分。</p>
<p>ParamE-MLP：三层，每一层都有W和b，维度是200，400，800，输入只有head entity embedding，激活函数relu；</p>
<p>ParamE-CNN：两层卷积层后变为vector，激活函数relu，第一层卷积核32个，第二层64个，都是3x3卷积核；</p>
<p>ParamE-Gate：使用了一个gate。</p>
<p>训练过程：每个epoch训练固定次数n，每次都按照比例抽取某一组关系，从中随机抽取batch size大小的triples。</p>
<p>在WN18RR和FB15k-237上实验，效果很好，在FB15k-237上的MRR达到了0.399，最好的模型是ParamE-Gate。</p>
<p>优点：将relation embedding作为network parameters，能够充分的捕获关系和实体之间的交互，也能够表示“翻译”这种属性。</p>
<p>缺点：如果模型很复杂，那肯定不能将所有网络的参数都作为relation embedding。可以尝试在哪些模型中哪一部分适合作为由relation embedding构建的参数，哪些不适合。</p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>KGE</category>
      </categories>
  </entry>
  <entry>
    <title>R-GCN</title>
    <url>/kge/R-GCN/</url>
    <content><![CDATA[<h1 id="modeling-relational-data-with-graph-convolutional-networks">Modeling Relational Data with Graph Convolutional Networks</h1>
<p>2018</p>
<p>首个在KG上应用GNN的模型R-GCN</p>
<span id="more"></span>
<h2 id="introduction">1 Introduction</h2>
<p>一个物品很多的信息隐藏在它的邻居内。R-GCN设计了一个encoder model，可以和其它的tensor factoraction model结合作为decoder。</p>
<p>R-GCN使用了DisMult作为decoder。</p>
<p>在FB15k-237，FB15k，WN18上面都进行了实验。</p>
<h2 id="neural-relational-modeling">2 Neural Relational Modeling</h2>
<h3 id="relational-graph-convolutional-networks">2.1 Relational Graph Convolutional Networks</h3>
<p>一般的GCN的形式可以定义为 <span class="math display">\[
h_i^{l+1}=\sigma(\sum_{m \in M_i}g_m(h_i^{l}, h_j^{l}))
\]</span> <span class="math inline">\(g_m\)</span>可以是neural network，也可以是简单的线性转换，<span class="math inline">\(g_m(h_i, h_j)=Wh_j\)</span></p>
<p>基于以上的原理，设计了如下的传播层： <span class="math display">\[
h_i^{l+1}=\sigma(\sum_{r\in R}\sum_{j\in N_i^r} \frac{1}{c_{i,r}} W_r^{l}h_j^{l} + W_o^l h_i^l)
\]</span> 公式中的<span class="math inline">\(c_{i,r}\)</span>可以为<span class="math inline">\(|N_i^r|\)</span>，某个关系r的邻居的数量。</p>
<p><em>以下公式非论文原本内容</em> <span class="math display">\[
h_i^{l+1}=\sigma(\sum_{r\in R}\sum_{j\in N_i^r} \frac{1}{\sqrt{c_{i}} \sqrt{c_{j}}} W_r^{l}h_j^{l})
\]</span></p>
<p><span class="math display">\[
h_i^{l+1}=\sigma(\sum_{r\in R}\sum_{j\in N_i^r} \frac{1}{\sqrt{c_{i}} \sqrt{c_{j}}} W_r^{l} [h_j^{l},e_r^{l}])
\]</span></p>
<h3 id="regularization">2.2 Regularization</h3>
<p>这样设计导致了不同层的不同关系都有不同的weight matrix，在large scale knowledge graph下会导致快速增加的参数数量，导致过拟合。</p>
<p>为此，R-GCN使用了两种正则方式，都是针对<span class="math inline">\(W_r^l\)</span>进行改进。</p>
<p>basis- and block-diagonal decomposition</p>
<p><strong>1、basis decomposition</strong>： <span class="math display">\[
W_r^l=\sum_b^B a_{r,b}^l V_b^l
\]</span> <span class="math display">\[
V_b^l \in R^{d^{(l+1)}\times (d^l)}
\]</span></p>
<p>这种情况下<span class="math inline">\(W_r^l\)</span>成为线性组合同一层的不同关系，能够共享<span class="math inline">\(V_b^l\)</span>，区别在于<span class="math inline">\(a_{r,b}^l\)</span>。</p>
<blockquote>
<p>The basis function decomposition can be seen as a form of eﬀective weight sharing between different relation types</p>
</blockquote>
<p>这种方式可以看做是有<span class="math inline">\(B\)</span>个矩阵<span class="math inline">\(V\)</span>，然后与邻居实体embedding相乘，得到<span class="math inline">\(B\)</span>个message embedding，然后对于不同的关系使用权重<span class="math inline">\(a_{1,b}^l,\dots,a_{B,b}^l\)</span>去聚合。</p>
<p><strong>2、block-diagonal decomposition</strong> <span class="math display">\[
W_r^l=\oplus_b^B Q_{b,r}^l
\]</span></p>
<p><span class="math display">\[
W_r = diag(Q_{1r}^{l},\cdots,Q_{Br}^{l})\ with\ Q_{br}^{l} \in \mathbb{R}^{(d^{l+1}/B)\times (d^{l}/B)}
\]</span></p>
<p>其中符号<span class="math inline">\(\oplus\)</span>是矩阵加法中的Direct Sum，不是普通的相加，而是下面的形式，</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210421222339835.png" style="zoom:50%;" /></p>
<p>这里说明下block-diagonal matrix，根据维基百科的解释</p>
<blockquote>
<p>A <strong>block diagonal matrix</strong> is a block matrix that is a <a href="https://en.wikipedia.org/wiki/Square_matrix">square matrix</a> such that the main-diagonal blocks are square matrices and all off-diagonal blocks are zero matrices.</p>
</blockquote>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210421222219414.png" style="zoom:50%;" /></p>
<p>对于这种正则化方式的理解</p>
<blockquote>
<p>The block decomposition can be seen as a sparsity constraint on the weight matrices for each relation type.</p>
</blockquote>
<p>它与bias decomposition的区别是它没有设置共享参数的结构，而是直接使用更加系数的<span class="math inline">\(W_r\)</span>去拟合。</p>
<h2 id="entity-classiﬁcation">3 Entity Classiﬁcation</h2>
<p>对于实体分类，就是将entity 分类为K个class当中，那么在R-GNN的基础上，直接在最后一层的输出增加softmax就可以，训练时的loss为 <span class="math display">\[
L=-\sum_{i\in Y}\sum_{k=1}^{K}t_{ik}lnh_{ik}^L
\]</span> <span class="math inline">\(Y\)</span>是所有有label的entity集合。</p>
<h2 id="link-prediction">4 Link Prediction</h2>
<p>要进行Link Prediction，在R-GCN的基础上需要设计一个score function。</p>
<p>论文直接使用了DistMult作为decoder， <span class="math display">\[
f(s,r,o)=e_s^TRe_o
\]</span> 训练的loss为 <span class="math display">\[
L=-\frac{1}{(1+w)|\varepsilon|}\sum_{(s,r,o,y)\in \Gamma}{ylog(f(s,r,o)) + (1-y)log(1-f(s,r,o)) }
\]</span> 前面的系数为归一系数，<span class="math inline">\(w\)</span>为对于每一个postivite sample取<span class="math inline">\(w\)</span>个negative samples，<span class="math inline">\(|\varepsilon|\)</span>为所有实体的个数。</p>
<h2 id="empirical-evaluation">5 Empirical Evaluation</h2>
<h3 id="entity-classiﬁcation-experiments">5.1 Entity Classiﬁcation Experiments</h3>
<p>数据集</p>
<ul>
<li>AIFB</li>
<li>MUTAG,</li>
<li>BGS</li>
<li>AM</li>
</ul>
<p>超参设计</p>
<ul>
<li>2-layer model with 16 hidden units</li>
<li>basis function decomposition</li>
<li>Adam，learning rate of 0.01</li>
</ul>
<p>Baseline：</p>
<ul>
<li><p>RDF2Vec embeddings</p></li>
<li><p>WeisfeilerLehman kernels (WL)</p></li>
<li><p>hand-designed feature extractors (Feat)</p></li>
</ul>
<h2 id="link-prediction-experiments">5.2 Link Prediction Experiments</h2>
<table>
<thead>
<tr class="header">
<th>Dataset</th>
<th>WN18</th>
<th>FB15K</th>
<th>FB15k-237</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Entities</td>
<td>40,943</td>
<td>14,951</td>
<td>14,541</td>
</tr>
<tr class="even">
<td>Relations</td>
<td>18</td>
<td>1,345</td>
<td>237</td>
</tr>
<tr class="odd">
<td>Train edges</td>
<td>141,442</td>
<td>483,142</td>
<td>272,115</td>
</tr>
<tr class="even">
<td>Val. edges</td>
<td>5,000</td>
<td>50,000</td>
<td>17,535</td>
</tr>
<tr class="odd">
<td>Test edges</td>
<td>5,000</td>
<td>59,071</td>
<td>20,466</td>
</tr>
</tbody>
</table>
<p>FB15k-237是FB15K的reduced版本，去除了所有的inverse relation。</p>
<p>评估指标：</p>
<ul>
<li>MRR</li>
<li>HIT@</li>
</ul>
]]></content>
      <categories>
        <category>Paper</category>
        <category>GNN</category>
      </categories>
      <tags>
        <tag>KG</tag>
      </tags>
  </entry>
  <entry>
    <title>RDGCN</title>
    <url>/kge/RDGCN/</url>
    <content><![CDATA[<h1 id="relation-aware-entity-alignment-for-heterogeneous-knowledge-graphs">Relation-Aware Entity Alignment for Heterogeneous Knowledge Graphs</h1>
<p>IJCAI 2019</p>
<p><a href="https://github.com/StephanieWyt/RDGCN"><strong>RDGCN</strong></a> (Relation-aware Dual-Graph Convolutional Network)，预测任务是KG的实体对齐，主要是为了捕获更多的在dual KG中的relation的信息。核心创新点是对于dual KG（即要对齐的两个KG），构造了Dual Relation Graph，捕获relation和relation之间的联系。之后在这个Dual Relation Graph上学习relation的表示，融入到original KG中进行entity的表示学习，最终用于entity之间的对齐。</p>
<span id="more"></span>
<h2 id="method">Method</h2>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210827161832004.png" alt="image-20210827161832004" style="zoom:50%;" /></p>
<h3 id="constructing-the-dual-relation-graph">Constructing the Dual Relation Graph</h3>
<p>有两个KG，<span class="math inline">\(G_1\)</span>和<span class="math inline">\(G_2\)</span>，然后这两个图看做是一个大的graph，<span class="math inline">\(G_e\)</span>。注意，两个KG没有相连。</p>
<p>构造relation graph，relation作为node，如果两个relation具有相同的头/尾实体，那么两个relation node构造一条边。</p>
<p>更进一步，为这个边赋值一个权重，表示两个关系相连的紧密程度。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210827162305649.png" alt="image-20210827162305649" style="zoom:50%;" /></p>
<p><span class="math inline">\(H\)</span>和<span class="math inline">\(T\)</span>是所有的头/尾实体。</p>
<h3 id="dual-attention-layer">Dual Attention Layer</h3>
<p>这一层是用来捕获更复杂的关系信息，从而辅助下面的实体表示的学习。</p>
<p>在dual realtion graph中，一个关系node的表示为：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210827193821221.png" alt="image-20210827193821221" style="zoom:50%;" /></p>
<p>需要注意，这里没有给关系赋予一个独立的表示，而是直接使用头尾实体的平均表示。</p>
<p>之后，基于gat进行relation之间的聚合。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210827193932914.png" alt="image-20210827193932914" style="zoom:50%;" /></p>
<h3 id="primal-attention-layer">Primal Attention Layer</h3>
<p>利用上面学习到的relation的表示，在original graph中进行实体的表示学习。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210827194018172.png" alt="image-20210827194018172" style="zoom:50%;" /></p>
<p>随后，使用残差</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210827194112479.png" alt="image-20210827194112479" style="zoom:50%;" /></p>
<h3 id="incorporating-structural-information">Incorporating Structural Information</h3>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210827194151191.png" alt="image-20210827194151191" style="zoom:50%;" /></p>
<p>使用highway gnn更新，保留上一步的信息</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210827194230407.png" alt="image-20210827194230407" style="zoom:50%;" /></p>
<p>最后，进行实体对齐</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210827194312252.png" alt="image-20210827194312252" style="zoom:50%;" /></p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>GNN</category>
      </categories>
      <tags>
        <tag>GNN</tag>
        <tag>KGE</tag>
      </tags>
  </entry>
  <entry>
    <title>RESCAL</title>
    <url>/kge/RESCAL/</url>
    <content><![CDATA[<h1 id="a-three-way-model-for-collective-learning-on-multi-relational-data">A Three-Way Model for Collective Learning on Multi-Relational Data</h1>
<p>2011</p>
<blockquote>
<p>we propose the relational learning approach RESCAL which is based on a tensor factorization that is related to DEDICOM but does not exhibit the same constraints.</p>
</blockquote>
<span id="more"></span>
<h2 id="introduction">1 Introduction</h2>
<p>说明最近tensor decomposition方法逐渐被应用在relational learning，原因：</p>
<ul>
<li>从建模的角度讲，tensor decomposition更直接，各种relation可以直接表示为high-order tensor。同时，没有先验知识需要</li>
<li>从learning的角度讲，关系型的数据通常是高维并且稀疏的，适用于tensor decomposition。</li>
</ul>
<p>关系型数据的重要特征是相关性能够通过各种相连的node产生，但是目前的模型都无法很好的满足要求。</p>
<blockquote>
<p>we propose the relational learning approach RESCAL which is based on a tensor factorization that is related to DEDICOM but does not exhibit the same constraints.</p>
</blockquote>
<h2 id="modelling-and-notation">2 Modelling and Notation</h2>
<p>看一下对于relational data如何表示：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20200223224511662.png" style="zoom:50%;" /></p>
<p>整体的数据被表示为张量<span class="math inline">\(\cal{X}\)</span>，<span class="math inline">\(\cal{X}_{ijk}=1\)</span>，表示fact存在。</p>
<h2 id="methods-and-theoretical-aspects">4 Methods and Theoretical Aspects</h2>
<p>文中定义了collective learning，大概含义是通过利用相关node的信息perform task。</p>
<blockquote>
<p>We will refer to the mechanism of exploiting the information provided by related entities regardless of the particular learning task at hand as collective learning.</p>
</blockquote>
<h3 id="a-model-for-multi-relational-data">4.1 A Model for Multi-Relational Data</h3>
<p>核心在于： <span class="math display">\[
{\cal{X}_k} \approx AR_kA^T,\ k = 1,2,\dots,m,\\
A \in R^{n\times r}, R_k\in R^{r\times r}
\]</span> 对于该式子的理解是，<span class="math inline">\(R_k\)</span>表示关系<span class="math inline">\(r\)</span>的转换，<span class="math inline">\(R_kA^T\)</span>将<span class="math inline">\(A\)</span>转换到了<span class="math inline">\(R_k\)</span>表示的向量空间当中，通过与<span class="math inline">\(A\)</span>乘积，最终得到的对于<span class="math inline">\((h,r,t)\)</span>，通过点积表示fact。</p>
<p>通过最小化得到最终的embedding： <span class="math display">\[
min\ f(A,R_k)+g(A,R_k) \\
f(A,R_k)=\frac{1}{2}(\sum_k {\lVert {\cal{X}}-AR_kA^T \rVert}_F^2) \\
g(A,R_k)=\frac{1}{2}\lambda({\lVert A \rVert}^2_F + \sum_k{\lVert R_k \rVert}^2_F)
\]</span></p>
<h2 id="evaluation">5 Evaluation</h2>
<p>进行了四方面的比较，</p>
<ul>
<li>Collective Classiﬁcation</li>
<li>Collective Entity Resolution</li>
<li>Kinships, Nations and UMLS</li>
<li>Runtime Performance and Technical Considerations</li>
</ul>
]]></content>
      <categories>
        <category>Paper</category>
        <category>KGE</category>
      </categories>
  </entry>
  <entry>
    <title>RGHAT</title>
    <url>/kge/RGHAT/</url>
    <content><![CDATA[<h1 id="relational-graph-neural-network-with-hierarchical-attention-for-knowledge-graph-completion">Relational Graph Neural Network with Hierarchical Attention for Knowledge Graph Completion</h1>
<p>AAAI 2020</p>
<p>使用了两层注意力，相同关系下的实体的注意力+不同关系的注意力</p>
<span id="more"></span>
<p><strong>无开源代码</strong></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20200906193934782.png" style="zoom:50%;" /></p>
<p>使用了两层的attention，</p>
<p>Relation-level attention <span class="math display">\[
\mathcal{a}_{h,r}=W_1[h||v_r] \\
\alpha_{h,r}=softmax_r(\alpha_{h,r})=\frac{exp(\sigma(p\cdot a_{h,r}))}{\sum_{r^\prime\in N_h } exp(\sigma(p\cdot a_{h,r^\prime}))}
\]</span> Entity-level attention <span class="math display">\[
b_{h,r,t}=W_2[a_{h,r}||t] \\
\beta_{r,t}=softmax_t(b_{h,r,t})
\]</span> 最后计算triple-level attention <span class="math display">\[
\mu_{h,r,t}=\alpha_{h,r}\cdot \beta_{r,t}
\]</span> 邻居信息的聚合 <span class="math display">\[
\hat{h} = \sum_{r\in \cal{N}_{h}} \sum_{t\in \cal{N}_{h,r}} \mu_{h,r,t} b_{h,r,t}
\]</span> 与自身信息的聚合 <span class="math display">\[
h^\prime = \frac{1}{2} ( \sigma(W_3(h+\hat{h})) + \sigma(W_3(h \odot \hat{h})))
\]</span> 以上就是encoder，decoder是ConvE。</p>
<p>在实践中，</p>
<blockquote>
<ul>
<li><p>In the training stage, we adopt a two-layer RGHAT</p></li>
<li>For the encoder, the embedding size of entities is set as 100 for both the input and output layer.</li>
<li><p>The number of heads for the multi-head attention mechanism is set as 8.</p></li>
<li>A dropout with the rate as 0.5 is applied to each input layer of the encoder and the normalized attention coefﬁcients following graph attention network.</li>
<li><p>L2 regularization with λ = 0.0005</p></li>
</ul>
</blockquote>
<p>实验效果看起来很漂亮，但是无法复现就无法确定代码是否有正误，特别是在KBGAT存在test data leakage的情况下。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20200906194013652.png" style="zoom:50%;" /></p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>GNN</category>
      </categories>
  </entry>
  <entry>
    <title>Re-eval-KGC</title>
    <url>/kge/Re-eval-KGC/</url>
    <content><![CDATA[<h1 id="a-re-evaluation-of-knowledge-graph-completion-methods">A Re-evaluation of Knowledge Graph Completion Methods</h1>
<p>2019-11-10 ACL 2020</p>
<p>重新发现目前的KGC方法中存在的问题，提出了一个RANDOM的评估策略。</p>
<span id="more"></span>
<h2 id="introduction">1 Introduction</h2>
<p>最近出现的nn的model for Knowledge Graph Completion(KGC)，的效果存在问题：</p>
<blockquote>
<p>in ConvKB, there is a 21.8% improvement over ConvE on FB15k-237, but a degradation of 42.3% on WN18RR, which is surprising given the method is claimed to be better than ConvE.</p>
</blockquote>
<p>它们在一个数据集(FB15K-237)上取得很好的结果，但是在另外的数据集(WRR18)上的效果反而下降了。本论文就针对这个问题进行了调查。发现是由于它们的评估策略的问题。</p>
<h2 id="observations">3 Observations</h2>
<p>经过调查发现，在最后进行评估的时候，部分受到影响的模型如ConvKB，KBAT等，它们会对于很多的negative sample产生和valid triple一样的score。</p>
<blockquote>
<p>On average, ConvKB and CapsE have 125 and 278 entities with exactly same score as the valid triplet over the entire evaluation dataset of FB15k-237, whereas ConvE has around 0.002,</p>
</blockquote>
<p>在这样的情况下，如果一开始的valid triple是作为评估triple的开头的话，效果就会虚假的高。</p>
<h2 id="evaluation-method">4 Evaluation Method</h2>
<p>因此，论文就提出了一个评估的策略：<em>RANDOM</em></p>
<blockquote>
<p>RANDOM:</p>
<p>In this, the correct triplet is placed randomly in <span class="math inline">\(\cal{T^{&#39;}}\)</span> .</p>
</blockquote>
<p>其中， <span class="math display">\[
\cal{T^{&#39;}} = \{ (h, r, t^{&#39;})\ |\ t^{&#39;} \in \cal{E} \}
\]</span></p>
<blockquote>
<p>RANDOM is the best evaluation technique which is both rigorous and fair to the model.</p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title>RelGNN</title>
    <url>/kge/RelGNN/</url>
    <content><![CDATA[<h1 id="relation-aware-graph-attention-model-with-adaptive-self-adversarial-training">Relation-aware Graph Attention Model With Adaptive Self-adversarial Training</h1>
<p>AAAI 2021</p>
<p>作者提出了RelGNN方法处理带attribute的heterogeneous graph，核心部分包括一个采取了注意力机制的R-GCN方法以及能够降低负采样false negative sample的自对抗负采样方法adaptive self-adversarial (ASA) negative sampling。</p>
<p>个人认为最大的创新点是这种负采样的方法ASA。负采样的核心思路是如何寻找最难区分discriminative的样本。而ASA方法的核心思想是计算正样本和负样本得分score之间的绝对差距，采样使这种差距最小的负样本。作者认为如果一个构造的负样本计算的得分比正样本的得分还要大，那么这样的负样本更有可能是false negative，因此不能直接选择这样最难区分的负样本，而是考虑正样本的预测值。</p>
<span id="more"></span>
<h2 id="introduction">Introduction</h2>
<p><strong>motivation</strong>：两个问题</p>
<ol type="1">
<li>作者认为目前处理heterogeneous graph的GNN在聚合邻居信息的时候没有考虑边的语义信息，知识在进行meta-path遍历或者消息构造函数时起到作用。</li>
<li>目前训练采用的负采样方法，无法考虑false negatives的问题。</li>
</ol>
<p><strong>method</strong>：提出RelGNN</p>
<ol type="1">
<li>在聚合消息时，使用注意力机制，同时考虑node state和edge state。</li>
<li>提出ASA采样方法，使用每一次训练好的模型为下一次训练寻找negative samples。思路是认为一个positive sample的confidence level应该和它衍生的negative sample的概率是匹配的。</li>
</ol>
<h2 id="method">Method</h2>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210702160109326.png" style="zoom:50%;" /></p>
<h3 id="attribute-embedding">attribute embedding</h3>
<p>在Attributed Heterogeneous Graph中，对于不同node type，有不同的attribute schema，有不同的attribute。使用不同的方法处理这些特征，然后拼接，投影至相同空间中，获得node <span class="math inline">\(v\)</span>的attribute embedding <span class="math inline">\(h_{v}^{(0)}\)</span>。</p>
<h3 id="message-passing">Message Passing</h3>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210702161233932.png" style="zoom:50%;" /></p>
<p>上面就是R-GCN。实际上，为了避免过度参数化，使用了R-GCN的<em>basis-decomposition</em>。</p>
<p>接下来，是使用了edge embedding的attention：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210702161403878.png" style="zoom:50%;" /></p>
<p>采用多头机制，同时要注意这个attention是不包括self-loop传递过来的信息的。</p>
<p>最后，为了融合attribute embedding <span class="math inline">\(h_{v}^{(0)}\)</span>以及graph embedding <span class="math inline">\(h_{v}^{last}\)</span>，使用attention来融合</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210702161953963.png" style="zoom:50%;" /></p>
<p>使用了<em>averaging</em>的multi-attention。</p>
<h3 id="asa-negative-sampling">ASA negative sampling</h3>
<p>以前的自对抗采样方法self-adversarial negative sampling，寻找最难预测的负样本。</p>
<blockquote>
<p>The core idea is to use the model itself to evaluate the hardness of negative samples,</p>
</blockquote>
<p>RelGNN预测三元组存在的概率，使用了DistMult来打分：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210702163057832.png" style="zoom:50%;" /></p>
<p>之前的自对抗负采样方法：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210702163147312.png" style="zoom:50%;" /></p>
<p>改进后的负采样方法：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210702163241877.png" style="zoom:50%;" /></p>
<p>加入<span class="math inline">\(u\)</span>之后，实际上是减小了小于正样本得分的负样本和正样本之间的差距，扩大了大于正样本得分的负样本和正样本之间的差距，让模型倾向于选择小于正样本得分的负样本。随着模型训练，模型越来越“正确”，可以考虑减小<span class="math inline">\(u\)</span>的值，让模型去选择更难预测的负样本。</p>
<p><span class="math inline">\(u\)</span>如果太小，会让模型倾向选择更hard的负样本，增大false negative的概率。</p>
<p><span class="math inline">\(u\)</span>如果太大，会倾向于选择那些trivial samples，不够discriminate。</p>
<h2 id="experiment">Experiment</h2>
<p>主要的结果忽略，可以学习的是它对于attention的可视化，计算每个node的领奖attention的熵entropy，计算不同节点的注意力的熵，熵约低，表示这个节点的邻居注意力差异越小，约不混沌，值约集中，越关注某些特定的邻居。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210702163855948.png" style="zoom:50%;" /></p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>GNN</category>
      </categories>
      <tags>
        <tag>GNN</tag>
        <tag>KRL</tag>
      </tags>
  </entry>
  <entry>
    <title>SACN</title>
    <url>/kge/SACN/</url>
    <content><![CDATA[<h1 id="end-to-end-structure-aware-convolutional-networks-for-knowledge-base-completion">End-to-End Structure-Aware Convolutional Networks for Knowledge Base Completion</h1>
<p>AAAI 2019</p>
<p>W-GCN+Conv-TransE</p>
<p>W-GCN把不同的关系类型看做是不同的sub graph，不同的sub graph包括不同的weight。请注意这里的weight不同于一般的self-attention，W-GCN的weight是只与relation type有关的，而且不包括self-loop。</p>
<span id="more"></span>
<p><span class="math display">\[
h_{i+1}=\sigma{\sum_{j\in N_{i}}\alpha_t^l h_j^l W^l + h_i^lW^l}
\]</span> Conv-TransE是在ConvE的基础上，取消了feature reshaing。直接将head entity embedding和relation embedding stack成<span class="math inline">\(2\times d\)</span>的矩阵，之后使用<span class="math inline">\(2\times k\)</span>的卷积核进行卷积操作，这样仍然能够保持TransE中的transformation property。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20200906104625946.png" style="zoom:50%;" /></p>
<p>需要注意的是，它构建了一个新的数据集，FB15k-237-Attr，从FB24k中导出属性，增加到FB15k-237数据集中。</p>
<blockquote>
<p>We extract the attribute triples of entities in FB15k-237 from FB24k. During the mapping, there are 7,589 nodes from the original 14,541 entities which have the node attributes. Finally, we extract 78,334 attribute triples from FB24k. These triples include 203 attributes and 247 relations. Based on these triples, we create the “FB15k-237-Attr” dataset, which includes 14,541 entity nodes, 203 attribute nodes, 484 relation types. All the 78,334 attribute triples are combined with the training set of FB15k-237.</p>
</blockquote>
<p>对于W-GCN如何处理attribute没有看懂。</p>
<p>实验结果</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20200906104552852.png" style="zoom:50%;" /></p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>GNN</category>
      </categories>
  </entry>
  <entry>
    <title>SEEK</title>
    <url>/kge/SEEK/</url>
    <content><![CDATA[<h1 id="seek-segmented-embedding-of-knowledge-graphs">SEEK: Segmented Embedding of Knowledge Graphs</h1>
<p>ACL 2020</p>
<p>虽然目前对于KGE的研究已经很多了，但是目前的方法都存在一个问题，简单的方法模型的表达能力不够；复杂的方法参数多，复杂性高，难以应用与实际的大规模的知识图谱。</p>
<p>本文就考虑如何在不增加复杂度的情况下增加模型的表达能力：</p>
<ul>
<li>增加特征之间的交互</li>
<li>保存关系的属性——对称性与不对称性</li>
<li>设计有效的得分函数</li>
</ul>
<p>核心方法是将实体和关系的embedding拆分为k个segment。</p>
<span id="more"></span>
<p>模型方法：</p>
<p>首先将实体和关系的embedding拆分为k个segment。</p>
<p>直接看最后的得分函数</p>
<p><span class="math display">\[
f_4(h,r,t)= \sum_{0\leq x,y&lt; k} s_{x,y} \cdot \left \langle r_x, h_y, t_{w_{x,y}} \right \rangle \\
\]</span></p>
<p><span class="math display">\[
w_{x,y} = \begin{cases}
y,  &amp; \mbox{if }x\mbox{ is even}, \\
(x+y)\%k, &amp; \mbox{if }x\mbox{ is odd}
\end{cases} 
\]</span></p>
<p><span class="math display">\[
s_{x,y} =
\begin{cases}
-1,  &amp; \mbox{if }x\mbox{ is odd and } x+y\geq k, \\
1, &amp; otherwise
\end{cases} \\
\]</span> 分析上面的方法，引入<span class="math inline">\(s_{x,y}\)</span>可以建模关系的对称和不对称性，将<span class="math inline">\(h\)</span>和<span class="math inline">\(t\)</span>互换的情况下，<span class="math inline">\(f_4(h,r,t)\)</span>不一样。</p>
<p>引入<span class="math inline">\(w_{x,y}\)</span>限制了<span class="math inline">\(t_{w_{x,y}}\)</span>，不再是所有分段的全体组合。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20200929220716325.png" style="zoom:50%;" /></p>
<p>最终实验在FB15K，DB100K，YAGO37三个数据集下进行了实验。</p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>KGE</category>
      </categories>
  </entry>
  <entry>
    <title>T-GAP</title>
    <url>/kge/T-GAP/</url>
    <content><![CDATA[<h1 id="learning-to-walk-across-time-for-interpretable-temporal-knowledge-graph-completion">Learning to Walk across Time for Interpretable Temporal Knowledge Graph Completion</h1>
<p>T-GAP KDD 2021</p>
<p><a href="https://github.com/sharkmir1/T-GAP.">https://github.com/sharkmir1/T-GAP</a></p>
<blockquote>
<p>Static knowledge graphs (KGs), despite their wide usage in relational reasoning and downstream tasks, fall short of realistic modeling of knowledge and facts that are only temporarily valid. Compared to static knowledge graphs, temporal knowledge graphs (TKGs) inherently reflect the transient nature of real-world knowledge. Naturally, automatic TKG completion has drawn much research interests for a more realistic modeling of relational reasoning. However, <strong>most of the existing models for TKG completion extend static KG embeddings that do not fully exploit TKG structure, thus lacking in 1) accounting for temporally relevant events already residing in the local neighborhood of a query, and 2) path-based inference that facilitates multi-hop reasoning and better interpretability.</strong> In this paper, we propose T-GAP, a novel model for TKG completion that maximally utilizes both temporal information and graph structure in its encoder and decoder. T-GAP encodes query-specific substructure of TKG by focusing on the temporal displacement between each event and the query timestamp, and performs path-based inference by propagating attention through the graph. Our empirical experiments demonstrate that T-GAP not only achieves superior performance against state-of-the-art baselines, but also competently generalizes to queries with unseen timestamps. Through extensive qualitative analyses, we also show that T-GAP enjoys transparent interpretability, and follows human intuition in its reasoning process.</p>
</blockquote>
<span id="more"></span>
<h2 id="introduction">Introduction</h2>
<p>作者期望解决的问题：</p>
<p>目前对于TKG补全的方法大多是之前对于静态KG方法的拓展，而在静态KG上的邻居信息已经证明了是有效的，但是如何在TKG上利用邻居信息还没有充分探究。</p>
<p>作者的解决方案：</p>
<p>编码阶段：作者看重对于捕获的邻居边的timestamp和要查询的timestamp之间的时间位移进行探究</p>
<blockquote>
<p>we focus on encoding the temporal displacement between the timestamps of the input query and each edge being encoded.</p>
</blockquote>
<p>比如下面的例子，要查询COVID-19在12月20日感染的人，明显重要的信息是COVID-19在两天前感染了A，然后在一天前A和B相遇。重要的是相对时间关系和时间的跨度，而不是具体的时间点。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220719183241840.png" style="zoom:30%;" /></p>
<p>解码阶段：作者提出了基于注意力value的路径传播方法</p>
<h2 id="method">Method</h2>
<p>整体上使用了GNN作为编码器，attention flow作为解码器。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220721214613250.png" style="zoom:50%;" /></p>
<p>在编码器部分，就是在原始的graph上（论文中是称作preliminary graph）进行GNN操作。核心是通过不同相对时间关系的参数+时间位移的embedding来改造基于transformer-attention的GNN。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220721220641053.png" style="zoom:40%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220721220210975.png" style="zoom:40%;" /></p>
<p>其中的，<span class="math inline">\(h_i\)</span>表示头实体embedding，初始值为随机初始化；<span class="math inline">\(\rho_{ij}\)</span>表示的是关系向量；<span class="math inline">\(\tau_{|\triangle t_{ij}|}\)</span>表示相对时间位移大小的向量。随后基于多头注意力进行聚合：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220721221555351.png" style="zoom:40%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220721221627204.png" style="zoom:40%;" /></p>
<p>在编码器部分，是执行最多<span class="math inline">\(T\)</span>次的解码过程，首先是采样得到新的子图；其次是利用这样的子图进行和编码器相同过程但是不同参数的GNN操作；最后进行attention flow，便于下一步的采样子图。</p>
<p>子图采样是一个非参数化的过程，核心思想是采样当前采样得到的子图中，最大attention value的node，然后在这些比较重要的node出发，采样它们引出的比较重要的边，加入到子图中去。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220721222218056.png" style="zoom:45%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220721222236241.png" style="zoom:45%;" /></p>
<p>在采样得到的子图上，进行GNN操作，聚合邻居信息。主要过程和编码器部分一致。最后融合query相关的embedding：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220721222552555.png" style="zoom:40%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220721222618124.png" style="zoom:40%;" /></p>
<p>最后，进行attention flow，核心思想是给定中心节点attention value <span class="math inline">\(1\)</span>，然后让这个value通过GNN聚合得到的信息，在graph上不断传播，自动计算各个路径的重要程度。公式里的第一个score用来计算已经在当前采样得到的子图中的node的重要程度，第二个score会更加偏好采样得到还没在当前采样子图图中的node。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220721222945282.png" style="zoom:40%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220721223059254.png" style="zoom:40%;" /></p>
<p>总的采样过程实例如下：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220721223151983.png" style="zoom:40%;" /></p>
<p>通过上面的过程，T-GAP可以让注意力不断通过路径在graph上流动，最后得到attention value最大的node，就可以看做是要预测的目标。训练使用的loss：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220802181917428.png" style="zoom:40%;" /></p>
<h2 id="experiment">EXPERIMENT</h2>
<p>T-GAP的实验包括下述三方面：</p>
<ul>
<li>时序KG补全的性能</li>
<li>对于未见过的时间戳的泛化性</li>
<li>可解释性/与人类直观认识的关联</li>
</ul>
<p>实验的数据集包括：ICEWS14, ICEWS05-15和Wikidata11k。这三个数据集是较为通用的数据集，被之前的研究者建议使用（<em>Learning Sequence Encoders for Temporal Knowledge Graph Completion</em>）</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220805173558600.png" style="zoom:40%;" /></p>
<p>ICEWS14包括2014年发生的社会政治事件；ICEWS05-15包括A.D. 25到2020发生的事件；</p>
<p>Wikidata11k是Wikidata的子集，在其中所有的fact加入了时空标识符<em>occurSince</em>和<em>occurUntil</em>。随着<em>Learning Sequence Encoders for Temporal Knowledge Graph Completion</em>的做法，作者把原来的时空标识符和关系合并起来，作为新的relation，比如(A, loves, B, since, 2020)变为(A, loves-since, B, 2020)。</p>
<h3 id="benchmark-performance">Benchmark Performance</h3>
<p>总体性能：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220805174207341.png" style="zoom:40%;" /></p>
<p>相对提升了10%，很明显的提升</p>
<p>消融实验：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220805174251297.png"  style="zoom:40%;" /></p>
<h3 id="temporal-generalization">Temporal Generalization</h3>
<p>沿着前人的做法（<em>Diachronic embedding for temporal knowledge graph completion</em>），把ICEWS14训练集中每个月的第5、15和25天发生的fact拿出来作为验证集和测试集，来验证对于queries with unseen timestamps的泛化性能。</p>
<p>结果：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220805174504528.png" alt="image-20220805174504528" style="zoom:40%;" /></p>
<h3 id="interpretability">Interpretability</h3>
<p>T-GAP的可解释性从两方面进行，（1）不同relation和时序位移的联系（2）attention flow推理过程的case study。</p>
<p>（1）Relation Type and Temporal Displacement</p>
<p>作者通过不同relation下，T-GAP学习到的attention的分布来分析relation和时序位移之间的联系。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220805174749639.png" alt="image-20220805174749639" style="zoom:50%;" /></p>
<p>（2）Reasoning Process</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220805174822723.png" alt="image-20220805174822723" style="zoom:40%;" /></p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>TKG</category>
      </categories>
      <tags>
        <tag>GNN</tag>
        <tag>TKG</tag>
      </tags>
  </entry>
  <entry>
    <title>TACT</title>
    <url>/kge/TACT/</url>
    <content><![CDATA[<h1 id="topology-aware-correlations-between-relations-for-inductive-link-prediction-in-knowledge-graphs">Topology-Aware Correlations Between Relations for Inductive Link Prediction in Knowledge Graphs</h1>
<p>AAAI 2021</p>
<p><a href="https://github.com/MIRALab-USTC/KG-TACT">TACT</a>，作者主要考虑的是inductive link prediction，使用gnn，捕获relation之间的语义上的关联性，即semantic correlation。作者认为relation之间的关联性通过relation的拓扑结构得到体现，因此，作者将所有的relation之间相连的拓扑结构分为7种，在relation形成的graph中进行学习，提出了RCN。</p>
<span id="more"></span>
<blockquote>
<p>Inductive link prediction—where entities during training and inference stages can be different—has been shown to be promising for completing continuously evolving knowledge graphs. Existing models of inductive reasoning mainly focus on predicting missing links by learning logical rules. However, many existing approaches do not take into account semantic correlations between relations, which are commonly seen in real-world knowledge graphs. To address this challenge, we propose a novel inductive reasoning approach, namely TACT, which can effectively exploit Topology-Aware CorrelaTions between relations in an entity-independent manner. TACT is inspired by the observation that the semantic correlation between two relations is highly correlated to their topological structure in knowledge graphs. Speciﬁcally, we categorize all relation pairs into several topological patterns, and then propose a Relational Correlation Network (RCN) to learn the importance of the different patterns for inductive link prediction. Experiments demonstrate that TACT can effectively model semantic correlations between relations, and signiﬁcantly outperforms existing state-of-the-art methods on benchmark datasets for the inductive link prediction task.</p>
</blockquote>
<h2 id="introduction">1 Introduction</h2>
<p>作者考虑的是比较另类的link prediction，inductive link prediction。即在测试集中要预测的实体没有在训练集中出现。像是最常见的link prediction都是transductive link prediction，不能保证对新出现的实体也有比较好的预测效果。</p>
<p>为了进行inductive learning，就必须保证能够将训练集中训练好的信息能够迁移到测试集上。具体到link prediction上，就是说需要方法能够进行entity-independent的学习。因为relation应该是已有的，不是新出现的。</p>
<p>之前的inductive link prediction很多事基于rule的学习，因为学习到规则的话，这种规则是entity-independent的。</p>
<p>作者考虑在relation上做文章，主要考虑利用topology pattern捕获relation之间的semantic correlation。</p>
<h2 id="methods">2 Methods</h2>
<p>首先把KG上所有的relation的关联关系分为7种，作者在附录中证明了一共只有7种。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210510171104138.png" style="zoom:50%;" /></p>
<p>然后看一下整体结构：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210510170648225.png" style="zoom:50%;" /></p>
<p>两个模块，RCN+R-GCN</p>
<p>RCN是核心创新点，构造了一个只由relation组成的graph，relation之间的edge有6种（NC的这种不存在）。</p>
<p>RCN学习了<span class="math inline">\(\mathbf{r}_t\)</span>，R-GCN学习<span class="math inline">\(\mathbf{e}_u\)</span>，<span class="math inline">\(\mathbf{e}_v\)</span>，以及graph embedding<span class="math inline">\(\mathbf{e}_G\)</span>。</p>
<p>主要看下RCN，两步：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210510171135345.png" style="zoom:50%;" /></p>
<p>其中的<span class="math inline">\(N_t^P\in \mathbb{R}^{1\times |R|}\)</span>，取值为0/1，表示关系之间的相连性；<span class="math inline">\(\Lambda_t^P\in \mathbb{R}^{1\times |R|}\)</span>，是可学习的参数，表示relation之间的correlation coefﬁcients，并且保证 <span class="math inline">\(\sum_{i=1}^{|R|}\Lambda_t^P=1\)</span>。</p>
<p>然后得到relation <span class="math inline">\(t\)</span>最终表示：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210510170730416.png" style="zoom:50%;" /></p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>KGE</category>
      </categories>
      <tags>
        <tag>GNN</tag>
        <tag>KGE</tag>
      </tags>
  </entry>
  <entry>
    <title>Essence-of-linear-algebra-3b1b</title>
    <url>/math/Essence-of-linear-algebra-3b1b/</url>
    <content><![CDATA[<h1 id="essence-of-linear-algebra">Essence of linear algebra</h1>
<p>这篇文章是3blue1brown的Essence of linear algebra系列视频的笔记</p>
<ol type="1">
<li>Vectors</li>
<li>Linear combinations, span and basis vectors</li>
<li>Linear transformations and matrices</li>
<li>Matrix multiplication</li>
<li>The Determinant</li>
<li>Inverse Matrices, column space and null space</li>
<li>Nonsquare matrices as transformation between dimensions</li>
<li>Dot products and Duality</li>
<li>Cross products</li>
<li>Change of basis</li>
<li>Eignvectors and eigenvalues</li>
<li>Abstract vector spaces</li>
</ol>
<span id="more"></span>
<h2 id="vectors">1 Vectors</h2>
<p>如何认识向量？</p>
<p>从一个物理学生的角度来看，一个向量是长度一定，角度一定，在空间中可以任意的移动。</p>
<p>从一个计算机学生的角度来看，一个向量是一系列数字的list，这些数字可能具有独特的现实含义，比如房子的面积、单位售价等。</p>
<p>从一个数学学家的角度来看，向量可以代表任何事物，只要能够保证向量相加和向量数乘有意义即可。</p>
<p>如果尝试从几何的角度来看，可以看做坐标系下的箭头，起始点是原点，向量是坐标系下的不同数轴的坐标，这些坐标说明了如何从原点到达箭头的终点。</p>
<p>向量的加法，可以看做是先沿着向量<span class="math inline">\(x\)</span>运动，然后沿着向量<span class="math inline">\(y\)</span>运动。</p>
<p>向量的数乘，就是长度的缩放操作。</p>
<h2 id="linear-combinations-span-and-basis-vectors">2 Linear combinations, span and basis vectors</h2>
<p>向量可以看做是基向量的线性组合，不同坐标，表示缩放不同数轴上的基向量。</p>
<p>span的定义：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210929190419825.png" alt="image-20210929190419825" style="zoom:33%;" /></p>
<p>basis vector的定义：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210929190523369.png" alt="image-20210929190523369" style="zoom:33%;" /></p>
<h2 id="linear-transformations-and-matrices">3 Linear transformations and matrices</h2>
<p>Linear transformation：</p>
<ul>
<li>transformation就是一种函数，用在线代领域是为了强调对于向量的变换操作</li>
<li>前面的linear有两个限制：转换后的直线仍然是直线，并且原点保持固定</li>
</ul>
<p>原点固定，是因为任何矩阵与0向量相乘，都是0向量，原点始终固定。</p>
<p>线性变换的重要性质是，所有的变换的网格线（网格是想象中的一些向量的终点）是保持平行且等距分布的。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210926105607256.png" alt="image-20210926105607256" style="zoom:50%;" /></p>
<p>所有变换后的新向量，都可以通过基向量的变换进行相同的转换操作。</p>
<p>在一个二维转换中，我们可以把矩阵的列完全看做是变换后的基向量</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210926105059810.png" alt="image-20210926105059810" style="zoom:50%;" /></p>
<p>向量<span class="math inline">\([a,c]\)</span>是变换后的基向量<span class="math inline">\(\hat{i}\)</span>，<span class="math inline">\([b,d]\)</span>是变换后的基向量<span class="math inline">\(\hat{j}\)</span>。</p>
<p>因此，linear transformation可以看做是空间的一种变换，即基向量的变换。因此，我们可以直观上把矩阵看做是对空间的变换。</p>
<h2 id="matrix-multiplication">4 Matrix multiplication</h2>
<p>矩阵相乘可以看做是连续的空间变换，这也解释了为什么矩阵位置互换，结果不能保证一样。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210927101847889.png" alt="image-20210927101847889" style="zoom: 33%;" /></p>
<p>上面的矩阵<span class="math inline">\(M_2\)</span>把矩阵<span class="math inline">\(M_1\)</span>的列向量进行变换，矩阵<span class="math inline">\(M_1\)</span>的列向量可以看做是新的基向量<span class="math inline">\(\hat{i}\)</span>。</p>
<h2 id="the-determinant">5 The Determinant</h2>
<p>行列式determinate的几何含义，以二维平面为例，就是以单位basis向量组成的单位正方形的面积的变化。比如：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210928104234148.png" alt="image-20210928104234148" style="zoom: 33%;" /></p>
<p>特殊的情况是determine为0：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210928104342827.png" alt="image-20210928104342827" style="zoom:33%;" /></p>
<p>此时整个空间被压缩为一条直线，甚至是一个点。因此，如果我们计算某个matrix的行列式是否为0，我们就知道这个矩阵是否表示把空间压缩到更小的维度上。</p>
<p>此时还有另外的问题，那就是行列式是可以为负数的，此时，行列式代表矩阵会把空间翻转（fliping），行列式的绝对值仍然是面积的变化。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210928153905266.png" alt="image-20210928153905266" style="zoom:33%;" /></p>
<p>对于行列式的变换，假设<span class="math inline">\(\hat{i}\)</span>逐渐接近<span class="math inline">\(\hat{j}\)</span>，看下面一系列的动画：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210928154121351.png" alt="image-20210928154121351" style="zoom:33%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210928154143886.png" alt="image-20210928154143886" style="zoom:33%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210928154205327.png" alt="image-20210928154205327" style="zoom:33%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210928154221033.png" alt="image-20210928154221033" style="zoom:33%;" /></p>
<p>出现了负数的行列式。</p>
<p>如果在三维空间中，那么矩阵行列式就是单位体积的变化：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210928154422622.png" alt="image-20210928154422622" style="zoom: 33%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210928154512960.png" alt="image-20210928154512960" style="zoom:33%;" /></p>
<p>因为矩阵的列向量可以看做是新的基向量，如果行列式为0，就表示出现了列向量线性相关的情况，某个或者多个列向量可以由其它列向量表示。</p>
<p>行列式的计算过程，实际就是在计算这个面积/体积的变化，比如：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210928155247793.png" alt="image-20210928155247793" style="zoom:33%;" /></p>
<p>懂了行列式的几何意义，可以很轻松的理解下面的定理：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210928155521866.png" alt="image-20210928155521866" style="zoom:33%;" /></p>
<p>对空间进行<span class="math inline">\(M_1M_2\)</span>然后的变换后的面积变化=先进行<span class="math inline">\(M_2\)</span>变换，然后进行<span class="math inline">\(M_1\)</span>变换后的面积变化。本质是一样的。</p>
<h2 id="inverse-matrices-column-space-and-null-space">6 Inverse Matrices, column space and null space</h2>
<p>我们都知道，对于多元一次的方程组，求解未知变量，可以用矩阵的角度来看：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210928161330460.png" alt="image-20210928161330460" style="zoom:33%;" /></p>
<p>从矩阵是空间变换的角度来看，我们已知变换后的向量<span class="math inline">\(v\)</span>，只要逆着矩阵<span class="math inline">\(A\)</span>的变换，就能够找到空间变换前的向量<span class="math inline">\(x\)</span>。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210928161613839.png" alt="image-20210928161613839" style="zoom:33%;" /></p>
<p>这就是矩阵的逆矩阵<span class="math inline">\(A^{-1}\)</span>，逆矩阵乘以矩阵，表示的是什么都不做的变换，即一个单位矩阵。</p>
<p>只要行列式不为0，就存在对应的逆矩阵。</p>
<p>如果行列式为0，表示矩阵<span class="math inline">\(A\)</span>将空间的维度进行了压缩，我们此时无法还原原来没有压缩的空间，它对应的解有无数种。</p>
<p>如果矩阵表示的变化，最后把空间压缩为直线，就叫做此时矩阵的秩是1。如果压缩为二维平面，矩阵的秩就是2。</p>
<p>rank表示变化空间后的空间维数，因此，一个矩阵最大的rank就是它本身的维数。</p>
<p>矩阵的列空间：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210928163229116.png" alt="image-20210928163229116" style="zoom:33%;" /></p>
<p>矩阵的列空间也就是变换后的空间，rank就是指列空间的维数。</p>
<p>矩阵的零空间null space：</p>
<p>矩阵进行空间变换时，所有变换后落在原点的向量集合，组成了null space。</p>
<p>在线性方程组上，就是变换后的向量是0向量：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210928163737132.png" alt="image-20210928163737132" style="zoom:33%;" /></p>
<h2 id="nonsquare-matrices-as-transformation-between-dimensions">7 Nonsquare matrices as transformation between dimensions</h2>
<p>前面讨论的都是方阵，如果是一个非方阵，那么表示的是维度的变化，比如从二维变换为三维，三维变化为二维。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210928170025088.png" alt="image-20210928170025088" style="zoom:33%;" /></p>
<p>上面的矩阵就是从三维，变化到二维，列表示的基向量，从三维被表示为二维的向量，但是由于原来是三维空间，所有仍然有三个基向量。</p>
<h2 id="dot-products-and-duality">8 Dot products and Duality</h2>
<p>向量的点积就是所有维度的元素相乘然后相加。</p>
<p>如果希望从几何角度来理解，可以把左边的向量<span class="math inline">\(u\)</span>转置，变为矩阵的形式：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210928190814830.png" alt="image-20210928190814830" style="zoom:33%;" /></p>
<p>此时，从几何角度来看1维矩阵的变换，因为对称性，新的基向量<span class="math inline">\(\hat{i}\)</span>刚好是<span class="math inline">\(u_x\)</span>，新的基向量<span class="math inline">\(\hat{j}\)</span>刚好是<span class="math inline">\(u_y\)</span>。使用新的变换矩阵作用在向量<span class="math inline">\([x, y]^T\)</span>上，就是对其进行和对投影的变化操作。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210928190944152.png" alt="image-20210928190944152" style="zoom:33%;" /></p>
<h2 id="cross-products">9 Cross products</h2>
<p>差积的定义：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210928193801179.png" alt="image-20210928193801179" style="zoom:33%;" /></p>
<p>一个新的向量，长度是<span class="math inline">\(v\)</span>和<span class="math inline">\(w\)</span>组成的平行四边形的面积，方向垂直于该平行四边形。</p>
<p>该平行四边形面积的计算，可以使用<span class="math inline">\(v\)</span>和<span class="math inline">\(w\)</span>组成矩阵，然后求该矩阵的行列式。</p>
<h2 id="change-of-basis">10 Change of basis</h2>
<p>当我们定义了不同的基向量来描述空间中的同一个向量时，即使是同一个向量，也会使用不同的坐标来描述。</p>
<p>一个矩阵的列向量可以看做是新的基向量，它描述的是另一个坐标系的基向量，在我们想象当中的坐标系中的表示，</p>
<p>比如下面的形式：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210929100441396.png" alt="image-20210929100441396" style="zoom:33%;" /></p>
<p><span class="math inline">\([2,1]\)</span>和<span class="math inline">\([-1, 1]\)</span>是另外的坐标系的基向量，但是如果在另外的坐标系中，它实际表示的是<span class="math inline">\([1,0]\)</span>和<span class="math inline">\([0,1]\)</span>。我们使用自己的语言/坐标系来描述另一个坐标系的基向量。坐标<span class="math inline">\([-1,2]\)</span>是在另一个坐标系下的坐标。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210929102903240.png" alt="image-20210929102903240" style="zoom:33%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210929103010269.png" alt="image-20210929103010269" style="zoom:33%;" /></p>
<p>如果我们是希望自己坐标系下的<span class="math inline">\([-1,2]\)</span>，被转换到另一个坐标系中，那么我们可以用逆矩阵进行转化。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210929103031185.png" alt="image-20210929103031185" style="zoom:33%;" /></p>
<p>接下来，如果我们在自己的坐标系下，使用矩阵，进行了一个空间变化/基向量的线性转换，那么在另一个坐标系下，进行相同的转化的矩阵应该是什么？</p>
<p>实际上，我们只需要首先，把另一个坐标系下的基向量变换到自己的坐标系下，然后进行要求的空间变换，最后通过逆矩阵再变换到另一个坐标系下。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210929103848552.png" alt="image-20210929103848552" style="zoom:33%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210929103911585.png" alt="image-20210929103911585" style="zoom:33%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210929103944245.png" alt="image-20210929103944245" style="zoom:33%;" /></p>
<p>因此，如果我们看到下面的形式，我们可以直接从中间矩阵<span class="math inline">\(M\)</span>来看发生了什么变化，左右两侧的矩阵表示空间基向量坐标的转化：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210929104136552.png" alt="image-20210929104136552" style="zoom:33%;" /></p>
<h2 id="eignvectors-and-eigenvalues">11 Eignvectors and eigenvalues</h2>
<p>如何从几何角度理解特征值和特征向量？</p>
<p>还是假设在一个空间坐标中，进行了空间变换。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210929104925700.png" alt="image-20210929104925700" style="zoom:33%;" /></p>
<p>在进行这个空间变换中，大多数的向量都会发生一个角度的偏移，即和原来的向量不在一条直线上。但是有一些向量是和原来的向量还在一条直线上，对于这些向量来说，空间的变换仅仅是发生了长度的缩放，比如说对于所有在x轴上的向量来说，仅仅是长度增长三倍。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210929110503345.png" alt="image-20210929110503345" style="zoom:33%;" /></p>
<p>还存在其它的向量，也是类似的只会进行长度的缩放</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210929110547707.png" alt="image-20210929110547707" style="zoom:33%;" /></p>
<p>这些向量就叫做特征向量，需要进行的长度缩放就叫做特征值。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210929110633803.png" alt="image-20210929110633803" style="zoom:33%;" /></p>
<p>求解特征向量与特征值的一般过程：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210929112852029.png" alt="image-20210929112852029" style="zoom:33%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210929112826908.png" alt="image-20210929112826908" style="zoom:33%;" /></p>
<p>根据前面的矩阵是空间变换可知，如果希望把一个向量通过矩阵空间变化，压缩为0向量，那么只有可能是行列式为0。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210929113141083.png" alt="image-20210929113141083" style="zoom:33%;" /></p>
<p>当然，这样的性质表明了不是所有的矩阵都会有特征值，有的矩阵会把所有的向量都进行旋转操作，也就不存在特征向量了。</p>
<p>对应特征值的特征向量不一定就在一条直线上，最简单的，考虑一个矩阵是将所有向量都扩放到2倍。那么对于特征值2的特征向量就是整个空间下的所有向量。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210929121210291.png" alt="image-20210929121210291" style="zoom:33%;" /></p>
<p>接下来，看一下由特征向量为基，组成的特征空间的作用。</p>
<p>对于一个矩阵，如果它的特征向量有多个，可以组成一个全空间，那么以这些特征向量作为新的基向量，如果我们将这个新的特征基组成的basis change矩阵作用在原始矩阵上：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210929163607041.png" alt="image-20210929163607041" style="zoom:33%;" /></p>
<p>我们知道，这样得到的新矩阵，和之前的矩阵对于空间中的向量来说是同一种变换，但是是从新的特征空间下看的。这样得到的新的变换矩阵 ，在特征空间下，一定是对角矩阵，对角值是特征值。</p>
<p>这是因为，整个原始矩阵的空间变换，对于新的特征空间下的作为基向量的特征向量来说，仅仅是起到了缩放的作用，所以新的特征空间下的矩阵变换，就是对角矩阵，只有长度进行了缩放。</p>
<h2 id="abstract-vector-spaces">12 Abstract vector spaces</h2>
<p>线性转换的概念不仅局限在向量上，对于函数同样存在这样的定义：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210929183502322.png" alt="image-20210929183502322" style="zoom:33%;" /></p>
<p>求导运算，实际就是一种线性运算。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210929183650226.png" alt="image-20210929183650226" style="zoom:33%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210929183711783.png" alt="image-20210929183711783" style="zoom:33%;" /></p>
<p>实际上，向量的很多概念是可以应用到函数上的。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210929184851866.png" alt="image-20210929184851866" style="zoom:33%;" /></p>
<p>向量的类似概念是可以推广到进行了任意定义的对象的，只要定义的数乘运算和相加运算，能够满足下面的checklist，就可以认为此时定义的新的运算，可以组成一个向量空间，可以使用向量的各种相关概念去思考，定义。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210929185244924.png" alt="image-20210929185244924" style="zoom:33%;" /></p>
]]></content>
      <categories>
        <category>Class</category>
      </categories>
      <tags>
        <tag>Math</tag>
      </tags>
  </entry>
  <entry>
    <title>EEGA</title>
    <url>/mmml/EEGA/</url>
    <content><![CDATA[<h1 id="joint-multimodal-entity-relation-extraction-based-on-edge-enhanced-graph-alignment-network-and-word-pair-relation-tagging">Joint Multimodal Entity-Relation Extraction Based on Edge-enhanced Graph Alignment Network and Word-pair Relation Tagging</h1>
<p>EEGA，<a href="https://github.com/YuanLi95/EEGA-for-JMERE" class="uri">https://github.com/YuanLi95/EEGA-for-JMERE</a>，AAAI 2023，联合MNER和MRE。</p>
<p>首个提出将MNER和MRE联合训练的方法，作者将text和image表示为两个graph，然后除了进行visual object和textual entity的对齐，还进行了object-object relation和entity-entity relation的对齐。</p>
<blockquote>
<p>Multimodal named entity recognition (MNER) and multimodal relation extraction (MRE) are two fundamental subtasks in the multimodal knowledge graph construction task. However, the existing methods usually handle two tasks independently, which ignores the bidirectional interaction between them. This paper is the ﬁrst to propose jointly performing MNER and MRE as a joint multimodal entity-relation extraction task (JMERE). Besides, the current MNER and MRE models only consider aligning the visual objects with textual entities in visual and textual graphs but ignore the entity-entity relationships and object-object relationships. To address the above challenges, we propose an edge-enhanced graph alignment network and a word-pair relation tagging (EEGA) for JMERE task. Speciﬁcally, we ﬁrst design a word-pair relation tagging to exploit the bidirectional interaction between MNER and MRE and avoid the error propagation. Then, we propose an edge-enhanced graph alignment network to enhance the JMERE task by aligning nodes and edges in the cross-graph. Compared with previous methods, the proposed method can leverage the edge information to auxiliary alignment between objects and entities and ﬁnd the correlations between entity-entity relationships and object-object relationships. Experiments are conducted to show the effectiveness of our model.</p>
</blockquote>
<span id="more"></span>
<h2 id="introduction">1. Introduction</h2>
<p>作者首次提出了多模态实体-关系联合抽取任务JMERE（joint multimodal entity-relation extraction），NER任务和RE任务进行交互能够相互辅助提升预测效果。</p>
<figure>
<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221212144428938.png" alt="image-20221212144428938" /><figcaption>image-20221212144428938</figcaption>
</figure>
<p>作者提出，在进行多模态信息抽取的时候，除了会类似于之前的论文要考虑object-entity的对齐，还应该考虑object-object relation和entity-entity relation的对齐。比如在上面的例子中，如果我们能够识别出image中的多个人object，那么可以辅助预测Thompson，Curry和Green可能是人；另外如果还能够知道image中的man_0和trophy的关系是holding，如果可以把holding对应到要预测的实体Thompson和O’Brien Trophy之间的关系可能是awarded。</p>
<p>如果把文本和实体都对应到两个graph上，就是除了要考虑node和node的对齐，还要考虑edge到edge的对齐：</p>
<figure>
<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221212144653728.png" alt="image-20221212144653728" /><figcaption>image-20221212144653728</figcaption>
</figure>
<p>另外，考虑到如果直接拼接MNER和MRE方法形成一个pipeline的话，可能会出现error propagation的情况，也就是MNER的错误输出会导致MRE的进一步错误预测（<em>Joint multi-modal aspect-sentiment analysis with auxiliary cross-modal relation detection. EMNLP 2021</em>），作者提出了一个word-pair relation tagging的方法实现同时实现NER和RE（目前不清楚是不是有很多联合抽取模型都是使用了相似的方法）：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221212145619808.png"  style="zoom:40%;" /></p>
<h2 id="method">2. Method</h2>
<figure>
<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221212145718727.png" alt="image-20221212145718727" /><figcaption>image-20221212145718727</figcaption>
</figure>
<h3 id="graph-encoder">2.1 Graph Encoder</h3>
<h4 id="textual-graph">2.1.1 Textual Graph</h4>
<p>使用<a href="https://spacy.io/models">语法依赖解析工具</a>将text解析为语法依赖树，形成一个textual graph。每个node使用BERT学习到的embedding作为初始表征；每个edge也有自己的可训练embedding。</p>
<h4 id="visual-graph">2.1.2 Visual Graph</h4>
<p>使用Mask-RCNN作为视觉特征导出器，然后构造场景图scene graph（<em>Unbiased scene graph generation from biased training. CVPR 2020</em>），只保留top-k的objects。每个node使用Mask-RCNN导出的视觉embedding作为初始表征；每个edge也有自己的可训练embedding。</p>
<h4 id="attribute-transformer">2.1.3 Attribute Transformer</h4>
<p>作者进一步提出使用Transformer把edge information融合到token/object表征上，使用edge embedding作为key和value：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221212151807388.png"  style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221212151817488.png"   style="zoom:50%;" /></p>
<p>其中的<span class="math inline">\(A_T^i\in R^{1\times n}\)</span>表示的是第<span class="math inline">\(i\)</span>-th token的邻接矩阵，<span class="math inline">\(Z_T^i\in R^{n\times d_{zT}}\)</span>表示的是对应的edge embedding。之后再经过FFN和layer normalization。最后得到的token embedding matrix记为<span class="math inline">\(H_T\)</span>。</p>
<p>在视觉侧，也有相同结构，不同参数的attribute Transformer。最后得到的object embedding matrix记为<span class="math inline">\(H_I\)</span>。</p>
<h3 id="edge-enhanced-graph-alignment-module">2.2 Edge-enhanced Graph Alignment Module</h3>
<p>接下来，希望对两个graph的node和edge进行对齐。</p>
<h4 id="edge-enhanced-graph-optimal-transport">2.2.1 Edge-enhanced Graph Optimal Transport</h4>
<p>作者借鉴了在迁移学习中出现的optimal transport method进行对齐。使用了两种距离度量方法：</p>
<ul>
<li>Wasserstein Distance (WD) (Peyr´e, Cuturi et al. 2019) for node matching (the red lines)</li>
<li>Gromov-Wasserstein Distance (GWD) (Peyr´e, Cuturi, and Solomon 2016) for edge matching(the blue and green lines)</li>
</ul>
<p>WD：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221212152751617.png"   style="zoom:50%;" /></p>
<p>其中<span class="math inline">\(T_{ij}\)</span>表示从<span class="math inline">\(i\rightarrow j\)</span>所需的代价，而<span class="math inline">\(c()\)</span>是cosine距离函数：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221212152835533.png"   style="zoom:50%;" /></p>
<p>GWD：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221212152806025.png"  style="zoom:50%;" /></p>
<p>其中，<span class="math inline">\(H_I^\prime,H_T^\prime\)</span>表示邻接节点集合。<span class="math inline">\(L()\)</span>函数用来度量两个graph的edge之间的距离： <span class="math display">\[
L(H_I^i,H_I^{i\prime},H_T^i,H_T^{i\prime})=||c(H_I^i,H_I^{i\prime})-c(H_T^i,H_T^{i\prime})||
\]</span> 之后，使用下面的loss函数优化对齐的效果：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221212154312654.png"  style="zoom:50%;" /></p>
<h4 id="image2text-attention">2.2.2 Image2text attention</h4>
<p>使用Transformer，将对齐后的视觉信息融合到文本表征中：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221212154422600.png"   style="zoom:50%;" /></p>
<h3 id="multi-channel-layer">2.3 Multi-channel Layer</h3>
<p>作者还额外的使用了三种文本的特征来辅助word-pair <span class="math inline">\((w_i,w_j)\)</span>的关系预测：</p>
<ul>
<li><p>Part of Speech (Pos)：使用spaCy导出Pos特征，参考下图，把word-pair的词性向量相加作为Pos特征；</p></li>
<li><p>Syntactic Distance (Sd)：使用word-pair之间的相对语法距离，参考下图：</p>
<figure>
<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221212155021596.png" alt="image-20221212155021596" /><figcaption>image-20221212155021596</figcaption>
</figure></li>
<li><p>Word Co-occurrences matrix (Co)：使用PMI（Point-wise Mutual Information）衡量两个word在整个语料中的correlation；</p></li>
</ul>
<figure>
<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221212154606639.png" alt="image-20221212154606639" /><figcaption>image-20221212154606639</figcaption>
</figure>
<p>这三个矩阵被用来进一步学习文本的表征，对于每个矩阵，对于<span class="math inline">\(i-th\)</span> word使用W-GCN聚合来自其它文本的信息：</p>
<figure>
<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221212155801935.png" alt="image-20221212155801935" /><figcaption>image-20221212155801935</figcaption>
</figure>
<p>三个矩阵的结果进行拼接：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221212155839668.png"  style="zoom:50%;" /></p>
<p>然后获得<span class="math inline">\(w_i,w_j\)</span>的最终表征：</p>
<figure>
<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221212155908737.png" alt="image-20221212155908737" /><figcaption>image-20221212155908737</figcaption>
</figure>
<p>预测：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221212155948456.png"   style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221212155957601.png"   style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221212160020384.png"   style="zoom:50%;" /></p>
<h2 id="experiment">3 Experiment</h2>
<figure>
<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221212160427390.png" alt="image-20221212160427390" /><figcaption>image-20221212160427390</figcaption>
</figure>
<p>这里的数据集JMERE，是作者联合了MNRE数据集和MNER（推测应该是Twitter-2015）取交集之后的结果：</p>
<figure>
<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221212160518901.png" alt="image-20221212160518901" /><figcaption>image-20221212160518901</figcaption>
</figure>
]]></content>
      <categories>
        <category>Paper</category>
        <category>MMKG</category>
      </categories>
      <tags>
        <tag>MMKG</tag>
        <tag>multimodal</tag>
        <tag>MNER</tag>
        <tag>MRE</tag>
      </tags>
  </entry>
  <entry>
    <title>FL-MSRE</title>
    <url>/mmml/FL-MSRE/</url>
    <content><![CDATA[<h1 id="fl-msre-a-few-shot-learning-based-approach-to-multimodal-social-relation-extraction">FL-MSRE: A Few-Shot Learning based Approach to Multimodal Social Relation Extraction</h1>
<p>AAAI 2021，<a href="https://github.com/%20sysulic/FL-MSRE">代码</a>。</p>
<blockquote>
<p>Social relation extraction (SRE for short), which aims to infer the social relation between two people in daily life, has been demonstrated to be of great value in reality. <strong>Existing methods for SRE consider extracting social relation only from unimodal information such as text or image, ignoring the high coupling of multimodal information</strong>. Moreover, previous studies overlook the serious unbalance distribution on social relations. To address these issues, this paper proposes FL-MSRE, a few-shot learning based approach to extracting social relations from both texts and face images. Considering the lack of multimodal social relation datasets, this paper also presents three multimodal datasets annotated from four classical masterpieces and corresponding TV series. Inspired by the success of BERT, we propose a strong BERT based baseline to extract social relation from text only. FL-MSRE is empirically shown to outperform the baseline signiﬁcantly. This demonstrates that using face images beneﬁts text-based SRE. Further experiments also show that using two faces from different images achieves similar performance as from the same image. This means that FL-MSRE is suitable for a wide range of SRE applications where the faces of two people can only be collected from different images.</p>
</blockquote>
<p>作者在这篇工作中，创建了包括文本和脸部图像的多模态social relation extraction数据集，Dream of the Red Chamber (DRC-TF), Outlaws of the Marsh (OM-TF) and the Four Classic (FC-TF)。红楼梦、水浒传和四大名著数据集，TF指text and face。</p>
<p>并且由于不同social relation的分布差异很大，作者考虑使用少次学习来解决，提出了方法FL-MSRE。</p>
<span id="more"></span>
<h2 id="introduction">1 Introduction</h2>
<p><strong>motivation</strong>：之前的social relation extraction主要集中在对单模态信息的处理，忽略了多模态之间信息可能存在高耦合。比如在下图，仅仅通过文本是不能推断Obama和正在拥抱的人的实际关系的。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221017223017579.png"  style="zoom:50%;" /></p>
<p><strong>method</strong>：为了能够同时从text和image中导出信息，由于目前没有合适的数据集，作者在Du et al.的从四大名著导出的基于文本数据集基础上进行拓展，通过从翻拍的电视剧中提取对应人物的图像，来预测人物实体之间的关系。同时，鉴于不同relation之间的分布差异巨大，作者考虑使用少次学习来进行关系抽取。</p>
<h2 id="multimodal-social-relation-datasets">2 Multimodal Social Relation Datasets</h2>
<p>构造过程：</p>
<ul>
<li>Du et al.等人从中国四大名著的文本中导出了至少包含两个人的句子；</li>
<li>作者在此基础上，通过人工标注判断两个人之间是否存在social relation；如果两个人有多种social relation，选择最specific的relation；</li>
<li>使用FFmepg删除字幕，删除重复的图片；</li>
<li>使用FaceNet选择出至少包括两个人的图片；每个人的脸部被bounding box框出来，并且标注了是哪个角色；</li>
</ul>
<p>最后，由于有的名著样本量太少，因此分为了三个数据集：Dream of the Red Chamber (DRC-TF), Outlaws of the Marsh (OM-TF) and the Four Classic (FC-TF)。</p>
<p>统计情况：</p>
<figure>
<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221017223855386.png" alt="image-20221017223855386" /><figcaption>image-20221017223855386</figcaption>
</figure>
<p>不同关系对应的句子数量：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221017224420446.png" style="zoom:40%;" /></p>
<p>查看下具体的数据：</p>
<p>Text：</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">&quot;servant_girl&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="attr">&quot;sbj&quot;</span>: <span class="string">&quot;雪雁&quot;</span>,</span><br><span class="line">            <span class="attr">&quot;obj&quot;</span>: <span class="string">&quot;林黛玉&quot;</span>,</span><br><span class="line">            <span class="attr">&quot;sentence&quot;</span>: <span class="string">&quot;每人一个奶娘并一个丫头照管，余者在外间上夜听唤．一面早有熙凤命人送了一顶藕合色花帐，并几件锦被缎褥之类．林黛玉只带了两个人来：一个是自幼奶娘王嬷嬷，一个是十岁的小丫头，亦是自幼随身的，名唤作雪雁．&quot;</span>,</span><br><span class="line">            <span class="attr">&quot;mask_sentence&quot;</span>: <span class="string">&quot;每人一个奶娘并一个丫头照管，余者在外间上夜听唤．一面早有熙凤命人送了一顶藕合色花帐，并几件锦被缎褥之类．$尾$只带了两个人来：一个是自幼奶娘王嬷嬷，一个是十岁的小丫头，亦是自幼随身的，名唤作#头#．&quot;</span></span><br><span class="line">        &#125;,</span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p>Image：</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">&quot;林黛玉&quot;: &#123;</span><br><span class="line">        &quot;雪雁&quot;: [</span><br><span class="line">            &quot;hlm_EP01_2282.jpg&quot;,</span><br><span class="line">            &quot;hlm_EP01_2336.jpg&quot;,</span><br><span class="line">            &quot;hlm_EP01_2448.jpg&quot;,</span><br><span class="line">            &quot;hlm_EP40_1706.jpg&quot;,</span><br><span class="line">            &quot;hlm_EP43_1600.jpg&quot;,</span><br><span class="line">            <span class="string">&quot;hlm_EP43_1645.jpg&quot;</span></span><br><span class="line">        ],</span><br><span class="line">        &quot;春纤&quot;: [</span><br><span class="line">            <span class="string">&quot;hlm_EP16_0681.jpg&quot;</span></span><br><span class="line">        ]</span><br><span class="line">    &#125;,</span><br></pre></td></tr></table></figure>
<p><code>hlm_EP01_2282.jpg</code>：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/hlm_EP01_2282.jpg" alt="hlm_EP01_2282" style="zoom: 33%;" /></p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>MMKG</category>
      </categories>
      <tags>
        <tag>MMKG</tag>
        <tag>MRE</tag>
      </tags>
  </entry>
  <entry>
    <title>HVPNeT</title>
    <url>/mmml/HVPNeT/</url>
    <content><![CDATA[<h1 id="good-visual-guidance-makes-a-better-extractor-hierarchical-visual-prefix-for-multimodal-entity-and-relation-extraction">Good Visual Guidance Makes A Better Extractor: Hierarchical Visual Prefix for Multimodal Entity and Relation Extraction</h1>
<p>Findings of NAACL 2022，<a href="https://github.com/zjunlp/HVPNeT">代码</a>。</p>
<p>作者认为目前的MNER和MRE方法无法很好的处理图像和文本内容不匹配的问题，因此提出了一种从图像中提取object-level的层级信息，用于补充文本信息的多模态信息抽取方法HVPNeT (Hierarchical Visual Prefix fusion NeTwork)。</p>
<blockquote>
<p>Multimodal named entity recognition and relation extraction (MNER and MRE) is a fundamental and crucial branch in information extraction. <strong>However, existing approaches for MNER and MRE usually suffer from error sensitivity when irrelevant object images incorporated in texts.</strong> To deal with these issues, we propose a novel Hierarchical Visual Prefix fusion NeTwork (HVPNeT) for visual-enhanced entity and relation extraction, aiming to achieve more effective and robust performance. Specifically, we regard visual representation as pluggable visual prefix to guide the textual representation for error insensitive forecasting decision. We further propose a dynamic gated aggregation strategy to achieve hierarchical multiscaled visual features as visual prefix for fusion. Extensive experiments on three benchmark datasets demonstrate the effectiveness of our method, and achieve state-of-the-art performance 1 .</p>
</blockquote>
<span id="more"></span>
<h2 id="introduction">1 Introduction</h2>
<p><strong>问题</strong>：作者认为一开始的MNER和MRE工作倾向于把整个图像的特征考虑到文本表征中；后来的工作倾向于把object-level的图像特征考虑到文本表征中；最近，RpBERT虽然能够判断整个图像和文本内容是否相关，但是不能做到判断visual object和文本是否相关。但考虑到实际情况，一个图像中可能包含了比较相关的object，也可能包含了不太相关的object。</p>
<p>因此有必要更好的学习视觉表征，同时降低模型对不相关的visual object的错误敏感性。</p>
<p><strong>方法</strong>：作者首先识别出图像中存在的多个visual object，然后利用CNN导出visual object的层级/金字塔型视觉特征；作者把这种层级的视觉特征看做是对于文本表征的视觉前缀visual prefix；visual prefix输入到BERT的每一层，用来提供文本表征所需的视觉信息。</p>
<h2 id="method">2 Method</h2>
<p>整体结构：</p>
<figure>
<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221020143211904.png" alt="image-20221020143211904" /><figcaption>image-20221020143211904</figcaption>
</figure>
<h3 id="collection-of-pyramidal-visual-feature">2.1 Collection of Pyramidal Visual Feature</h3>
<p>作者首先利用visual grounding tool (A fast and accurate one-stage approach to visual grounding. ICCV 2019) 来标注出图像的前<span class="math inline">\(m\)</span>个视觉对象（在实现中<span class="math inline">\(m=3\)</span>）。</p>
<p>然后把整个图像<span class="math inline">\(I\)</span>和不同的视觉对象<span class="math inline">\(O=\{ o_1,o_2,\dots,o_m \}\)</span> resale为<span class="math inline">\(224\times244\)</span>的图像。</p>
<p>对于每个图像，作者利用Resnet-50的不同block，导出<span class="math inline">\(c\)</span>层的视觉特征：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221020143823646.png"   style="zoom:50%;" /></p>
<p>然后把这些不同size的特征，利用1维卷积和池化操作重新映射为具有相同size合适大小：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221020144000443.png"   style="zoom:50%;" /></p>
<h3 id="dynamic-gated-aggregation">2.2 Dynamic Gated Aggregation</h3>
<p>由于不同的Transformer层可能会需要不同的视觉特征，因此对于第<span class="math inline">\(l\)</span>层的Transformer，对于单个图像导出的层级视觉信息<span class="math inline">\(V_i\)</span>，作者首先通过一个全局平均池化操作把3维张量进行压缩，然后求和，最后计算attention weight：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221020144533174.png"   style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221020144648816.png"   style="zoom:50%;" /></p>
<p>随后，聚合不同层的视觉特征：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221020145201860.png"   style="zoom:50%;" /></p>
<p>每个图像都进行了gated aggregation之后，把所有图像的聚合结果拼接到一起，作为最后要输入到第<span class="math inline">\(l\)</span>层Transformer的视觉特征：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221020145335875.png"   style="zoom:50%;" /></p>
<h3 id="visual-prefix-guided-fusion">2.3 Visual Prefix-guided Fusion</h3>
<p>接下来的问题是，如何把视觉特征加入到文本表征中去。</p>
<p>首先是，基于BERT-base结构，对于输入文本序列</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221020145541385.png"   style="zoom:50%;" /></p>
<p>先产生query、key和value：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221020145612172.png"  style="zoom:50%;" /></p>
<p>然后根据视觉特征，产生key和value作为visual prefix：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221020145704861.png"   style="zoom:50%;" /></p>
<p>最后，进行聚合：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221020145746443.png"   style="zoom:50%;" /></p>
<p>作者这种做法，是follow了Simvlm: Simple visual language model pretraining with weak supervision.的工作。</p>
<h3 id="classifier">2.4 Classifier</h3>
<p>对于MNER，采用常用的CRF层：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221020150008708.png"   style="zoom:50%;" /></p>
<p>对于MRE，通过提前加入的<span class="math inline">\([CLS]\)</span> token进行分类：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221020150148608.png"   style="zoom:50%;" /></p>
<h2 id="experimental-results">3 Experimental Results</h2>
<p>总体实验结果：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221020150325131.png"   style="zoom:50%;" /></p>
<ul>
<li>HVPNeT-Flat：直接使用ResNet的最后输出结果，而不是使用层级的视觉特征（个人感觉从结果来看，不同层级的视觉特征对MRE的影响更大）</li>
<li>HVPNeT-1T3：由于ResNet有4个block，BERT有12层，所以作者尝试了把1个block对应到3个BERT层，而不是直接把所有的block都输入到每一个BERT层</li>
<li>HVPNeT-OnlyObj：只使用object-level的特征，不使用image-level的特征。（可以看到，即使不使用image-level的信息，差距也不是很大，说明主要起作用的还是object level的信息）</li>
</ul>
]]></content>
      <categories>
        <category>Paper</category>
        <category>MMKG</category>
      </categories>
      <tags>
        <tag>MMKG</tag>
        <tag>MRE</tag>
      </tags>
  </entry>
  <entry>
    <title>IKRL</title>
    <url>/mmml/IKRL/</url>
    <content><![CDATA[<h1 id="image-embodied-knowledge-representation-learning">Image-embodied Knowledge Representation Learning</h1>
<p>清华大学2017年发表在IJCAI上的paper，IKRL，应该是第一个把图像信息注入到KGE中的方法。</p>
<p>基于TransE的思想，为不同的entity学习一个额外的image embedding，然后image embedding和原来的entity embedding通过<span class="math inline">\(h+r\approx t\)</span>评估三元组是否成立。</p>
<blockquote>
<p>Entity images could provide signiﬁcant visual information for knowledge representation learning. Most conventional methods learn knowledge representations merely from structured triples, ignoring rich visual information extracted from entity images. In this paper, we propose a novel Imageembodied Knowledge Representation Learning model (IKRL), where knowledge representations are learned with both triple facts and images. More speciﬁcally, we ﬁrst construct representations for all images of an entity with a neural image encoder. These image representations are then integrated into an aggregated image-based representation via an attention-based method. We evaluate our IKRL models on knowledge graph completion and triple classiﬁcation. Experimental results demonstrate that our models outperform all baselines on both tasks, which indicates the signiﬁcance of visual information for knowledge representations and the capability of our models in learning knowledge representations with images.</p>
</blockquote>
<span id="more"></span>
<h2 id="introduction">1 Introduction</h2>
<p>作者首先举了一个例子来说明图片包含了能够辅助建模KGE：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220907162730326.png"   style="zoom:40%;" /></p>
<p>关系<span class="math inline">\(has\ part\)</span>是一个spatial relation，它在图像体现的信息就比单纯在文本上要丰富，与action relation一样都比较适合可视化。但是要注意有很多的relation是很难可视化的，单纯在图像上也不太好进行推测，除非图像本身包含了明确的信息。比如关系<span class="math inline">\(spouse\)</span>，我们无法单纯从两个男女的照片上判断是不是配偶，但如果有两个人结婚的照片，我们就可以推测他们是配偶。</p>
<h2 id="method">2 Method</h2>
<p>作者的方法：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220907163134837.png"   style="zoom:40%;" /></p>
<p>主要就是多了一个image embedding <span class="math inline">\(e_I\)</span>，在获得了image embedding后，进行translation-based的推测：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220907163251722.png"   style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220907163304108.png"   style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220907163411546.png"   style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220907163314931.png"   style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220907163326499.png"  style="zoom:50%;" /></p>
<p>如何获得image embedding？作者通过AlexNet（5卷积层+2全连接层）获得image representation，然后投影至entity space：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220907163549753.png"   style="zoom:40%;" /></p>
<p>然后结合注意力聚合：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220907163736360.png"  style="zoom:40%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220907163750179.png"   style="zoom:40%;" /></p>
<h2 id="experiments">3 Experiments</h2>
<p>值得一提的是，作者构造了一个新的数据集WN9-IMG，可惜的是效果已经要做到顶了。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220907164242990.png"   style="zoom:50%;" /></p>
<p>链路预测结果：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220907164347235.png"   style="zoom:50%;" /></p>
<p>三元组分类结果（判断三元组是否成立），通过计算<span class="math inline">\(||h+r-t||\)</span>是否高于阈值<span class="math inline">\(\delta_r\)</span>，<span class="math inline">\(\delta_r\)</span>是一个可训练参数，通过评估在验证集下的效果进行更新：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220907164431131.png"   style="zoom:50%;" /></p>
<h2 id="conclusion">Conclusion</h2>
<p>一个比较简单直接的基于TransE的MM KGE方法，把图像信息注入到实体表示中。有以下缺点：</p>
<ul>
<li>以image为单位进行attention过于粗糙，明显会带来大量的noise；并且从作者的实验来看，attention效果不够显著。</li>
<li>没有融合text information，算不上真正的多模态（不把单纯的三元组看做是一种模态的话）。</li>
<li>实际上学习到的image embedding也没有真正的融合到entity embedding中，仅仅独立存在着作为预测效果的一部分。entity embedding只是被用来评估那个image embedding更重要而已。</li>
</ul>
]]></content>
      <categories>
        <category>Paper</category>
        <category>MMKG</category>
      </categories>
      <tags>
        <tag>MMKG</tag>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>MAF</title>
    <url>/mmml/MAF/</url>
    <content><![CDATA[<h1 id="maf-a-general-matching-and-alignment-framework-for-multimodal-named-entity-recognition">MAF: A General Matching and Alignment Framework for Multimodal Named Entity Recognition</h1>
<p>WSDM 2022，<a href="https://github.com/xubodhu/MAF">代码</a>，复旦大学。</p>
<p>作者通过判断post的text和image的匹配程度，计算进入文本表征中的图像信息，并且期望能够通过保持text和image不同模态表征的一致性。</p>
<blockquote>
<p>In this paper, we study multimodal named entity recognition in social media posts. Existing works mainly focus on using a crossmodal attention mechanism to combine text representation with image representation. However, they still suffer from two weaknesses: (1) the current methods are based on a strong assumption that each text and its accompanying image are matched, and the image can be used to help identify named entities in the text. However, this assumption is not always true in real scenarios, and the strong assumption may reduce the recognition effect of the MNER model; (2) the current methods fail to construct a consistent representation to bridge the semantic gap between two modalities, which prevents the model from establishing a good connection between the text and image. To address these issues, we propose a general matching and alignment framework (MAF) for multimodal named entity recognition in social media posts. Specifically, <strong>to solve the first issue, we propose a novel cross-modal matching (CM) module to calculate the similarity score between text and image, and use the score to determine the proportion of visual information that should be retained.</strong> <strong>To solve the second issue, we propose a novel cross-modal alignment (CA) module to make the representations of the two modalities more consistent.</strong>We conduct extensive experiments, ablation studies, and case studies to demonstrate the effectiveness and efficiency of our method.The source code of this paper can be found in https://github.com/xubodhu/MAF.</p>
</blockquote>
<span id="more"></span>
<h2 id="introduction">1. Introduction</h2>
<p><strong>问题</strong>：</p>
<ol type="1">
<li>很多目前的MNER方法建立在认为post的text和image是匹配的假设上，因此总是会同时使用text和image的信息进行NER。但是并不是所有的text和image都是匹配的。</li>
</ol>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221117204106769.png"   style="zoom:30%;" /></p>
<ol start="2" type="1">
<li>现有的方法忽略了学习text和image两个模态表征的一致性，因此两个模态的表征之间存在语义差异（semantic gap）</li>
</ol>
<p><strong>方案</strong>：</p>
<ol type="1">
<li>提出了一个跨模态匹配模块（cross-modal matching，CM）来计算text和image的相似度得分</li>
<li>提出了一个跨模态对齐模块（cross-modal alignment，CA）使得两个模态的表征更加一致</li>
</ol>
<h2 id="method">2. Method</h2>
<p>总体结构：</p>
<figure>
<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221117204539318.png" alt="image-20221117204539318" /><figcaption>image-20221117204539318</figcaption>
</figure>
<p>两个模态的encoder：</p>
<ul>
<li>text encoder：pretrained BERT-base。输入是最大长度128的token序列，在开头加入[CLS] token，在空白处加入[PAD] token。输出是768维向量。总的Transformer block有12层。</li>
<li>image encoder：pretrained 152层ResNet。输入是224x224的resized image，输出是<span class="math inline">\(2048\times 7\times 7\)</span>的代表49个region的张量。每个region向量通过独立的投影矩阵，转化为768维向量。</li>
</ul>
<h3 id="cross-modal-alignment-module-ca">2.1 Cross-Modal Alignment Module (CA)</h3>
<p>使用[CLS]的embedding作为文本序列的总表示<span class="math inline">\(T_s\)</span>；使用<span class="math inline">\(7\times 7\)</span>的均值池化操作获得图像的总表示<span class="math inline">\(T_g\)</span>。</p>
<p>两个表示通过独立的MLP来投影到具有相同维度大小的空间中，获得<span class="math inline">\(T_c\)</span>和<span class="math inline">\(V_c\)</span>。</p>
<p>然后使用这两个表征来尝试让两个模态空间下的向量表示具有更多的一致性。</p>
<p>基于对比学习学习text和image的匹配距离，在一个batch中，把image或者text换为其它post对应的image或者text作为负样本，把原来的样例作为正样本。</p>
<p>首先，计算正样本的Image embedding和所有样本的Text embeddings的距离，对比损失为：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221117205634445.png"   style="zoom:50%;" /></p>
<p>其中，计算相似度的函数是余弦相似度。</p>
<p>然后，计算正样本的Text embedding和所有样本的Image embeddings的距离，对比损失为：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221117210112669.png"   style="zoom:50%;" /></p>
<p>两个loss加载一起，就是CA模块的text-image匹配loss：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221117210157967.png"   style="zoom:50%;" /></p>
<h3 id="cross-modal-interaction-module">2.2 Cross-Modal Interaction Module</h3>
<p>基于co-attention，text作为query，image作为key和value，学习text-aware的图像表征：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221117210326579.png"   style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221117210336910.png"  style="zoom:50%;" /></p>
<h3 id="cross-modal-matching-module-cm">2.3 Cross-Modal Matching Module (CM)</h3>
<p>作者设计了个CM模块，用来计算前一步学习到的text-aware的图像表征应该有多少被保留。</p>
<p>同样是基于text-image匹配任务，但是并不是基于对比学习，不使用对比学习，而是二分类问题，预测是否matching。</p>
<p>训练样本的构造也不同，不再是基于每个正样本都分别构造负样本，而是直接在一个batch中，选择2k个样本，前k个样本的image和后k个样本的image进行互换，构造出负样本；剩下的batch中的样本作为正样本。</p>
<p>预测是否匹配，直接把text和image图像展开，然后拼接输入到MLP</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221117212038708.png" style="zoom:50%;" /></p>
<p>训练的loss就是BCE：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221117212103443.png"   style="zoom:50%;" /></p>
<p>然后，根据这个分类器，可以判断在整个图像层次下，有多少信息应该保留：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221117212309060.png"  style="zoom:50%;" /></p>
<h3 id="cross-modal-fusion-module">2.4 Cross-Modal Fusion Module</h3>
<p>一个基于gate的模块被作者用来决定，在token level上有多少图像信息应该保留：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221117212538168.png"  style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221117212546622.png"   style="zoom:50%;" /></p>
<p>其中，<span class="math inline">\(g\in \mathbb{R}^{d\times (n+2)}\)</span>是token level的权重。</p>
<p>最后，经过层层过滤的图像信息，与文本表征进行拼接，就得到了最后的表征<span class="math inline">\(H\)</span>。</p>
<p>整个模型训练的loss，<span class="math inline">\(\alpha=\beta=0.2\)</span>：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221117213043975.png"   style="zoom:50%;" /></p>
<h2 id="experiment">3. Experiment</h2>
<p>整体性能并不是非常突出：</p>
<figure>
<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221117213142042.png" alt="image-20221117213142042" /><figcaption>image-20221117213142042</figcaption>
</figure>
<p>消融实验：</p>
<figure>
<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221117213159257.png" alt="image-20221117213159257" /><figcaption>image-20221117213159257</figcaption>
</figure>
]]></content>
      <categories>
        <category>Paper</category>
        <category>MMKG</category>
      </categories>
      <tags>
        <tag>MMKG</tag>
        <tag>multimodal</tag>
        <tag>MNER</tag>
      </tags>
  </entry>
  <entry>
    <title>MEGA</title>
    <url>/mmml/MEGA/</url>
    <content><![CDATA[<h1 id="multimodal-relation-extraction-with-efficient-graph-alignment">Multimodal Relation Extraction with Efficient Graph Alignment</h1>
<p>ACM MM 21，<a href="https://github.com/thecharm/Mega">代码</a></p>
<p>作者提出了一种，通过识别图像的scene graph和textual graph，进行图对齐的多模态关系抽取方法MEGA。</p>
<blockquote>
<p>Relation extraction (RE) is a fundamental process in constructing knowledge graphs. However, previous methods on relation extraction suffer sharp performance decline in short and noisy social media texts due to a lack of contexts. Fortunately, the related visual contents (objects and their relations) in social media posts can supplement the missing semantics and help to extract relations precisely. We introduce the multimodal relation extraction (MRE), a task that identifies textual relations with visual clues. To tackle this problem, we present a large-scale dataset which contains 15000+ sentences with 23 pre-defined relation categories. Considering that the visual relations among objects are corresponding to textual relations, we develop a dual graph alignment method to capture this correlation for better performance. Experimental results demonstrate that visual contents help to identify relations more precisely against the text-only baselines. Besides, our alignment method can find the correlations between vision and language, resulting in better performance. Our dataset and code are available at https://github.com/thecharm/Mega.</p>
</blockquote>
<span id="more"></span>
<h2 id="introduction">1 Introduction</h2>
<p><strong>problem</strong>：之前的关系抽取主要有两种，sequence-based和dependency-based方法。但是这些方法主要集中在文本信息的抽取，如果应用到social media posts这样文本信息比较少，缺乏上下文信息的时候，效果会很差。</p>
<p><strong>motivation</strong>：作者发现，可以使用post中的image来补充缺失的上下文信息。作者认为，和多模态命名实体有所区别的是，MRE不仅要考虑捕获visual object和textual entity的联系，还要考虑visual object之间的visual relation和textual relation之间的联系。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221019102038143.png"   style="zoom:40%;" /></p>
<p>比如在上面的实例中，visual relation <code>holding</code>可以用来辅助推测textual relation <code>awarded</code>。</p>
<h2 id="method">2 Method</h2>
<p>总体结构：</p>
<figure>
<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221019152840889.png" alt="image-20221019152840889" /><figcaption>image-20221019152840889</figcaption>
</figure>
<h3 id="semantic-feature-representation">2.1 Semantic Feature Representation</h3>
<p>首先是通过BERT和scene graph generation来获得文本和视觉的表征。</p>
<h4 id="textual-semantic-representation">2.1.1 Textual Semantic Representation</h4>
<p>对于输入的文本序列<span class="math inline">\(s_1\)</span>，添加token为下面的形式：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221019153202538.png" alt="image-20221019153202538" style="zoom:50%;" /></p>
<p>为了让所有的序列有固定长度<span class="math inline">\(l\)</span>，对于长度不足<span class="math inline">\(l\)</span>的序列添加token <span class="math inline">\([PAD]\)</span>。</p>
<p>随后，作者还设置了另一个序列<span class="math inline">\(s_2\)</span>区分正常token和<span class="math inline">\([PAD]\)</span> token，<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221019153452300.png"   style="zoom:50%;" />。</p>
<p>最后，fine-tune下pretrained好的BERT就得到了token的表征：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221019153612318.png"   style="zoom:50%;" /></p>
<h4 id="visual-semantic-representation">2.1.2 Visual Semantic Representation</h4>
<p>使用前人的工作<em>Unbiased Scene Graph Generation From Biased Training CVPR 2020</em>来获得scene graph以及对应的表征：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221019153833135.png"   style="zoom:50%;" /></p>
<p>由于可能识别出很多不相关的visual object，因此作者设置一个阈值，只有大于这个阈值的，并且是前<span class="math inline">\(m\)</span>个最大分类得分的object才会被使用。如果选择出来的数量小于<span class="math inline">\(m\)</span>，就添加0向量。</p>
<h3 id="structural-feature-representation">2.2 Structural Feature Representation</h3>
<p>使用ELMo (<em>Deep contextualized word representations. NAACL 2018</em>)将input text解析为语法依赖树Syntax Dependency Tree。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221019154240627.png"  style="zoom:50%;" /></p>
<p>这样的语法依赖树就可以表示为一个文本图 <span class="math inline">\(G_1\)</span>：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221019154320428.png"   style="zoom:50%;" /></p>
<p>类似的，场景图也是一个graph <span class="math inline">\(G_2\)</span>：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221019154414958.png"  style="zoom:50%;" /></p>
<h3 id="multimodal-feature-alignment">2.3 Multimodal Feature Alignment</h3>
<h4 id="graph-structure-alignment">2.3.1 Graph Structure Alignment</h4>
<p>图结构的对齐，主要是有两步，一是通过分解节点标识node identity相似矩阵来获取node embedding；二是通过计算node embedding之间的相似度来对齐实体。</p>
<p>在学习node embedding时，作者follow了<em>REGAL: Representation Learning-based Graph Alignment. CIKM 2018</em>的工作。下面的具体原理没懂。</p>
<p>首先是统计两个graph下每个node的度分布：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221020185920116.png"   style="zoom:50%;" /></p>
<p>然后，利用这样的度分布可以评估两个node之间的节点相似度：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221020190013767.png"   style="zoom:50%;" /></p>
<p>然后，随机选择<span class="math inline">\(p\)</span>个node作为&quot;landmark&quot; node，计算它们和所有node之间的相似度，可以得到相似度矩阵<span class="math inline">\(C\in \mathbb{R}^{n\times p}\)</span>。从矩阵<span class="math inline">\(C\)</span>中，可以选出<span class="math inline">\(p\times p\)</span>的landmark-to-landmark矩阵<span class="math inline">\(W_p\)</span>。</p>
<p>。。。</p>
<p>最后 ，得到了node embedding：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221020191126590.png"   style="zoom:50%;" /></p>
<p>使用node embedding计算node之间的相似度，对于每个node，选择和它相似度最大的node作为对齐的node：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221020191221431.png"   style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221020191236209.png"  style="zoom:50%;" /></p>
<p>最后的矩阵<span class="math inline">\(\alpha\)</span>的第<span class="math inline">\(i\)</span>行<span class="math inline">\(j\)</span>列表示第<span class="math inline">\(i\)</span>个word和第<span class="math inline">\(j\)</span>个object的结构的相似度。这本文当中，作者只保留了最相关的object（也就是每一行相似度最大的值保留下来），其它的都置为0。</p>
<h4 id="semantic-features-alignment">2.3.2 Semantic Features Alignment</h4>
<p>假设得到的文本表征是<span class="math inline">\(X\)</span>，视觉表征是<span class="math inline">\(Y\)</span>，通过自注意力来进行计算：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221020191522282.png"   style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221020191541700.png"   style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221020191722037.png"   style="zoom:50%;" /></p>
<p>最后，同时使用structural和semantic对齐的结果：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221020191915124.png"   style="zoom:50%;" /></p>
<h3 id="entities-representation-concatenation">2.4 Entities Representation Concatenation</h3>
<p>聚合所有object的视觉表征：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221020192128473.png"   style="zoom:50%;" /></p>
<p>获得实体的文本表征：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221020192148484.png"   style="zoom:50%;" /></p>
<p>输出关系预测结果：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221020192207164.png"   style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221020192222002.png"   style="zoom:50%;" /></p>
<h2 id="experimental-results">3 Experimental Results</h2>
<p>MRE实验结果：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221020192344464.png"   style="zoom:50%;" /></p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>MMKG</category>
      </categories>
      <tags>
        <tag>MMKG</tag>
        <tag>MRE</tag>
      </tags>
  </entry>
  <entry>
    <title>MKGformer</title>
    <url>/mmml/MKGformer/</url>
    <content><![CDATA[<h1 id="hybrid-transformer-with-multi-level-fusion-for-multimodal-knowledge-graph-completion">Hybrid Transformer with Multi-level Fusion for Multimodal Knowledge Graph Completion</h1>
<p>SIGIR 2022，<a href="https://github.com/zjunlp/MKGformer">代码</a>，Zhejiang University。</p>
<p>作者提出了一种基于Transformer的能够适用于不同多模态知识图谱预测任务的方法，MKGformer。对于不同的预测任务，作者通过定义输入数据和输出数据拥有相同的格式，从而到达不改变模型结构，还能够同时用于不同预测任务；其次，作者提出了一种在text和image模态之间，进行multi-level混合的Transformer结构。</p>
<p>作者在多模态KG补全、多模态关系抽取和多模态命名实体识别三个任务的有监督学习和低资源学习的场景上进行了实验。</p>
<blockquote>
<p>Multimodal Knowledge Graphs (MKGs), which organize visualtext factual knowledge, have recently been successfully applied to tasks such as information retrieval, question answering, and recommendation system. Since most MKGs are far from complete, extensive knowledge graph completion studies have been proposed focusing on the multimodal entity, relation extraction and link prediction. However, different tasks and modalities require changes to the model architecture, and not all images/objects are relevant to text input, which hinders the applicability to diverse real-world scenarios. In this paper, we propose a hybrid transformer with multi-level fusion to address those issues. Specifically, we leverage a hybrid transformer architecture with unified input-output for diverse multimodal knowledge graph completion tasks. Moreover, we propose multi-level fusion, which integrates visual and text representation via coarse-grained prefix-guided interaction and fine-grained correlation-aware fusion modules. We conduct extensive experiments to validate that our MKGformer can obtain SOTA performance on four datasets of multimodal link prediction, multimodal RE, and multimodal NER.</p>
</blockquote>
<span id="more"></span>
<h2 id="introduction">1 Introduction</h2>
<p>作者认为目前的多模态KGC任务存在以下问题：</p>
<ol type="1">
<li>Architecture universality：不同的KGC任务，对于不同模态需要设计不同的编码器，从而限制了模型的通用性和易用性。</li>
<li>Modality contradiction：大多的multimodal KGC的方法很大程度上忽略了图像信息可能带来的噪音问题，因为在多模态KG中，一个实体可能会关联到多个不同的image，实际上只有部分的图像信息可能才是所需的。</li>
</ol>
<p>为了解决上述问题，作者提出了：</p>
<ol type="1">
<li>之前有研究者发现，预训练模型能够在Transformer的self-attention层和feed-forward层激活和输入数据相关的knowledge。因此，作者尝试基于Transformer架构，同时学习textual和visual的信息。</li>
<li>作者提出的MKGformer，有两个核心结构，prefix-guided interaction module (PGI)和correlation-aware fusion module (CAF)。前者用于pre-reduce不同模态的heterogeneity，后者用来进一步降低模型对于irrelevant image/text的错误敏感性。</li>
</ol>
<h2 id="approach">2 Approach</h2>
<p>总体结构：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220902145750837.png"  style="zoom:40%;" /></p>
<h3 id="unified-multimodal-kgc-framework">2.1 Unified Multimodal KGC Framework</h3>
<p>对于文本，使用BERT进行编码（T-Encoder）；对于图像，使用ViT (<em>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</em>)进行编码（V-Encoder）。先分别独立进行几层的学习之后，在最后<span class="math inline">\(M\)</span>层，利用作者提出的M-Encoder进行模态混合。需要注意的是，这里的M-Encoder并不是额外的层，而是作者在BERT和ViT的架构基础上，直接进行了改进，让不同模态模型之间能够进行信息流通。</p>
<p>模型对于输入和输入数据格式的变形，首先是有三个预测任务：</p>
<ol type="1">
<li><p>Multimodal Link Prediction is the most popular task for multimodal KGC, which focuses on predicting the tail entity given the head entity and the query relation, denoted by <span class="math inline">\((𝑒_ℎ ,𝑟, ?)\)</span>. 预测未知fact。多模态带来的新条件是，每个实体可能拥有多个image <span class="math inline">\(I_h\)</span>。</p></li>
<li>Multimodal Relation Extraction aims at linking relation mentions from text to a canonical relation type in a knowledge graph. 给定一段描述文本<span class="math inline">\(T\)</span>，已知其中的头尾实体<span class="math inline">\((e_h,e_t)\)</span>，预测实体间的关系<span class="math inline">\(r\)</span>。多模态带来的新条件是，描述文本有对应的image <span class="math inline">\(I\)</span>。</li>
<li><p>Multimodal Named Entity Recognition is the task of extracting named entities from text sequences and corresponding images. 从一个token序列中<span class="math inline">\(T=\{w_1,\dots,w_n\}\)</span>，预测对应的标签序列<span class="math inline">\(y={y_1,\dots,y_n}\)</span>。多模态带来的条件是，描述文本有对应的image <span class="math inline">\(I\)</span>。</p></li>
</ol>
<p>对于输入数据和预测数据的变形：</p>
<ol type="1">
<li><p>对于多模态链路预测，作者首先设计了特别的一步操作，Image-text Incorporated Entity Modeling，具体而言，在保持整个模型参数不动的情况下，只训练学习新出现的entity embedding。这样是的文本信息和视觉信息都能够融合到entity embedding上。对于实体<span class="math inline">\(e_i\)</span>关联的图像，输入到V-Encoder；对于实体<span class="math inline">\(e_i\)</span>的文本描述<span class="math inline">\(d_{e_i}=(w_1,\dots,w_n)\)</span>，改造为：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220902153930132.png" style="zoom:50%;" /></p>
<p>然后预测<span class="math inline">\([mask]\)</span>是实体<span class="math inline">\(e_i\)</span>的概率。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220902154509283.png"   style="zoom:50%;" /></p>
<p>随后，正式开始预测missing entity，将<span class="math inline">\((𝑒_ℎ ,𝑟, ?)\)</span>变形为：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220902154627839.png"   style="zoom:50%;" /></p></li>
<li><p>对于多模态命名实体识别，作者利用CRF函数（<em>Neural Architectures for Named Entity Recognition.</em>）进行预测（这个没看过..）</p></li>
<li><p>对于多模态关系抽取，作者在原来的文本描述上，加入<span class="math inline">\([CLS]\)</span> token，最后预测<span class="math inline">\([CLS]\)</span>是目标关系的概率：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220902155007744.png"   style="zoom:50%;" /></p></li>
</ol>
<p>对于MNER和MRE任务，使用<em>A Fast and Accurate One-Stage Approach to Visual Grounding. ICCV 2019</em> 导出前<span class="math inline">\(m\)</span>个visual objects。</p>
<p>对于MMKGC任务，直接使用整个图像。</p>
<h3 id="hybrid-transformer-architecture">2.2 Hybrid Transformer Architecture</h3>
<p>首先是原始的Transformer结构，MHA表示多头注意力，FFN表示前馈网络。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220902155227398.png"   style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220902155249701.png"   style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220902155307966.png"   style="zoom:50%;" /></p>
<p>V-Encoder，ViT：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220902155348746.png"   style="zoom:50%;" /></p>
<p>T-Encoder，BERT：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220902155427336.png"   style="zoom:50%;" /></p>
<p>M-Encoder，在V-Encoder和T-Encoder之间，先PGI，再CAF：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220902155825058.png"   style="zoom:50%;" /></p>
<h3 id="insights-of-m-encoder">2.3 Insights of M-Encoder</h3>
<h4 id="pgi">2.3.1 PGI</h4>
<p>对于PGI（Prefix-guided Interaction Module），作者是受到了前面研究的影响（<em>Prefix-Tuning: Optimizing Continuous Prompts for Generation</em>和<em>Towards a Unified View of Parameter-Efficient Transfer Learning.</em>）。</p>
<p>作者在自注意力层，让visual Transformer侧考虑聚合textual信息，通过让visual query和textual key，textual value进行操作。实际上是询问当前的patch image和哪些token更接近，然后聚合token embedding。视觉侧的query，文本侧和视觉侧的key，文本侧和视觉侧的value：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220902161732093.png"   style="zoom:50%;" /></p>
<p>很简单的操作，应该是直接拼接。作者进一步推算公式为：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220902161833850.png"   style="zoom:50%;" /></p>
<p>这里我没有直接推算出来。但是从作者推算出的可以看出来，实质上它是降低了原来单纯的visual attention，增加了文本-图像的跨模态注意力。</p>
<h4 id="caf">2.3.2 CAF</h4>
<p>对于CAF（Correlation-aware Fusion Module），作者受到前面研究的影响，之前有人发现Transformer中的FFN层能够学习到task-specific textual pattern（<em>Transformer Feed-Forward Layers Are Key-Value Memories</em>）。因此作者通过计算token embedding和patch embedding之间的相似性矩阵来衡量视觉信息的重要性。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220902163451512.png" alt="image-20220902163451512" style="zoom:50%;" /></p>
<p>然后聚合视觉信息，文本侧的query，视觉侧的key，视觉侧的value：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220902163506163.png" alt="image-20220902163506163" style="zoom:50%;" /></p>
<p>上述过程实际和自注意力的过程是一样的。最后把聚合的视觉信息和原来的文本信息拼接到一起：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220902163525975.png"   style="zoom:50%;" /></p>
<p>回顾下上述两个过程，作者都是没有直接创建新的layer进行信息融合，而是通过让信息在dual Transformer之间进行流通。因为作者提出图像的信息噪音很大，对自注意力层和全连接层的改造都是围绕这一点来的。先在注意力层让文本信息流通到视觉信息上，让V-Encoder侧能够考虑文本信息，而不是单纯在patch之间聚合信息。试想下，如果让视觉信息流通到文本信息上，那么就意味着视觉的噪音直接加入到了文本侧，不太合适。随后，在全连接层让已经考虑了文本信息的视觉信息，再流通回文本侧，进一步降低视觉噪音。</p>
<h2 id="experiments">3 Experiments</h2>
<h3 id="experimental-setup">3.1 Experimental Setup</h3>
<p>数据集：</p>
<ul>
<li>链路预测：WN18-IMG和FB15k-237-IMG，都是原来的数据集的实体分别关联到了10个image。</li>
<li>关系抽取：MNRE数据集，人工构造，来源Twitter。</li>
<li>命名实体识别：Twitter-2017，包括了2016-2017年间用户的多模态posts。</li>
</ul>
<p>训练设置：</p>
<p>在所有的情况下，M-Encoder保持3层，基于BERT_base和ViT-B/32。</p>
<h3 id="overall-performance">3.2 Overall Performance</h3>
<p>链路预测（作者提到了，原来的论文中对于FB15k-237-IMG的结果由于作者代码对于数据处理错误，因此出现了错误的性能提升，作者在arxiv上上传了更新后的结果）：</p>
<p>原论文结果：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220902165320438.png"   style="zoom:40%;" /></p>
<p>更新后的结果，可以看出来结果变化挺大</p>
<figure>
<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221118160223529.png" alt="image-20221118160223529" /><figcaption>image-20221118160223529</figcaption>
</figure>
<p>关系抽取和命名实体识别：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220902165357479.png"   style="zoom:40%;" /></p>
<h3 id="low-resource-evaluation">3.3 Low-Resource Evaluation</h3>
<p>作者认为对文本和图像，使用类似的网络结构进行处理，降低了差异性，在低资源预测任务中这种作用更加突出。在数据量更少的情况下，需要想办法更好的处理数据模态之间的差异性，因此模型对于不同模态的差异性的处理能力可能需要更加突出。</p>
<p>在低资源的设置下，作者发现直接把视觉-语言预训练模型应用到KGC任务上，并没有表现出特别优越的性能。作者认为可能是原来的预训练数据和KGC任务相关性不是特别相关的原因。</p>
<p>低资源链路预测：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220902165501174.png"  style="zoom:40%;" /></p>
<p>低资源关系抽取：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220902165640257.png"   style="zoom:40%;" /></p>
<p>低资源命名实体识别：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220902165544824.png"   style="zoom:40%;" /></p>
<h3 id="ablation-study">3.4 Ablation Study</h3>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220902165723859.png"   style="zoom:40%;" /></p>
<h3 id="case-analysis-for-image-text-relevance">3.5 Case Analysis for Image-text Relevance</h3>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220902165850066.png"   style="zoom:50%;" /></p>
<p>从这个实际案例可以看出，图像确实和整个描述文本是相关的，但是图像不一定能够对应到所需要的实体。并且，一个图像中存在很多不需要的噪音。</p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>KG</tag>
        <tag>multimodal</tag>
        <tag>Transformer</tag>
      </tags>
  </entry>
  <entry>
    <title>MM-Transformer-Survey</title>
    <url>/mmml/MM-Transformer-Survey/</url>
    <content><![CDATA[<h1 id="multimodal-learning-with-transformers-a-survey">Multimodal Learning with Transformers: A Survey</h1>
<p>2022-06 arxiv</p>
<p>牛津大学学者的一篇多模态Transformer综述，系统的描述了目前多模态Transformer可能注意的不同改进点。</p>
<blockquote>
<p>Transformer is a promising neural network learner, and has achieved great success in various machine learning tasks. Thanks to the recent prevalence of multimodal applications and big data, Transformer-based multimodal learning has become a hot topic in AI research. This paper presents a comprehensive survey of Transformer techniques oriented at multimodal data. The main contents of this survey include: (1) a background of multimodal learning, Transformer ecosystem, and the multimodal big data era, (2) a theoretical review of Vanilla Transformer, Vision Transformer, and multimodal Transformers, from a geometrically topological perspective, (3) a review of multimodal Transformer applications, via two important paradigms, i.e., for multimodal pretraining and for specific multimodal tasks, (4) a summary of the common challenges and designs shared by the multimodal Transformer models and applications, and (5) a discussion of open problems and potential research directions for the community.</p>
</blockquote>
<span id="more"></span>
<h2 id="introduction">1. Introduction</h2>
<p>我们期待的理想的人工智能具有的能力至少是可以做到人类能够做到的一切，这里就包括了人类感知世界的方式：看、听、摸等。人类使用特定感知器sensor和外界建立特定的交流通道，这种特定交流通道中传递/表达的信息形式我们称作是模态modality，比如语言或视觉：</p>
<blockquote>
<p>In general, a modality is often associated with a specific sensor that creates a unique communication channel, such as vision and language.</p>
</blockquote>
<p>这篇survey主要是考虑使用Transformer解决多模态任务，Transformer适用于多模态的几点原因：</p>
<ul>
<li>更少的模态特定的假设，比如RNN的序列化输入；CNN的局部迁移不变性，使得Transformer天然的适用于处理更多模态数据</li>
<li>对于许多多模态数据来说，可以被轻易的转换成适合于Transformer的序列输入形式</li>
<li>Transformer的内部结构，比如self-attention，很适合被改造为跨模态交互/多模态融合的形式</li>
</ul>
<p>有一些其它的survey是从更加广泛的模型来讨论多模态学习：</p>
<ul>
<li>Multimodal machine learning: A survey and taxonomy. 2018</li>
<li>Multimodal intelligence: Representation learning, information fusion, and applications. 2020</li>
<li>Multimodal co-learning: Challenges, applications with datasets, recent advances and future directions. 2022</li>
</ul>
<h2 id="background">2. Background</h2>
<p>多模态学习（multimodal machine learning，MML）并不是一个新词，从20世纪80年代开始就有人研究视觉听觉语音识别（<em>Integration of acoustic and visual speech signals using neural networks. 1989</em>）。在深度学习时代，随着Transformer模型的出现，算力的急速增长，多模态数据集规模的不断增加共同促进多模态学习进步。</p>
<p><em>更多背景请参考论文内容</em></p>
<h2 id="multimodal-transformers">3. Multimodal Transformers</h2>
<h3 id="multimodal-input">3.1 Multimodal Input</h3>
<p>对于任意模态数据，要输入到Transformer通常是做两步：</p>
<ol type="1">
<li><p>tokenize the input</p></li>
<li><p>select an embedding space to represent the tokens</p></li>
</ol>
<p>对于单模态数据，我们有不同的方法实现tokenization和选择合适的token embedding。比如对于image，我们可以选择ROI作为tokens，然后CNN导出的feature作为token embedding；可以选择将image划分成不同的patch，每个patch经过linear projection之后作为token embedding；也可以选择将image上的不同object作为tokens，使用GNN学习场景图的特征作为token embedding（<em>Multimodal sentiment detection based on multi-channel graph neural networks. 2021</em>）。</p>
<p>下面的表格是总结的一些多模态tokens处理的方法：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230223150507977.png" /></p>
<p>通常还会加入一些special tokens，用来服务一些特定的目的：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230223150851043.png" /></p>
<p>在实践中，人们常常会在embedding层选择融合不同的信息，属于early-fusion的一种。最常见的方式就是在每个位置的token上直接加上不同的信息。比如在原始的Transformer中，token embedding会加上position embedding；VL-BERT选择“linguistic token embedding <span class="math inline">\(\oplus\)</span> full image visual feature embedding”；InterBERT选择在ROI的embedding加入位置信息，“ROI embedding <span class="math inline">\(\oplus\)</span> location embedding”。</p>
<h3 id="self-attention-variants-in-multimodal-context">3.2 Self-Attention Variants in Multimodal Context</h3>
<p>接下来讨论用于多模态的self-attention变体。</p>
<p>下面是作者总结的变体：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230301104906563.png" /></p>
<p><strong>Early summation</strong></p>
<p>在embedding layer对于两个模态的token embedding直接进行element-wise summing：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230223172136061.png" /></p>
<p>其中的<span class="math inline">\(Z_{(A)}\)</span>和<span class="math inline">\(Z_{(B)}\)</span>表示是来自两个模态的token embedding matrix；<span class="math inline">\(\alpha\)</span>和<span class="math inline">\(\beta\)</span>是两个人工定义的权重；<span class="math inline">\(TF\)</span>表示Transformer layer/block。</p>
<p>这样做的好处是不会增加计算量。</p>
<p>坏处是<span class="math inline">\(\alpha\)</span>和<span class="math inline">\(\beta\)</span>需要人工选择，并且直接相加两个模态的embedding，显得过于粗暴了。</p>
<p><strong>Early Concatenation</strong></p>
<p>不是相加，而是直接拼接两个模态的token序列，组成新的序列输入到Transformer：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230223172712248.png" /></p>
<p>这种做法使得一个模态的token embedding可以直接以其它模态的token embedding作为context进行学习。这种做法也叫做“all-attention”。</p>
<p>坏处是更大的输入序列长度，当然会增加计算复杂度。</p>
<p><strong>Hierarchical Attention (multi-stream to one-stream)</strong></p>
<p>每个模态各自有Transformer，然后拼接到一起输入到一个统一的Transformer：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230223173045776.png" /></p>
<p>这种做法是late fusion的一种；也可以看做是一种特殊的early concatenation（在输入到真正的多模态Transformer之前，先使用单模态Transformer对token embedding进行了编码）。</p>
<p><strong>Hierarchical Attention (one-stream to multi-stream)</strong></p>
<p>和前面的相反，首先使用一个统一的Transformer处理多模态数据，然后每个模态再有自己独立的Transformer：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230223173438330.png" /></p>
<p>这种做法的一个例子是InterBERT。</p>
<p><strong>Cross-Attention</strong></p>
<p>很常见也非常自然的想法，每个模态都有Transformer，但是内部Transformer的query是来自于其它模块：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230223174400978.png" /></p>
<p>这种做法叫做cross attention或者co-attention，是VilBERT方法首次提出（<em>Vilbert: Pretraining taskagnostic visiolinguistic representations for vision-and-language tasks. 2019</em>）。这种做法能够将其它模型的信息引入到当前模态，也没有增加Transformer的输入token序列长度，但是它丢失了全局上下文，也就是不能够像前面的all-attention一样，同时考虑所有模态的token embedding。</p>
<p><strong>Cross-Attention to Concatenation</strong></p>
<p>另外一个变种就是在co-attention之后，使用拼接或者另外的Transformer来继续处理。这样同样可以捕获多个模态的global context。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230227144954372.png" /></p>
<h2 id="transformers-for-multimodal-pretraining">4 Transformers for Multimodal Pretraining</h2>
<h3 id="task-agnostic-multimodal-pretraining">4.1 Task-Agnostic Multimodal Pretraining</h3>
<p>对于任务无关的预训练Transformer模型，存在以下的几个现状/趋势：</p>
<ul>
<li>Vision-language pretraining (VLP)是有最多研究的方向，包括了image+language，video+language。</li>
<li>VLP模型有两种组合方式：two-stage方式，（如LXMERT，VilBERT，VL-BERT等）使用了object detector（如Faster R-CNN）。end-end方式（如Pixel-BERT，SOHO，KD-VLP等）没有使用额外的object detector。</li>
<li>大多数都是以自监督self-supervised的方式进行训练，但是这种训练方法非常依赖于大量提前对齐的多模态数据作为跨模态监督。比如最常用的image-text pairs，instructional videos（比如教做饭的视频，其中的图像和文本更可能是对齐的）。这种数据实际上也不是很好获得，更多现实情况下可能是weakly-aligned或者unaligned的多模态数据。当然目前也出现了一些弱对齐/无对齐的多模态数据进行预训练的工作（<em>Product1m: Towards weakly supervised instance-level product retrieval via cross-modal pretraining ICCV 21</em>，<em>Simvlm: Simple visual language model pretraining with weak supervision 21</em>，<em>Zero-shot text-to-image generation 21</em>）</li>
</ul>
<p>另外一个很重要的点是如何设计pretext task。pretext task起源于CV领域，可以翻译为前置任务/代理任务/预训练任务；它一般是比较泛化的，能够潜在的对一系列下游任务有帮助的辅助任务。通常是某种自监督学习任务，比如masked language modelling (MLM)、masked object classiﬁcation (MOC)、image rotation等等。</p>
<p>单纯的从任务角度讲，这些pretext task可以分为是单模态预测任务和多模态预测任务。但要注意的是，单模态预测任务实际上很可能涉及到利用多模态信息，这和具体模型训练时的信息编码策略有关。</p>
<p>从motivation的角度讲，pretext task可以分为masking、describing、matching和ordering，如：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230301162531606.png" /></p>
<p>尽管目前多模态预训练Transformer方法已经取得了很大的进展，比如VLP模型可以在一系列下游的multimodal discriminative tasks达到很好的效果，但是对于生成任务generative tasks不能直接应用。如文献（<em>Xgpt: Cross-modal generative pretraining for image captioning 21</em>）中指出，VideoBERT和CBT都需要额外训练一个解码器才能够完成video captioning任务。</p>
<p>另外，如何设计合适的pretext task也是个重点。多任务学习和对抗学习都被研究用来提升预训练效果（<em>12-in1: Multi-task vision and language representation learning CVPR 20</em>，<em>Product1m: Towards weakly supervised instance-level product retrieval via cross-modal pretraining. ICCV 21</em>），然而多个pretext任务如何平衡？如何设计pretext，越复杂的就越好吗？</p>
<h3 id="task-speciﬁc-multimodal-pretraining">4.2 Task-Speciﬁc Multimodal Pretraining</h3>
<p>也有很多的研究工作是针对特定领域/任务的预训练，这是因为上述的通用预训练模型有些情况下（如预训练语料领域不重叠/结构不能够充分捕获领域特征/预训练任务设计不合适等）很难直接应用到特定领域。此类特定领域/问题/任务包括：</p>
<ul>
<li>vision and language navigation：需要做sequential decision</li>
<li>generative task：一般的VLP模型无法无缝的适用于生成任务</li>
<li>programming：需要考虑代码结构</li>
<li>health</li>
<li>fashion domain</li>
</ul>
<h3 id="transformers-for-speciﬁc-multimodal-tasks">4.3 Transformers for Speciﬁc Multimodal Tasks</h3>
<p>Multimodal Transformer结构当然也可以直接用于特定的多模态任务，具体不展开。</p>
<h2 id="challenges-and-designs">5 Challenges and Designs</h2>
<h3 id="fusion">5.1 Fusion</h3>
<p>按照阶段，fusion可以分为early fusion（input level）、middle fusion(intermediate representation)、late fusion（prediction）。</p>
<p>一个值得注意的方法是bottleneck fusion（<em>Attention Bottlenecks for Multimodal Fusion</em>）</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230301201634535.png" /></p>
<h3 id="alignment">5.2 Alignment</h3>
<p>真实的数据常常是多个模态数据同时发生，因此天然存在对齐的模态数据。多模态数据的对齐是最常用的想法就是把多模态数据映射到通用空间下，然后通过对比学习等方法进行优化。除了比较粗粒度的image-text match的问题，还有更加细粒度的对齐任务，比如需要图像上region-level的对齐。</p>
<p>很多下游任务都需要对齐能力，比如visual grounding、text-to-speech等。当使用多任务学习进行训练时，可以看做是一种隐式的task-level的对齐。</p>
<h3 id="transferability">5.3 Transferability</h3>
<p>可迁移性是multimodal Transformer的另一个挑战，包括以下几个方面：</p>
<ul>
<li>训练数据与实际/测试数据之间的差距，比如如何把在well-aligned cross-modal pairs训练好的模型迁移到weakly-aligned cross-modal pairs。CLIP是一个很好的工作，它通过prompt template来弥补训练和测试之间的差异。</li>
<li>过拟合是另一个妨碍迁移性的问题。由于Transformer的建模能力比较强，很容易在训练数据上过拟合。</li>
<li>不同任务之间的差异也是需要克服的困难。比如判别任务和生成任务之间的差异，BERT-like的模型不能直接应用在生成任务上。再比如有时候多模态模型需要处理某些模态数据缺失的问题，这种情况下知识蒸馏是一个可能的解决方案。可以用多个单模态Transformer作为teacher，一个多模态Transformer作为student（<em>Towards a uniﬁed foundation model: Jointly pre-training transformers on unpaired images and text 21</em>）。</li>
<li>跨语言的差异。</li>
</ul>
<h3 id="efficiency">5.4 Efficiency</h3>
<p>multimodal Transformer的效率问题主要体现在两个互相影响的方面：</p>
<ol type="1">
<li>需要大量训练样本</li>
<li>随着输入序列长度的增加，训练时间和显存按照平方级增长</li>
</ol>
<p>解决的核心方法是减少训练样本或者减少模型参数，目前有以下几种思路来提高效率：</p>
<ul>
<li>Knowledge distillation. 通过知识蒸馏，从大的Transformer模型获得小的Transformer模型。</li>
<li>Simplifying and compressing model. 移除一些模型的模块，比如在VLP模型中移除object detector；权重共享，multimodal Transformer中的部分模型参数可以共享。</li>
<li>Asymmetrical network structures. 不同的模态给定不同大小的模型部分。</li>
<li>Improving utilization of training samples. 充分挖掘训练样本的潜在信息。</li>
<li>Compressing and pruning model. 选择multimodal Transformer的最优子结构。</li>
<li>Optimizing the complexity of self-attention. 直接优化Transformer的self-attention，比如稀疏注意力。</li>
<li>Optimizing the complexity of self-attention based multimodal interaction/fusion. 优化多模态交互带来的计算成本，比如bottleneck fusion方法。</li>
<li>Optimizing other strategies. 其它策略，比如有研究者（<em>Multiview transformers for video recognition 22</em>）提出可以逐步的融合多模态tokens，而不是直接融合所有多模态token。</li>
</ul>
<h3 id="universalness">5.5 Universalness</h3>
<p>通用性是当前很多模型主要考虑的问题之一，出现了以下几种体现通用性的思路：</p>
<ul>
<li>Unifying the pipelines for both uni-modal and multimodal inputs/tasks. 单模态场景和多模态场景通用，比如上面提到的使用知识蒸馏来增加迁移性。</li>
<li>Unifying the pipelines for both multimodal understanding and generation. 判别任务和生成任务通用。</li>
<li>Unifying and converting the tasks themselves. 模型不变，通过改动任务设置让模型在多个任务上通用，比如CLIP。</li>
</ul>
<h3 id="interpretability">5.4 Interpretability</h3>
<p>可解释性。研究者尝试设计一些探测任务来评估预训练过程中模型到底学习到了什么（<em>Behind the scene: Revealing the secrets of pre-trained vision-and-language models ECCV 20</em>，<em>Probing image language transformers for verb understanding 21</em>）。</p>
<h2 id="discussion-and-outlook">6 Discussion and outlook</h2>
<p>作者提出了几个开放问题：</p>
<ul>
<li>设计更加通用的多模态架构，不仅仅是在多模态任务上，也要在各个单模态任务上取得最好的效果。常常发现尽管使用了更多的多模态数据，多模态模型在单模态任务上的表现不如单模态模型。为了解决这个问题，可能探究和理解multimodal Transformer背后的原理和机制是比不断尝试新的网络架构更有价值的问题。</li>
<li>发现跨模态数据之间的隐式对齐。</li>
<li>multimodal Transformer的高效学习问题还没有被充分探究，尽管efficient Transformer的各种变体已经出现了很多研究工作。</li>
</ul>
]]></content>
      <categories>
        <category>Paper</category>
        <category>Multimodal</category>
      </categories>
      <tags>
        <tag>Transformer</tag>
        <tag>Multimodal</tag>
        <tag>Survey</tag>
      </tags>
  </entry>
  <entry>
    <title>MMKG-survey-fudan</title>
    <url>/mmml/MMKG-survey-fudan/</url>
    <content><![CDATA[<h1 id="multi-modal-knowledge-graph-construction-and-application-a-survey">Multi-Modal Knowledge Graph Construction and Application: A Survey</h1>
<p>复旦大学计算机系在2022年出的关于MMKG的综述，主要针对图片和语言组成的MMKG，从construction和application两个方面进行描述。</p>
<blockquote>
<p>Recent years have witnessed the resurgence of knowledge engineering which is featured by the fast growth of knowledge graphs. However, most of existing knowledge graphs are represented with pure symbols, which hurts the machine’s capability to understand the real world. The multi-modalization of knowledge graphs is an inevitable key step towards the realization of human-level machine intelligence. The results of this endeavor are Multi-modal Knowledge Graphs (MMKGs). In this survey on MMKGs constructed by texts and images, we ﬁrst give deﬁnitions of MMKGs, followed with the preliminaries on multi-modal tasks and techniques. We then systematically review the challenges, progresses and opportunities on the construction and application of MMKGs respectively, with detailed analyses of the strength and weakness of different solutions. We finalize this survey with open research problems relevant to MMKGs.</p>
</blockquote>
<span id="more"></span>
<h2 id="introduction">1 Introduction</h2>
<p>首先我们已经拥有了很多不同领域的知识图谱：</p>
<ul>
<li>常识知识图谱common sense knowledge：Cyc, ConceptNet</li>
<li>语言知识图谱lexical knowledge：WordNet, BabelNet</li>
<li>百科式知识图谱encyclopedia knowledge：Freebase, DBpedia, YAGO , WikiData, CN-Dbpedia</li>
<li>分类学知识图谱taxonomic knowledge：Probase</li>
<li>地理知识图谱geographic knowledge：GeoNames</li>
</ul>
<p>但是大多数这些KG仅仅是用纯符号pure symbols表示的，这实际上很大程度限制了机器描述和理解现实世界的能力。单纯的符号表达的信息不够充分。比如dog这个词，我们知道现实的狗拥有远比dog这个单词表达含义丰富的信息。因此我们需要把symbolic链接到non-symbolic experiences。</p>
<blockquote>
<p>it is necessary to ground symbols to corresponding images, sound and video data and map symbols to their corresponding referents with meanings in the physical world, enabling machines to generate similar “experiences” like a real human [12].</p>
</blockquote>
<p>从另外的方面讲，很多的应用比如关系抽取、视觉问答等任务都需要知识图谱拥有更多模态信息去进行推理。</p>
<h2 id="definitions-and-preliminaries">2 Definitions and Preliminaries</h2>
<p>在这个survey中，作者把KG的属性attribute和关系relation做了区分，虽然都是三元组形式。作者使用谓词predicate同时描述属性和关系。</p>
<p>作者把现存的主要的MMKG分为了两类：A-MMKG和N-MMKG（没搞懂为什么简称N）</p>
<ol type="1">
<li><p>A-MMKG：<em>multimodal data (images in this survey) as particular attribute values of entities or concepts.</em> 指image仅仅是作为实体的一个属性存在，image之间不存在更多的语义联系。举例如下图：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220905170655742.png"  style="zoom:40%;" /></p></li>
<li><p>N-MMKG：<em>multi-modal data as entities in KGs.</em> image作为新的实体，因此image之间存在互相的语义联系。举例如下：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220905171029083.png"   style="zoom:40%;" /></p></li>
</ol>
<p>对于N-MMKG来说，不同image之间可以存在关系：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220905180159624.png"   style="zoom:40%;" /></p>
<p>除此之外，在N-MMKG当中对于image还常常会保存它的一些简要的image descriptors，比如Gray Histogram Descriptor (GHD)、Histogram of Oriented Gradients Descriptor (HOG)、Color Layout Descriptor (CLD)等。下面是A-MMKG和N-MMKG中对于image是如何表示的示例：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220905180421754.png"   style="zoom:40%;" /></p>
<p>目前在各个multimodal tasks领域内都已经有很大的进展，但是引入MMKG可以进一步带来更多的好处：</p>
<ul>
<li>MMKG provides sufficient background knowledge to enrich the representation of entities and concepts, especially for the long-tail ones. (<em>Knowledge aware semantic concept expansion for image-text matching. IJCAI 2019</em>)</li>
<li>MMKG enables the understanding of unseen objects in images. (<em>Describing natural images containing novel objects with knowledge guided assitance. arXiv 2017</em>)</li>
<li>MMKG enables multi-modal reasoning. (<em>Okvqa: A visual question answering benchmark requiring external knowledge. IEEE Conference on Computer Vision and Pattern Recognition 2019</em>)</li>
<li>MMKG usually provides multi-modal data as additional features to bridge the information gaps in some NLP tasks. (<em>Adaptive co-attention network for named entity recognition in tweets. AAAI 2018</em>)</li>
</ul>
<p>下面的图是目前几个主流的MMKG：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220905185128319.png"   style="zoom:40%;" /></p>
<h2 id="construction">3 Construction</h2>
<p>构造MMKG的关键是能够把原始KG中的symbolic knowledge关联到对应的image。有两种不同方向的方法，一个是从image出发，把image关联到symbols上；一个是从symbols出发，关联到相应的image。</p>
<h3 id="from-images-to-symbols-labeling-images">3.1 From Images to Symbols: Labeling Images</h3>
<p>在CV界已经出现了很多的image labeling方法，正好使用与把images和KG中的symbols关联起来。下面是几个有名的image-based的visual knowledge extraction系统：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220905193241031.png"   style="zoom:40%;" /></p>
<p>From Images to Symbols，把图片和符号对应起来，隐含的假设是我已经拥有了一大堆相关图片，并不需要额外去寻找。这个或许适用于某个特定的项目里，我拥有了项目相关的很多图片，这些图片的领域性非常强，在外界很难找得到，也不需要继续寻找了。那么接下来我要做的就是把图片对应到文字上。</p>
<p>根据导出的knowledge的不同，可以分为visual entity/concept extraction、visual relation extraction和visual event extraction。下面先介绍视觉实体导出。</p>
<h4 id="visual-entityconcept-extraction">3.1.1 Visual Entity/Concept Extraction</h4>
<blockquote>
<p>Visual entity (or concept) extraction aims to detect and locate target visual objects in images, and then label these objects with entity (or concept) symbols in KG.</p>
</blockquote>
<p>先识别image上的不同object，然后再关联到KG的实体上。</p>
<p><strong>Challenges.</strong> 这一task面临的最大挑战是有合适的标注数据集。一般的CV数据集对于KG来说粒度太粗，entity表现在image上可能是更小的区域/单元。</p>
<p>这一方向的方法主要可分为两类：object recognition方法和visual grounding方法。前者是通过识别image上的object；后者是通过把caption中的phrase对应到image上最相关的区域。</p>
<p><strong>Object Recognition Methods</strong></p>
<p>基于object recognition的方法首先通过提前训练好多个不同的目标探测器，让这些目标探测器到图像上分别探测不同的objects；之后由于可能对于同一实体，会产生多个不同的探测出来的objects（比如由于位置、姿势等不同很多探测出来的objects实际上指的是同一个entity）。因此通常又会使用聚类的办法找出最具代表性的object作为visual entity。</p>
<p>基于object recognition的方法是有监督的方法，它需要大量具有bounding box的image数据，提前训练好的多个探测器以及提前定义好的可以获取的实体/概念。因此，它实际上很难应付具有大量实体的KG，比如拥有数十亿实体的KG。（试想一下对于每个实体都需要训练一个探测器并且准备好数据集）</p>
<p><strong>Visual Grounding Methods</strong></p>
<p>接下来，更加实际的采取visual grounding的方法，我们需要一种方法能够同时识别出大量的、不同的objects。首先，从Web（比如新闻网站）上可以较为容易的获得大量的image-caption pairs，接下来我们要做的就是把caption中的phrase在image上的对应region找出来。这就是一个弱监督学习问题。</p>
<p>在实现的时候，我们通常会直接找找出在图像中和word对应的pixels：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220905195939709.png"   style="zoom:40%;" /></p>
<p>当前有基于注意力的方法（<em>Gaia: A ﬁne-grained multimedia knowledge extraction system. ACL 2020</em>）和基于重要性的方法（<em>Cross-media structured common space for multimedia event extraction. ArXiv 2020</em>）：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220905200212691.png"   style="zoom:40%;" /></p>
<p>尽管visual grounding方法不再需要人工画bounding boxes，但是由于可能存在错误匹配的情况，所以仍然需要辅助人工验证。比如visual grounding出来的visual entities可能在语义层次上是不一致的，例如troops可能会匹配到一个有几个人穿着军队服装的images上；Ukraine (country)可能会匹配到一个乌克兰国旗的图片上。尽管它们都是有关联的，但是并不等价，我们不能直接进行把图片和实体关联到一起。</p>
<p><strong>Opportunities.</strong> 另外，作者提出基于预训练的language model已经表现出了很强的visual object segmentation能力,利用这样的模型或许可以很大程度地减轻标注visual objects的工作量，从而辅助构造MMKG。下面是ViLT的实例：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220905200911530.png"   style="zoom:40%;" /></p>
<h4 id="visual-relation-extraction">3.1.2 Visual Relation Extraction</h4>
<p>视觉关系抽取的定义：</p>
<blockquote>
<p>Visual relation extraction aims at identifying semantic relations among detected visual entities (or concepts) in images, and then labeling them with the relations in KGs. (<em>Neil: Extracting visual knowledge from web data. ICCV 2016</em>)</p>
</blockquote>
<p><strong>Challenges.</strong> 在CV界对于探测到的objects的关系探测已经有了很多的研究。但是原来CV中对一个图片上的objects关系的推测都是很简单的，语义层级较弱，不能用在KG里。比如CV方法预测的是<em>(person, standing on, beach)</em>，构建MMKG需要更general的semantic relations。</p>
<p>现有的方法主流是两种：rule-based relation extraction和statistic-based relation extraction。除此之外，也有一些研究关注long-tail relation和fine-grained relation。</p>
<p><strong>Rule-based Relation Extraction</strong></p>
<p>基于规则的关系导出方法需要人工定义好标准。比如通过识别出来的objects的label和它们的bounding box之前的相对位置，可以推测它们之间的可能的relation。比如在NEIL方法中探测出来的关系类型（<em>Neil: Extracting visual knowledge from web data. ICCV 2016</em>）：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220906103359640.png"   style="zoom:33%;" /></p>
<p>比如一个探测到的wheel对象的bounding box总是包含在car对象的bounding box里，结合wheel属于entity，car属于concept，依据定义好的rules，我们推测wheel和car的relation是<span class="math inline">\(kind\ of\)</span>。</p>
<p>这种做法准确度比较高，但是需要大量人工，不适应大规模MMKG。</p>
<p><strong>Statistic-based General Relation Extraction</strong></p>
<p>基于统计的关系导出方法就是通过编码特征，然后通过分类模型预测relation。比起基于规则的关系导出，这种做法更”自动“，也更”黑“。</p>
<p>一些研究发现，预测关系是非常依赖于头尾实体的类型的；但是实体并不依赖于关系的类型（<em>Neural motifs: Scene graph parsing with global context. CVPR 2018</em>）。那么就有研究尝试把language model的先验知识加到objects的label中，从而提高关系预测性能（<em>Visual relationship detection with language priors. ECCV 2016.</em>, <em>Visual translation embedding network for visual relation detection. CVPR 2017</em>）。</p>
<p>另外，也有研究工作尝试建模探测到的objects之间的graph structure信息（<em>Scene graph generation by iterative message passing. CVPR 2017</em>，<em>Graph r-cnn for scene graph generation. ECCV 2018</em>）。</p>
<p><strong>Long-tail and Fine-grained Relation Extraction</strong></p>
<p><strong>Long-tail relations</strong>：关系导出中，由于不同relation对应的sample数量是非常悬殊的，因此可能导致模型倾向于预测有更多sample数量的relation。为了解决这一问题，[Learning to compose dynamic tree structures for visual contexts. CVPR 2019]提出了一个新的评估指标，先计算出每个relation的指标，然后再平均，这样long-tail relation能预测效果就能够在指标上得到体现。还有基于对比学习（<em>Large-scale visual relationship understanding. AAAI 2019</em>）、少次学习（<em>One-shot learning for long-tail visual relation detection。 AAAI 2020</em>）和迁移学习的方法（<em>Memory-based network for scene graph with unbalanced relations. ACM-MM 2020</em>）。</p>
<p><strong>Fine-grained Relation</strong>：fine-grained relation是long-tail relations的一种。研究者认为很多的预测long-tail relation的方法实际上不能更进一步分辨fine-grained relation。比如关系<span class="math inline">\(on\)</span>可以被长尾分布方法预测出来，但是关系<span class="math inline">\(on/sit,\ on/walk,\ on/lay\)</span>就没法区分出来了（<em>Unbiased scene graph generation from biased training. CVPR 2020</em>）。</p>
<p><strong>Opportunities.</strong> 作者提出了这一方向存在的两个挑战。（1）Visual Knowledge Relation Judgement. 第一个问题是识别出来的visual triples不能够直接作为visual knowledge使用，特别是很多的visual triples仅仅是对于images场景的描述。（2）Relation Detection based on Reasoning. 第二个问题是对于关系的预测缺乏推理过程，我们需要能够自动总结这样的推理链。</p>
<h4 id="visual-event-extraction">3.1.3 Visual Event Extraction</h4>
<p>在一般的，基于文本的事件图谱构建过程中，对于事件（event）的探测，核心有两步，一是通过发现关键的触发词（trigger）判断事件的类型；二是判断在这个时间中的关键元素（argument）并判断它们的角色（argument role），比如时间/地点/任务。</p>
<div class="note info"><p>事件图谱和事理图谱有所区别，经过调研，事理图谱是哈工大社会计算与信息检索研究中心提出的，可参考<a href="https://www.jiqizhixin.com/articles/2018-12-29-23">[事理图谱，下一代知识图谱]</a>进一步进行了解。事件图谱更多是事理图谱的一个初级阶段，不包含本体。</p>
</div>
<p><strong>Challenges.</strong> 作者认为这一领域的挑战包括：（1）通常事件的抽取需要提前定义好不同事件类型的schema，不适用大规模事件抽取，如何能够从visual patterns中自动挖掘出事件schema？（2）如何从image/videos中抽取出visual事件元素？</p>
<p>当前visual event extraction工作集中在两方面：visual event schema mining和visual event arguments extraction。</p>
<p><strong>Visual Event Schema Mining</strong></p>
<p>在CV领域有个很相似的任务叫做场景识别（situation recognition task），这个任务会识别一个图片表示的主要事件以及相应的元素，存在几个标注好的数据集，如SituNet（<em>Situation recognition: Visual semantic role labeling for image understanding. CVPR 2016</em>）和SWiG（<em>Grounded situation recognition. ECCV 2020</em>）。</p>
<p>同样的，如果要导出大量的事件，不能够期望人工标注。借助于大量的image-caption pairs，可以辅助进行事件抽取（<em>Improving event extraction via multimodal integration. ACM-MM 2017</em>）。</p>
<p><strong>Visual Event Arguments Extraction</strong></p>
<p>事件元素的抽取就是对相关事件的images，识别不同argument role对应的visual objects。这里需要注意的是，我们需要确保visual arguments之间的关系和text arguments之间的关系是一致的。因此还需要对齐。（<em>Cross-media structured common space for multimedia event extraction. arXiv 2020</em>）</p>
<p>作者提到，实际上对于visual事件的抽取，更加合理且前景广阔的场景是从video中抽取。一个video可能包含了多个不同的事件，单一帧上的事件元素会关联到不同时间帧上的事件元素，因此是一个更加复杂的task。</p>
<p>visual event extraction还是一个在初期阶段的领域。</p>
<h3 id="from-symbols-to-images-symbol-grounding">3.2 From Symbols to Images: Symbol Grounding</h3>
<p>Symbol grounding符号定位指的是把传统KG中的元素定位到multimodal data。</p>
<blockquote>
<p>Symbol grounding refers to the process of finding proper multi-modal data items such as images to denote a symbol knowledge exists in a traditional KG.</p>
</blockquote>
<p>和image到symbol不同，这里我需要自己找对应的图片，因此是symbol到image。</p>
<p>symbol grounding方法是目前主流的MMKG构造方法，大多数见到的MMKG都是使用这种方式构建的。</p>
<h4 id="entity-grounding">3.2.1 Entity Grounding</h4>
<blockquote>
<p>Entity grounding aims to ground entities in KGs to their corresponding multi-modal data such as images, videos and audios. (<em>The symbol grounding problem. 1990</em>)</p>
</blockquote>
<p><strong>Challenges.</strong> 两个关键问题：（1）如何找到足够数量且高质量的图片？（2）如何选择最符合实体的图片？</p>
<p>两种主要的方法：（1） from online encyclopedia (such as Wikipedia)；（2）from the Internet through Web search engines.</p>
<p><strong>From Online Encyclopedia</strong></p>
<p>网上的百科，通常是一篇article描述一个entity。把article里的图片都拿过来，就变成了entity的对应图片了。并且这样的图片通常和实体相关性很高，难度也比较低。下面以维基百科为例说明这种方法可能存在的问题。</p>
<p>但是基于维基百科搭建MMKG存在3个问题：1. 每个实体对应的图像太少，比如在维基百科中，一个实体的平均图片仅有1.16；2. 维基百科中出现的图片和期望的实体仅仅是相关（relevant），而不是确切的指向该实体（exactly refered to），比如北京动物园文章里的图片包括了动物、建筑等，但都不是直接指向北京动物园；3. 使用维基百科搭建的MMKG的覆盖率有限，比如80%的维基百科文章没有图片，只有超过8.6%的文章有超过2张图片，并且维基百科涉及的实体数量也不一定覆盖所有的实体。</p>
<p><strong>From Search Engines</strong></p>
<p>为了提高MMKG的覆盖率，另一种简单有效的方法是直接用搜索引擎寻找相应的图片，然后以排在前面的照片作为候选。这种做法无疑很大程度提高了覆盖率，但是它的图片相关性不一定足够高，图片的噪音可能也比较大，还可能出现很多个非常相似的图片。</p>
<p>比如我们希望寻找实体Bank相关的图片，如果直接用Bank去搜，可能会找到River Bank相关的照片，但是我们实际是希望找到Commercial Bank。很多的方法研究如何清洗候选的图片，比如通过拓展查询语句（<em>Imagenet: A large-scale hierarchical image database. CVPR 2009</em>）。另一方面，我们期望search到的图片，除了能够符合目标实体外，还能够能够充分反应实体不同方面的（diversity），具有较好的多样性（<em>Richpedia: a large-scale, comprehensive multi-modal knowledge graph. Big Data Research 2020</em>）。</p>
<p>事实上，上面的两种方法通常会一起使用之后作为搭建MMKG的主要手段（<em>Richpedia: a large-scale, comprehensive multi-modal knowledge graph. Big Data Research 2020</em>）。</p>
<p><strong>Opportunities.</strong> 作者提出两个在entity grounding方向的挑战：（1）实体会对应到不同的图片，哪些图片是最具有代表性的？（2）作者提出一个很有趣也很难的新task，叫做<strong>multiple grounding</strong>。作者认为事实上每个实体在实际中有不同的方面，不同的图片应该对应到这些不同的方面中去。这个想法实际上是把hasImage这个关系，进一步细分到不同的fine-grained relations上。比如下面的例子：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220906165126632.png"  style="zoom:33%;" /></p>
<p>想法很好，但是从哪里找这么多质量高的图片呢？</p>
<h4 id="concept-grounding">3.2.2 Concept Grounding</h4>
<blockquote>
<p>Concept grounding aims to find representative, discriminative and diverse images for visual concepts.</p>
</blockquote>
<p><strong>Challenges.</strong> 虽然concept grounding和entity grounding有很多的相似之处，但是面临新的挑战。我们可以把概念分为可以可视化的concept和不可可视化concept两种。对于visualizable concepts存在的问题是，它可能对应了很多不同的图片，怎么样选择期望的图片，比如Princess这个概念，它可以联系到迪士尼公主、古代公主等等，怎么样选择合理的图片？对于non-visualizable concepts来说，我们较难直接定位到符合的图片，比如irreligionist无宗教信仰者就很难直接可视化。</p>
<p>这一方向的主要工作包括：</p>
<p><strong>Visualization Concept Judgment</strong></p>
<blockquote>
<p>The task aims to automatically judge visualizable concepts and is a new task to be solved.</p>
</blockquote>
<p>判断概念是否能够被可视化的新task。[Towards fairer datasets: Filtering and balancing the distribution of the people subtree in the imagenet hierarchy.] 研究发现只有12.8%的Person相关概念可以被较好的可视化。比如Rock star容易可视化，Job candidate就不太容易可视化。</p>
<p>为了自动判断一个概念是否能够被可视化，[80 million tiny images: A large data set for nonparametric object and scene of singapore. ] 就移除了所有的抽象概念，仅仅保留非抽象概念。但也不是所有的抽象词都无法可视化，比如Anger这个概念是可以定位到人发火的图片的。另外的方法是使用谷歌搜索引擎，如果搜索出来的图片结果比web结果多，那这个概念就可能是可以可视化的（<em>Graph-based clustering and ranking for diversified image search.</em>）。</p>
<p><strong>Representative Image Selection</strong></p>
<p>这个任务主要是选择合适的图片。</p>
<blockquote>
<p>The task essentially aims to re-rank the images according to their representativeness.</p>
</blockquote>
<p>首先我们希望选择的图片足够有代表性，主流方法采用聚类算法，然后认为一个image在它所位于的cluster中越靠近中心，这个image在这个cluster中就越具有代表性（<em>Representative image selection from image dataset.</em>）。</p>
<p>还有的方法把image的caption和tag也加入到判断过程中来（<em>A joint optimization model for image summarization based on image content and tags. AAAI 2014</em>）。</p>
<p><strong>Image Diversiﬁcation</strong></p>
<p>我们还希望选择的图片有足够的多样性，</p>
<blockquote>
<p>The task requires the images which concepts are grounded to should balance diverse and relevant.</p>
</blockquote>
<p>尽可能从不同类的image clusters中选择，而不是反复从一个cluster中选择图片（<em>Towards a relevant and diverse search of social images. IEEE Trans. MM 2010</em>）。</p>
<p>这一方向的方法大多只是关注text-image检索，但是搜集到的图片中可能同样蕴含着偏见bias，比如对gender、age、race的偏见。</p>
<p><strong>Opportunities.</strong> concept grounding还是属于新生的领域，还有很多问题需要解决。</p>
<ul>
<li><p>Abstract Concept Grounding：抽象概念很难可视化，但是很多概念还是可以关联/定位到某些场景的。比如提到<span class="math inline">\(Love\)</span>这个概念，事实上它可能关联到baby/cute/newborn, dog/pet, heart/red/valentine, beach/sea/couple, sky/cloud/sunset, flower/rose等单词（<em>Computing iconic summaries of general visual concepts. 2008</em>）。</p></li>
<li><p>Gerunds Concept Grounding：动名词的可视化很大程度依赖于对人的姿态的判断（<em>Zero-shot learning via visual abstraction. ECCV 2014</em>）。</p></li>
<li><p>Non-visualizable Concept Grounding via Entity Grounding：一个概念可能很难可视化，但是可以通过它的对应实体，来寻找符合的图片。比如拿爱因斯坦实体的照片作为物理学家概念的图片：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220906183203563.png"   style="zoom:40%;" /></p>
<p>但是这种方法很难决定什么是typical的图片，比如为什么我们会使用爱因斯坦而不是其他物理学家的图片。</p></li>
</ul>
<h4 id="relation-grounding">3.2.3 Relation Grounding</h4>
<p>关系定位主要指寻找能够表达特定relation的image。</p>
<blockquote>
<p>Relation grounding is to find images from an image data corpus or the Internet that could represent a particular relation.</p>
</blockquote>
<p>输入可以是具有这个relation的多个triples，输出就是预期的图片。</p>
<p><strong>Challenges.</strong> 当使用三元组进行查询的时候，找到的图片常常会过于和头尾实体相关，而不是和关系本身相关。</p>
<p>目前主要的研究都是针对spatial relation或者action relation。那这样看来，在KG更常见的更多的语义relation很难找到符合的图片。</p>
<p><strong>Text-Image Matching</strong></p>
<p>把text和image都表示为同一表达空间下的向量，然后计算相似性。[Visual relations augmented cross-modal retrieval. 2020]方法使用GNN来学习visual relation。也有方法使用预训练方法[<em>Unimo: Towards uniﬁed-modal understanding and generation via cross-modal contrastive learning. arXiv 2020</em>]。</p>
<p><strong>Graph Matching</strong></p>
<p>接下来有的研究方法尝试能够更加显式地进行relation和objects的匹配。比如[<em>Cross-modal scene graph matching for relationship-aware image-text retrieval. 2020</em>]方法就是通过描述的语法结构和图片上对象的依赖关系结构进行graph match（<em>Crossmodal scene graph matching for relationship-aware image-text retrieval. IEEE/CVF 2020</em>）：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220906193254305.png"   style="zoom:40%;" /></p>
<p><strong>Opportunities.</strong> 作者提出能否把除了空间关系和行为关系以外的关系也进行可视化？个人认为很难，因为首先人工都很难标注。</p>
<h2 id="application">4 Application</h2>
<h3 id="in-mmkg-applications">4.1 In-MMKG Applications</h3>
<p>知识图谱本身就有knowledge，我们当然可以直接基于符号进行推理。但是难用，也不好与主流的DL方法结合。</p>
<p>在编码MMKG的图像信息时，通常还是直接使用visual encoder的编码器的hidden state，很少利用其它visual features比如Gray Histogram Descriptor (GHD)，Histogram of Oriented Gradients Descriptor (HOG)，Color Layout Descriptor (CLD)等。</p>
<p>下面是MMKG的一些主要应用和benchmark datasets：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220906211153963.png"   style="zoom:40%;" /></p>
<h4 id="link-prediction">4.1.1 Link Prediction</h4>
<p>预测头尾实体或者预测关系。</p>
<p>来想一个关键问题，Visual信息对预测实体有帮助吗？ 比如设想某个relation对应的尾实体通常具备某种共通的视觉特征，头实体也具备某种共通的视觉特征，这种视觉特征可以辅助预测，并且无法被单纯的文本描述。比如spouse两边表现出来的应该都是人形，而不是动物。</p>
<p>再比如，Person的图像可以提供信息，用来判断age等profession（<em>Embedding multimodal relational data for knowledge base completion</em>）。</p>
<h4 id="triple-classiﬁcation">4.1.2 Triple Classiﬁcation</h4>
<p>直接判断一个给定的三元组是不是成立的，可以看做是KG补全的一种形式。具体的工作参见论文。</p>
<h4 id="entity-classiﬁcation">4.1.3 Entity Classiﬁcation</h4>
<p>判断实体属于哪一类？同样可以看做是一种特殊的链路预测，预测关系<span class="math inline">\(IsA\)</span>，候选项是MMKG中的concepts。</p>
<h4 id="entity-alignment">4.1.4 Entity Alignment</h4>
<p>MMKG实体对齐。具体相关工作看论文。这里提一个有意思的想法，[<em>Mmkg: Multi-modal knowledge graphs. ESWC 2019</em>]把MMKG对齐也看做了是一个链路预测任务<span class="math inline">\(&lt;h?,sameAs,t&gt;\)</span>或者<span class="math inline">\(&lt;h,sameAs,t?&gt;\)</span>只不过是这里的头尾实体不在同一个MMKG中。</p>
<h3 id="out-of-mmkg-applications">4.2 Out-of-MMKG Applications</h3>
<p>在MMKG以外的应用当然有很多，包括：</p>
<ul>
<li>Multi-modal Entity Recognition and Linking：给定一段文本和相应的图片，识别出描绘的实体（<em>Adaptive co-attention network for named entity recognition in tweets. AAAI 2018</em>）</li>
<li>Visual Question Answering：针对图片提问。由于需要对于detected objects实现复杂的推理过程，因此引入MMKG进行辅助推理（<em>Okvqa: A visual question answering benchmark requiring external knowledge. CVPR 2019</em>）。</li>
<li>Image-Text Matching：是一个非常关键的基础的task，判断image-text pair的相似程度。引入MMKG来获得更多的信息，对image和text有更多的理解，从而提升预测效果（<em>Knowledge aware semantic concept expansion for image-text matching. IJCAI 2019</em>）。</li>
<li>Multi-modal Generation Tasks
<ul>
<li>Image Tagging：引入增强对图像的理解（<em>Enhancing the quality of image tagging using a visio-textual knowledge base. IEEE Trans. MM 2019</em>）</li>
<li>Image Captioning：引入MMKG生成更加准确和合理的caption（<em>Relational reasoning using prior knowledge for visual captioning. arXiv 2019</em>）</li>
<li>Visual Storytelling：给定连续的图片序列，然后讲故事。引入MMKG来更好的寻找不同图片中的对象的关系，并且能够获得额外的训练集意外的知识（<em>“Knowledge-enriched visual storytelling. AAAI 2020</em>）</li>
</ul></li>
<li>Multi-modal Recommender System：买东西的时候我们当然会很关注视觉信息，并且视觉信息很难用文本充分描述出来，引入MMKG提升推荐性能（<em>Multi-modal knowledgeaware reinforcement learning network for explainable recommendation. Knowledge-Based Systems 2021</em>）</li>
</ul>
<h2 id="open-problems">5 Open Problems</h2>
<h3 id="complex-symbolic-knowledge-grounding">5.1 Complex Symbolic Knowledge Grounding</h3>
<p>在前面介绍了entity、concept和relation的grounding方法。一方面这些方向都有自身的挑战，比如abstract concept找不到；relation也不好找。</p>
<p>另一方面，如果我们拓展下思维，大多数的MMKG仅仅是实体找到了对应图片，但并不是KG中所有的knowledge都找到了图片。比如一个path或者subgraph也找不到符合的图片，而这样的信息往往在某些任务中是可用的。比如对于实体Trump，他的妻子的照片，儿子的照片和女儿的照片都可以结合到一起成为一个subgraph，这样的subgraph可以对应到一个他们全家福的照片上。</p>
<p>作者把这样的task叫做<em>multiple relational grounding</em>，这无疑是一个很难的任务。</p>
<h3 id="quality-control">5.2 Quality Control</h3>
<p>MMKG本身也存在质量问题，比如errors、missing facts和outdated facts。举例，对于long-tail的entity来说，它就可能会对应到不相符的照片上，因为可能在Web上就没有符合的图片。</p>
<p>MMKG除了一般KG可能存在的问题外，还由于引入图像带来了新的问题：</p>
<ol type="1">
<li><p>一个实体的图片可能会和另外的很相关的实体图片混合在一起。比如下图：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220907103645534.png"   style="zoom:50%;" /></p>
<p>鳄鱼鸟的照片就经常出现鳄鱼，如果鳄鱼本身也是一个实体的话，这样就可能出现了信息的混合。</p></li>
<li><p>越是出名的照片越有可能出现。比如上图中刘慈欣的《漫步地球》在搜索引擎上找到的照片总是另一本书《黑暗森林》的照片。</p></li>
<li><p>一些抽闲的概念很可能对应到错误的照片上。比如上图中的发怒arrogance实体对应到了错误的图片。</p></li>
</ol>
<p>为了解决上述问题，可能需要对视觉信息进行更多分析，结合背景信息综合判断。</p>
<h3 id="efficiency">5.3 Efficiency</h3>
<p>在构建MMKG时，因为要处理多模态的数据，往往需要大量的时间。比如NEIL（<em>Neil: Extracting visual knowledge from web data. ICCV 2013</em>）就花了350K的CPU hours处理了2273个objects的400K图像实例，这当然没法处理可能拥有上亿乃至数十亿实体的大规模KG。</p>
<p>另一个是对于在线实时的MMKG应用算法，大多数的算法无法适应实时场景。</p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
        <tag>survey</tag>
      </tags>
  </entry>
  <entry>
    <title>MNRE-dataset</title>
    <url>/mmml/MNRE-dataset/</url>
    <content><![CDATA[<h2 id="mnre-a-challenge-multimodal-dataset-for-neural-relation-extraction-with-visual-evidence-in-social-media-posts">MNRE: A Challenge Multimodal Dataset for Neural Relation Extraction with Visual Evidence in Social Media Posts</h2>
<p>MNRE，ICME 2021。作者创建了首个用于multimodal relation extraction的数据集MNRE，<a href="https://github.com/thecharm/MNRE">地址</a>。</p>
<p>数据来源于Twitter posts，关注点是文本中的上下文信息不够充分时，通过post中的image，来补充上下文信息。</p>
<blockquote>
<p>Extracting relations in social media posts is challenging when sentences lack of contexts. However, images related to these sentences can supplement such missing contexts and help to identify relations precisely. To this end, we present a multimodal neural relation extraction dataset (MNRE), consisting of 10000+ sentences on 31 relations derived from Twitter and annotated by crowdworkers. The subject and object entities are recognized by a pretrained NER tool and then ﬁltered by crowdworkers. All the relations are identiﬁed manually. One sentence is tagged with one related image. We develop a multimodal relation extraction baseline model and the experimental results show that introducing multimodal information improves relation extraction performance in social media texts. Still, our detailed analysis points out the difﬁculties of aligning relations in texts and images, which can be addressed for future research. All details and resources about the dataset and baselines are released on https://github.com/thecharm/MNRE.</p>
</blockquote>
<span id="more"></span>
<h3 id="introduction">1. Introduction</h3>
<p>relation extraction（RE）是预测一个句子中两个命名实体之间的关系relation。</p>
<p><strong>challenges</strong>:之前大多数的RE模型关注的是文本信息很充分的场景下的关系抽取，比如newswire domain。但是，一旦文本很短，并且缺少必要的上下文信息的时候，RE模型效果会出现严重的下降。即便是使用了pretrained modal来进行关系抽取，效果也很糟糕。</p>
<p><strong>solution</strong>: 作者认为，对于在推特post这样很可能文本中缺乏足够充分的上下文信息的场景，可以使用image的visual information来补充上下文信息。</p>
<p>比如在下面的图中，如果只有文本，那么可能会判断出来JFK和Obama和Harvard的关系是residence；但是如果能够识别图像中的信息，比如校园、学位帽等，可以判断出来JFK和Obama和Harvard的关系应该是graduated_at。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221012204338075.png"   style="zoom:50%;" /></p>
<p>但是目前并没有这样满足文本+图像的数据集存在，因此作者就希望能够解决这一点，主要贡献如下：</p>
<ul>
<li>创建了在社交媒体posts上的数据集Multimodal dataset for Neural Relation Extraction（MNRE）</li>
<li>在MNRE数据集基础上，构建了几个不同的baseline方法</li>
</ul>
<h3 id="mnre-dataset">2. MNRE Dataset</h3>
<h4 id="dataset-collection">2.1 Dataset Collection</h4>
<p>数据来源有三个：</p>
<ul>
<li>Twitter 2015：有8357个候选实例（指一个完整的post和对应image、named entities和relations）</li>
<li>Twitter 2017：有4819个候选实例</li>
<li>Crawled Twitter data：爬取了Twitter 2019年1月到2月的post和对应图片，不限制具体的领域；如果一个post有多张图片，就随机选择一张。最终获取了20000候选实例</li>
</ul>
<h4 id="twitter-name-tagging">2.2 Twitter Name Tagging</h4>
<p>使用预训练的<a href="https://allennlp.org/elmo">NER tagging tool</a>在爬取的Twitter data上标注实体和对应的实体类型entity type。</p>
<p>大多数的RE数据集没有对应的实体类型标注；但是作者认为实体类型是很重要的。</p>
<h4 id="human-annotation">2.3 Human Annotation</h4>
<p>众包。让4个受到过良好教育的标注者过滤掉错误标注的句子，并且标注实体之间的关系；</p>
<p>每个标注者需要判断是否能够从text和image中判断出对应的relation；同时，需要给选择的relation打confidence score；</p>
<p>最后，汇总4个标注者的标注结果，根据候选relation的总的confidence score来作为标注的relation。</p>
<h4 id="dataset-statistics">2.4 Dataset Statistics</h4>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221012210628691.png"   style="zoom:50%;" /></p>
<p>最终包含了10k的实例，31种关系；平均句子的长度是11.62，远远小于之前文本上下文信息丰富的RE数据集中的句子平均长度。 作者在后续更新了数据集，得到了MNRE-2：</p>
<blockquote>
<p>2021.6.22 We provide MNRE-2, a refined version which merges several ambigious categories with much more support samples. The original version has been moved to <a href="https://github.com/thecharm/MNRE/blob/main/Version-1">Version-1</a></p>
</blockquote>
<p>MNRE-2的统计： <img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/20221017115553.png" /></p>
<p>下图是不同关系类型的统计：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221017145814092.png"   style="zoom:50%;" /></p>
<p>经过检查发现，实际的训练集还包括了关系<code>None</code>。上面的统计图没有展现出None关系的分布。</p>
<p>作者的MNRE-2数据集从32变为了23种关系，发现大部分的关系还是和人相关的。MNRE-2训练集有12247、验证集1624和测试集1614实例。</p>
<p>查看下具体的数据集内容，在一个训练实例中，包括</p>
<ul>
<li><p><code>token</code>: <code>['The', 'latest', 'Arkham', 'Horror', 'LCG', 'deluxe', 'expansion', 'the', 'Circle', 'Undone', 'has', 'been', 'released', ':']</code></p></li>
<li><p><code>h</code>: <code>&#123;'name': 'Circle Undone', 'pos': [8, 10]&#125;</code></p></li>
<li><p><code>t</code>: <code>&#123;'name': 'Arkham Horror LCG', 'pos': [2, 5]&#125;</code>，这个<code>Arkham Horror LCG</code>应该是一种卡牌游戏</p></li>
<li><p><code>img_id</code>: <code>'twitter_19_31_16_6.jpg'</code>，所有的图片下载完后是1.2GB，下图是对应的图片</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/twitter_19_31_16_6.jpg"   style="zoom:40%;" /></p></li>
<li><p><code>relation</code>: <code>/misc/misc/part_of</code></p></li>
</ul>
<figure>
<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221012210820441.png" alt="image-20221012210820441" /><figcaption>image-20221012210820441</figcaption>
</figure>
<p>上图是数据集中的实例。可以看到，需要同时结合视觉和文本信息，才能够做出准确的关系预测。</p>
<h3 id="experimental-results">3. Experimental Results</h3>
<p>作者试验了几个已有的方法，Glove+CNN、BertNRE、Bert+CNN、PCNN。并且尝试了几种加入视觉信息的方法，包括拼接image label的embedding、拼接通过一个pretrained object detector导出的视觉特征embedding和利用visual-text的bi-linear注意力层。</p>
<p>实验结果：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221012211312936.png"   style="zoom:50%;" /></p>
<p>几个需要引入视觉信息，才能实现正确预测relation的实例：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221012211533605.png"  style="zoom:50%;" /></p>
<h3 id="thoughts">Thoughts</h3>
<ol type="1">
<li><p>感觉每个post对应一个image，然后单纯的从图像中抽取视觉信息来辅助关系抽取，所能够获得信息和覆盖范围还是比较受限。可能更理想的还是要有多张图片，从多张关联图片中进行信息抽取；或者说要从单个图片出发，进行信息拓展（图片搜索，寻找更多的相似/关联图片？图片进行场景识别后，转化为text，然后再从文本信息出发进行搜索？）</p></li>
<li><p>另外的一个问题是，如果随机检查图像信息和对应的文本，发现有不少的实例还是可能不需要图像信息就能够预测关系的，比如：</p>
<ul>
<li><p><code>token</code>: <code>['(', 'UPDATE', 'CHARA', ')', 'Baejoohyunews', ':', '[', 'PHOTO', ']', '190130', '#', &quot;REDVELVET'REDMARE&quot;, ':', 'Japan', 'Arena', 'Tour', 'in', 'Yokohama', &quot;'&quot;, 'Day2', 'RVsmtown']</code></p></li>
<li><code>h</code>: <code>&#123;'name': 'Japan Arena Tour', 'pos': [13, 16]&#125;</code></li>
<li><code>t</code>: <code>&#123;'name': 'Yokohama', 'pos': [17, 18]&#125;</code></li>
<li><code>img_id</code>: <code>twitter_19_31_9_14.jpg</code></li>
<li><p><code>relation</code>: <code>/misc/loc/held_on</code></p></li>
</ul>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/twitter_19_31_9_14.jpg"  style="zoom:50%;" /></p>
<p>实际上通过文本中的单词<code>in</code>就能够判断出来关系可能是<code>held_on</code></p></li>
</ol>
]]></content>
      <categories>
        <category>Paper</category>
        <category>MRE</category>
      </categories>
      <tags>
        <tag>MMKG</tag>
        <tag>MRE</tag>
      </tags>
  </entry>
  <entry>
    <title>TRC_Dataset</title>
    <url>/mmml/TRC-Dataset/</url>
    <content><![CDATA[<h1 id="categorizing-and-inferring-the-relationship-between-the-text-and-image-of-twitter-posts">Categorizing and Inferring the Relationship between the Text and Image of Twitter Posts</h1>
<p>ACL 2019，<a href="https://github.com/danielpreotiuc/text-image-relationship">代码</a>，彭博社</p>
<blockquote>
<p>Text in social media posts is frequently accompanied by images in order to provide content, supply context, or to express feelings. This paper studies how the meaning of the entire tweet is composed through the relationship between its textual content and its image. <strong>We build and release a data set of image tweets annotated with four classes which express whether the text or the image provides additional information to the other modality.</strong> We show that by combining the text and image information, we can build a machine learning approach that accurately distinguishes between the relationship types. Further, we derive insights into how these relationships are materialized through text and image content analysis and how they are impacted by user demographic traits. These methods can be used in several downstream applications including pre-training image tagging models, collecting distantly supervised data for image captioning, and can be directly used in end-user applications to optimize screen estate.</p>
</blockquote>
<p>作者对tweet的文本和图像之间的关系进行了定性与定量的分析，提出了文本和图像之间存在两个维度的关系：</p>
<ol type="1">
<li>文本内容是否在图像中表示（Text is represented / Text is not represented），关注文本和图像之间是否存在信息的重叠overlap</li>
<li>图像内容是否增加了tweet的语义（Image adds / Image does not add），关注图像的语义在整个tweet语义的作用，关注图像能否提供文本之外的信息</li>
</ol>
<p>作者创建了基于Twitter数据的文本-图像分类数据集TRC（Text-image relation classiﬁcation）</p>
<figure>
<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221116234200056.png" alt="image-20221116234200056" /><figcaption>image-20221116234200056</figcaption>
</figure>
<span id="more"></span>
<h2 id="introduction">1. Introduction</h2>
<p><strong>问题</strong>：图像在tweet中是一个很重要的角色，很大一部分的tweet都会带有图像，并且根据2016年的一个调查（What 1 Million Tweets Taught Us About How People Tweet Successfully. 2016）发现拥有图片的tweet的参与度是没有图像的tweet的两倍。但是没有研究讨论post的文本内容是如何和图像信息相关的。</p>
<p><strong>方案</strong>：作者从两个维度描述tweet上文本和图像信息的关系：</p>
<figure>
<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221116202933322.png" alt="image-20221116202933322" /><figcaption>image-20221116202933322</figcaption>
</figure>
<ul>
<li>对于a和b，图像增加了文本内容。a的图像通过提供更多的解释信息，b的图像通过提供更加明确的上下文情景理解文本内容</li>
<li>对于c和d，图像没有增加文本内容。c的图像仅仅是文本的重复表示，d的图像是一个网络meme，甚至与文本内容无关。</li>
<li>对于a和c，文本和图像的信息有重叠。</li>
<li>对于b和d，文本和图像的信息没有重叠。</li>
</ul>
<h2 id="categorizing-text-image-relationships">2. Categorizing Text-Image Relationships</h2>
<p>判断文本内容和图像内容是否有重叠的几个依据（Text task）：</p>
<ol type="1">
<li>文本内容在图像中进行了表示，有信息重叠（Text is represented）：部分或者全部的文本在图像中进行了表示</li>
<li>文本内容在图像中没有进行表示（Text is not represented）：
<ul>
<li>没有文本单词在图像中有对应</li>
<li>文本是对于图像内容的评论</li>
<li>文本是对于图像内容的feeling</li>
<li>文本仅仅是指向了图像内容</li>
<li>文本与图像无关</li>
</ul></li>
</ol>
<p>判断图像是否能够拓展文本语义（Image task）：</p>
<ol type="1">
<li>图像能够拓展文本语义
<ul>
<li>图像中包括了其它额外的文本</li>
<li>图像描绘了增加文本内容的其它信息</li>
<li>图像描绘了文本中引用的实体</li>
</ul></li>
<li>图像没有拓展文本语义：图像没有描绘任何文本内容之外的信息</li>
</ol>
<h2 id="data-set">3. Data Set</h2>
<p>数据采样来源是从一个已知用户个人信息的列表中（Beyond Binary Labels: Political Ideology Prediction of Twitter Users. ACL 2017），随机选择他们发布的tweet：</p>
<ul>
<li>只采样2016年的tweet，避免随着时间用户发布tweet可能的改变</li>
<li>过滤掉所有的非英语tweet</li>
<li>只选择美国的用户，避免文化差异</li>
</ul>
<p>最终获得了2263个用户发布的4471个tweet。</p>
<p>在CrowdFlower上通过众包进行标注，最终的统计结果如下，来自RpBERT论文：</p>
<figure>
<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221116213926778.png" alt="image-20221116213926778" /><figcaption>image-20221116213926778</figcaption>
</figure>
<h2 id="predicting-text-image-relationship">5. Predicting Text-Image Relationship</h2>
<p>使用80%训练，20%测试，作者使用了一系列的方法进行判断：</p>
<figure>
<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221116214107361.png" alt="image-20221116214107361" /><figcaption>image-20221116214107361</figcaption>
</figure>
<p>结果发现同时使用文本和图像的方法，更能够准确的判断出来text-image的关系。</p>
<h2 id="analysis">6. Analysis</h2>
<p>作者使用用户的人口特征、tweet metadata（如账号的follow人数、tweet是否是reply等）以及文本，进行了综合的分析（使用Pearson correlation）。</p>
<p>结果发现用户的特征中，年龄和text-image的关系比较大，而其他的特征关系较弱。年龄大的喜欢发送和文本内容有重叠的tweet，年龄较小的用户喜欢发送和文本内容无关的tweet（比如一个表情包meme）。</p>
<figure>
<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221116214514737.png" alt="image-20221116214514737" /><figcaption>image-20221116214514737</figcaption>
</figure>
<p>作者发现tweet metadata和text-image关系没有显著关联。</p>
<p>对文本进行分析，发现某些特定文本词和text-image关系有关联：</p>
<figure>
<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221116214915524.png" alt="image-20221116214915524" /><figcaption>image-20221116214915524</figcaption>
</figure>
]]></content>
      <categories>
        <category>Paper</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>modality-discriminator</title>
    <url>/mmml/modality-discriminator/</url>
    <content><![CDATA[<h1 id="different-data-different-modalities-reinforced-data-splitting-for-effective-multimodal-information-extraction-from-social-media-posts">Different Data, Different Modalities! Reinforced Data Splitting for Effective Multimodal Information Extraction from Social Media Posts</h1>
<p>COLING 2022，<a href="https://github.com/xubodhu/RDS">代码</a>。</p>
<p>作者认为，不是所有的social media post都需要多模态信息，可能有的post更适合单模态模型，如果加入多模态信息反而可能造成错误的后果。因此，作者基于强化学习，提出了一种可以把social post分为单模态集合和多模态集合的方法。</p>
<blockquote>
<p>Recently, multimodal information extraction from social media posts has gained increasing attention in the natural language processing community. <strong>Despite their success, current approaches overestimate the significance of images. In this paper, we argue that different social media posts should consider different modalities for multimodal information extraction. </strong>Multimodal models cannot always outperform unimodal models. Some posts are more suitable for the multimodal model, while others are more suitable for the unimodal model. Therefore, we propose a general data splitting strategy to divide the social media posts into two sets so that these two sets can achieve better performance under the information extraction models of the corresponding modalities. Specifically, for an information extraction task, we first propose a data discriminator that divides social media posts into a multimodal and a unimodal set. Then we feed these sets into the corresponding models. Finally, we combine the results of these two models to obtain the final extraction results. Due to the lack of explicit knowledge, we use reinforcement learning to train the data discriminator. Experiments on two different multimodal information extraction tasks demonstrate the effectiveness of our method. The source code of this paper can be found in https://github.com/xubodhu/RDS.</p>
</blockquote>
<span id="more"></span>
<h2 id="introduction">1 Introduction</h2>
<p><strong>问题</strong> ：不是所有的social post都适合于多模态信息抽取方法。比如下图：</p>
<figure>
<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221020164307709.png" alt="image-20221020164307709" /><figcaption>image-20221020164307709</figcaption>
</figure>
<p>对于(a)和(c)来说，加入图像信息能够辅助信息抽取，比如我们可以知道[Kolo MISC]是一条狗，而不是一个人；从图像中两个人拉着手，可以判断出Meghan Markle和Prince Harry是夫妻而不是同事。</p>
<p>但是对于(b)和(d)来说，加入图像信息反而可能导致错误判断。在(b)里可能因为看到图像中有个人像，然后判断[Nasa ORG]是个人；在(d)里因为看到很多人类似的服装，错误判断出Angel和Jesennia Rodriguez是同事而不是夫妻。</p>
<p><strong>方法</strong>：作者期望能够设计一种把不同social post分类为适用于单模态和适用于多模态的方法。由于不存在这样的数据集，因此作者提出了一种基于强化学习的data discriminator。</p>
<h2 id="method">2 Method</h2>
<figure>
<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221020165100186.png" alt="image-20221020165100186" /><figcaption>image-20221020165100186</figcaption>
</figure>
<p>首先，作者会把训练集随机分为<span class="math inline">\(D_{model}\)</span> (80%)和<span class="math inline">\(D_{split}\)</span> (20%)。</p>
<p><span class="math inline">\(D_{model}\)</span>用来训练单模态和多模态模型，这两种模型可以是任意的方法，训练完毕之后，就freeze所有参数。</p>
<p><span class="math inline">\(D_{split}\)</span>是用来训练Data discriminator，Data discriminator会判断某个输入样本是否适用于多模态方法，根据Data discriminator的判断结果，检测单模态和多模态模型在判断的单模态集合和多模态集合上的表现效果。如果在划分的多模态集合中，多模态模型效果比单模态模型效果越好；在划分的单模态集合中，单模态模型效果比多模态模型效果越好，就证明这个Data discriminator的判断效果越准确。</p>
<h3 id="data-discriminator">2.1 Data Discriminator</h3>
<p>作者基于<a href="https://huggingface.co/openai/clip-vit-base-patch32"><span class="math inline">\(CLIP_{32}\)</span></a>进行实现，对CLIPTextModel和CLIPVisionModel的输出进行投影、element-wise相乘，最后经过MLP得到预测结果：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221020170009371.png"   style="zoom:50%;" /></p>
<p><span class="math inline">\(p\)</span>越大，表示越适合于使用多模态方法进行信息抽取</p>
<p>在划分单模态集合和多模态集合的时候，在训练阶段，依据伯努利采样；在测试阶段，<span class="math inline">\(p\)</span>大于<span class="math inline">\(0.5\)</span>的被放入多模态集合，<span class="math inline">\(p\)</span>小于<span class="math inline">\(0.5\)</span>的被放入单模态集合。</p>
<h3 id="reward-function">2.2 Reward Function</h3>
<p>如何判断划分结果的好坏？核心思想是让单模态模型在单模态集合上表现效果更好；让多模态模型在多模态集合上表现效果更好。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221020170334868.png"   style="zoom:50%;" /></p>
<p><span class="math inline">\(k\)</span>代表适合于使用多模态数据的数据； <span class="math inline">\(l\)</span>代表适合用于单模态数据的数据</p>
<p>计算reward：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221020170433346.png"   style="zoom:50%;" /></p>
<p>在得到了reward之后，data discriminator如何更新参数？</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221020170523043.png"   style="zoom:50%;" /></p>
<h3 id="training-algorithm">2.3 Training Algorithm</h3>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221020170546235.png"   style="zoom:40%;" /></p>
<h2 id="experiment">3 Experiment</h2>
<p>对于MRE，作者使用MTB作为单模态模型，MEGA作为多模态模型，最终MRE结果：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221020170616500.png"   style="zoom:50%;" /></p>
<p>对于MNER，作者使用BERT-CRF作为单模态模型，UMT-BERT-CRF和MAF作为多模态模型，最终MNER结果：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221020170659801.png"   style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221020170718093.png"   style="zoom:50%;" /></p>
<p>案例研究，作者挑选了两个在Twitter-2017中，两个拥有最低分类得分的post：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221020172105930.png"   style="zoom:50%;" /></p>
<p>可以看到，在这两个实例中，text本身就有足够的信息，不需要引入图像的信息。</p>
<p>两个拥有最高分类得分的post：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221020172202700.png"   style="zoom:50%;" /></p>
<p>对于(c)如果没有图像，很容易把Harry Potter和the Philosopher’s Stone分开，然后把Harry Potter判断为是一个人名；对于(d)如果没有图像，很容易把R.Shemiste认为是人名，但实际上它是品牌名。</p>
<p>两个得分在中间的post：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221020172402192.png"   style="zoom:50%;" /></p>
<p>可以看到，图像信息既没有带来更丰富的信息；也没有带来噪音。这两张图片里重复了一遍文本内容，实际上这一类的图像还挺常见的，使用单模态模型和多模态模型对这一类post的判断效果可能都差不多。</p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>MMKG</category>
      </categories>
      <tags>
        <tag>MMKG</tag>
        <tag>MRE</tag>
      </tags>
  </entry>
  <entry>
    <title>Attention-survey-2021</title>
    <url>/ml/Attention-survey-2021/</url>
    <content><![CDATA[<h1 id="a-general-survey-on-attention-mechanisms-in-deep-learning">A General Survey on Attention Mechanisms in Deep Learning</h1>
<p>TKDE 2021</p>
<blockquote>
<p>Attention is an important mechanism that can be employed for a variety of deep learning models across many different domains and tasks. This survey provides an overview of the most important attention mechanisms proposed in the literature. The various attention mechanisms are explained by means of a framework consisting of a general attention model, uniform notation, and a comprehensive taxonomy of attention mechanisms. Furthermore, the various measures for evaluating attention models are reviewed, and methods to characterize the structure of attention models based on the proposed framework are discussed. Last, future work in the ﬁeld of attention models is considered.</p>
</blockquote>
<p>这篇文章调研了大量的注意力方法，集中在surprised learning领域。</p>
<span id="more"></span>
<h2 id="introduction">1 Introduction</h2>
<p>让模型模仿人的注意力（attention），只关注重要部分，忽略次要部分的思想，最早出现在CV领域，例如论文<em>Learning to combine foveal glimpses with a third-order Boltzmann machine NIPS 2010</em>和<em>Recurrent models of visual attention NIPS 2014</em>。但是我们普遍认为现在的注意力机制起源，是在NLP领域，<em>Neural machine translation by jointly learning to align and translate ICLR 2015</em>。</p>
<p>注意力受到研究人员的重视，有以下原因：</p>
<ul>
<li>效果好，取得SOTA的模型往往会采用注意力方法（特别是在Transformer方法被提出后）。使用注意力方法在各个领域都被证明有效。</li>
<li>注意力机制可以很容易的和原来的base model结合，一起通过BP优化。</li>
<li>某些情况下，注意力可以为深度学习带来更合理的解释</li>
</ul>
<p>这篇survey的贡献：</p>
<ul>
<li>使用统一的描述、统一的框架描述了大量不同领域的注意力机制</li>
<li>提出注意力的一种分类法</li>
<li>评估注意力模块的不同方法</li>
</ul>
<h2 id="general-attention-model">2 General Attention Model</h2>
<p>作者讨论的模型架构分为四部分，feature model、attention model、query model和output model。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220516213153701.png" style="zoom:50%;" /></p>
<p>feature model是base model，也就是用来处理原始数据，进行信息提取，然后得到需要进行attention的特征向量。它可以是CNN、RNN、GNN等各种网络网络结构，假设经过feature model之后，得到了特征集合<span class="math inline">\(F\)</span>：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220516213545170.png" style="zoom:50%;" /></p>
<p>query model会产生查询向量，它根据此刻output model需要的context生成，用来评估各个特征向量哪个是可能更重要的：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220516214139491.png" style="zoom:50%;" /></p>
<p>query vector可能是直接定义的某个向量；也可能是RNN中之前的隐藏状态等。</p>
<p>attention model是关键，它的整个过程如图：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220516214418565.png" style="zoom:50%;" /></p>
<p>attention model接收到上面feature model和query model的输入后，根据特征向量会构建对应的<strong>value集合</strong>和<strong>key集合</strong>。这一步可能是类似于Transformer中的线性投影；也可能什么都不做，直接使用特征向量；可以是任意合理的映射函数。下面是采用Transformer中的线性投影：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220516213902561.png" style="zoom:50%;" /></p>
<p>接下来经过三个步骤：</p>
<p><strong>1. score function</strong></p>
<p>利用query和key，计算attention value：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220516214648154.png" style="zoom:50%;" /></p>
<p><span class="math inline">\(e_l\)</span>表示第<span class="math inline">\(i\)</span>个key vector对于query <span class="math inline">\(\mathbf{q}\)</span>有多重要。最终对应每个value vector都有一个attention value：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220516214804659.png" alt="image-20220516214804659" style="zoom:50%;" /></p>
<p><strong>2. alignment function</strong></p>
<p>在很多情况下，得到的attention value可能会超出<span class="math inline">\([0,1]\)</span>，并且我们期望的注意力最后得到的输出是平均加权和。所以attention value会经过一个alignment function进行重新分布（redistributed）：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220516215219118.png" style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220516215321857.png" style="zoom:50%;" /></p>
<p>最常用的方法是softmax，当然还有其它的方法。</p>
<p><strong>3. weight average</strong></p>
<p>对value vector根据attention weight，加权求和：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220516215431614.png" style="zoom:50%;" /></p>
<p>output model是使用feature model提取出的特征向量经过attention之后得到的输出，表示后续的网络结构。</p>
<h2 id="attention-taxonomy">3 Attention Taxonomy</h2>
]]></content>
      <categories>
        <category>Paper</category>
        <category>Survey</category>
      </categories>
      <tags>
        <tag>Attention</tag>
      </tags>
  </entry>
  <entry>
    <title>Fast-weights-RNN</title>
    <url>/ml/Fast-weights-RNN/</url>
    <content><![CDATA[<h1 id="using-fast-weights-to-attend-to-the-recent-past">Using Fast Weights to Attend to the Recent Past</h1>
<p>NIPS 2016</p>
<p>作者将fast weights引入到RNN中实现了更好的效果。本质上是在RNN的t时刻到t+1时刻中间，插入了一段新的RNN结构，每个step计算之前的隐藏状态和当前隐藏状态的关系权重，不断累加，最后达到比较好的效果。</p>
<span id="more"></span>
<p>这里需要先介绍下fast weights。</p>
<p>在1987年的时候，有一篇paper《Using Fast Weights to Deblur Old Memories》，提出了下面的说法：</p>
<blockquote>
<p>Despite the emerging biological evidence that changes in synaptic efficacy at a single synapse occur at many different time-scales (Kupferman, 1979; Hartzell, 1981), there have been relatively few attempts to investigate the computational advantages of giving each connection several different weights that change at different speeds.</p>
</blockquote>
<p>意思是说如果把一个weight的更新看做是神经元的一次神经活动，那么weight的更新也应该是有不同time scalse的。</p>
<p>那么如果模仿这个过程，除了一般的weight外，还可以尝试加入其它time scale的weight，也就是fast weight，fast weight用来模拟短时的记忆。</p>
<ul>
<li>Slow weight: The slow weights are like the weights normally used in connectionist networks-they change slowly and they hold all the long-term knowledge of the network.</li>
<li>Fast weight: The fast weights change more rapidly and they continually regress towards zero so that their magnitude is determined solely by their recent past.</li>
</ul>
<p>来看一下作者具体怎么样把fast weight引入到RNN中：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220401213438036.png" alt="image-20220401213438036" style="zoom:40%;" /></p>
<p>首先定义一个fast weight matrix：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220401213526767.png" alt="image-20220401213526767" style="zoom:50%;" /></p>
<p>然后在RNN的<span class="math inline">\(t\)</span> step到<span class="math inline">\(t+1\)</span> step中间，插入新的多个step：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220401213626396.png" alt="image-20220401213626396" style="zoom:50%;" /></p>
<p>从这里可以看出来，<span class="math inline">\(A\)</span>被用来快速更新状态。</p>
<p>由于实际中，sequence的time step数量是要比定义的hidden state vector的维度要小的，所以最后计算出来的A实际上远远不是一个满秩的矩阵。为了计算方便，作者假定对于不同sequence，<span class="math inline">\(A\)</span>的初始值是0，那么有：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220401213830869.png" alt="image-20220401213830869" style="zoom:50%;" /></p>
<p>通过简单计算之前时刻隐藏状态和最近隐藏状态的点积，作为权重（也是attention），然后加上以前的隐藏状态，计算很快速。</p>
<p>作者还使用了layer normalization来防止两个向量的点积可能出现的数值消失或者爆炸的问题。</p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>Theory</category>
      </categories>
  </entry>
  <entry>
    <title>HyperNetworks</title>
    <url>/ml/HyperNetworks/</url>
    <content><![CDATA[<h1 id="hypernetworks">HYPERNETWORKS</h1>
<p>ICLR 2017</p>
<p>核心贡献是将Hypernetwork扩展到了convolutional networks和long recurrent networks，证明其在使用更少的参数情况下，在序列模型和卷积网络的多个预测任务下都达到了不错的训练结果。</p>
<span id="more"></span>
<h2 id="introduction">Introduction</h2>
<p>Hypernetwork是一个能够为另一个更大的网络产生weight的较小的网络。示例：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210725110111444.png" alt="image-20210725110111444" style="zoom:50%;" /></p>
<blockquote>
<p>In this work, we consider an approach of using a small network (called a “hypernetwork&quot;) to generate the weights for a larger network (called a main network).</p>
</blockquote>
<blockquote>
<p>hypernetwork takes a set of inputs that contain information about the structure of the weights and generates the weight for that layer</p>
</blockquote>
<p>HyperNEAT是一个使用hypernetwork的实例，输入时weight的virtual coordinates。</p>
<p>这篇文章的hypernetwork直接接收一个描述weight的embedding vector。同时设计了CNN和RNN的两种变体。</p>
<h2 id="method">Method</h2>
<h3 id="static-hypernetwork-a-weight-factorization-approach-for-deep-convolutional-networks">Static Hypernetwork : A Weight Factorization Approach For Deep Convolutional Networks</h3>
<p><span class="math inline">\(K^j\)</span>是第<span class="math inline">\(j\)</span>​层的卷积核，一共有<span class="math inline">\(D\)</span>层</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210725111008315.png" alt="image-20210725111008315" style="zoom:50%;" /></p>
<p>它由一个hypernetwork产生，每层接收一个描述weight的embedding <span class="math inline">\(z^j\)</span></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210725111037575.png" alt="image-20210725111037575" style="zoom:50%;" /></p>
<p>具体产生方法，一个静态的hypernetwork，简单看了下实验，<span class="math inline">\(z^j\)</span>是一个比较小的embedding，甚至只有4。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210725111153567.png" alt="image-20210725111153567" style="zoom:50%;" /></p>
<h3 id="dynamic-hypernetwork-a-daptive-weight-generation-for-recurrent-networks">Dynamic Hypernetwork : A Daptive Weight Generation For Recurrent Networks</h3>
<p>一个新的RNN，weight是生成的：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210725111430981.png" alt="image-20210725111430981" style="zoom:50%;" /></p>
<p>这个hypernetwork同样是用另一个小的RNN产生。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210725111544605.png" alt="image-20210725111544605" style="zoom:50%;" /></p>
<p>示例图：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210725111631387.png" alt="image-20210725111631387" style="zoom:50%;" /></p>
<p>实际上，作者使用了另一种简化的版本，每一层定义了一个weight scaling vector <span class="math inline">\(d\)</span>​​，不再是完成生成weight matrix，而是生成weight vector。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210725111733324.png" alt="image-20210725111733324" style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210725111717560.png" alt="image-20210725111717560" style="zoom:50%;" /></p>
]]></content>
      <categories>
        <category>Paper</category>
      </categories>
  </entry>
  <entry>
    <title>Independence Survey</title>
    <url>/ml/Independence-Survey/</url>
    <content><![CDATA[<h1 id="independence-modeling-method">Independence Modeling Method</h1>
<p>对目前接触的集中能够约束差异性/独立性的方法做个简单汇总。包括</p>
<ul>
<li>互信息Mutual information.</li>
<li>距离相关性Distance correlation.</li>
<li>Hilbert-Schmidt Independence Criterion (HSIC)</li>
</ul>
<span id="more"></span>
<p>Mutual information和Distance correlation.在推荐模型KGIN（Learning Intents behind Interactions with Knowledge Graph for Recommendation）中得到了使用，用于约束几个特定embedding之间的独立性。</p>
<p>Mutual information：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210709151338821.png" style="zoom:50%;" /></p>
<p>其中的函数<span class="math inline">\(s()\)</span>是计算相似度的函数，在文章中使用了cosine similarity。</p>
<p>Distance correlation：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210709151449098.png" style="zoom:50%;" /></p>
<p>其中的<span class="math inline">\(dCor()\)</span>是距离相关性distance correlation：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210709151520931.png" style="zoom:50%;" /></p>
<p>Hilbert-Schmidt Independence Criterion (HSIC)方法在AM-GCN中应用，</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210709151711018.png" style="zoom:50%;" /></p>
<p>对于上述的集中方法还没有深入理解，先做一下记录。</p>
]]></content>
      <categories>
        <category>Note</category>
      </categories>
  </entry>
  <entry>
    <title>Meta-learning-intro</title>
    <url>/ml/Meta-learning-intro/</url>
    <content><![CDATA[<h1 id="meta-learning-learning-to-learn-fast">Meta-Learning: Learning to Learn Fast</h1>
<p>这是一篇博客（<a href="https://lilianweng.github.io/posts/2018-11-30-meta-learning/">Meta-Learning: Learning to Learn Fast</a>）的笔记，另外参考了对应的<a href="https://wei-tianhao.github.io/blog/2019/09/17/meta-learning.html">中文博客</a>，简单了解什么是meta-learning。</p>
<p>元学习尝试解决深度学习经常需要大量实例数据才能收敛的问题。我们期望好的元学习模型拥有好的泛化能力和适应能力，能够根据少量的样本就学习到比较合适的信息。</p>
<p>元学习可以解决一类定义好的预测任务，这篇文章主要讨论的是监督学习下的元学习问题。例如让一个图片分类器在训练集中没有猫的情况下，在测试集中能够实现只看到几张猫的图片就能够学会识别猫。</p>
<span id="more"></span>
<h2 id="overview">Overview</h2>
<p>假设有很多的任务可以学习，我们期望元学习模型能够在整个的任务空间下都达到比较好的效果，即使是遇到了一个新的任务也能够表现不错。比如下面的任务：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220330211915167.png" style="zoom:35%;" /></p>
<p>上面的任务是少次学习的一个示例图，它是元学习在监督学习问题的一个实例，一般说明K-shot N-class，是指一个数据集中class包括K个labeled examples。</p>
<p>为了模拟测试集中的推理过程，在训练的时候，会尝试采样一个support set和一个prediction set，support set用来计算一次loss，然后假梯度下降，使用这个假更新后的参数计算模型在prediction set上的loss，计算梯度，使用这时候的梯度才真正的更新原来的梯度。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220330212645013.png" style="zoom:50%;" /></p>
<p>通过上面的操作，让梯度下降的方向不是当前loss的梯度，而是“往前看了一步”，让梯度朝着更能拟合新的prediction task的方向优化。</p>
<p>接下来介绍三类经典的meta-learning模型。</p>
<h2 id="metric-based">Metric-Based</h2>
<p>基于度量的方法，核心思想是计算新的样本和当前support set中的样本的相似程度，让后让已有的样本提供信息。</p>
<h3 id="siamese-neural-networks">Siamese Neural Networks</h3>
<p><a href="http://www.cs.toronto.edu/~rsalakhu/papers/oneshot1.pdf">Koch, Zemel &amp; Salakhutdinov</a>提出。对于one-shot任务，设计一个CNN网络导出图片特征，然后分辨两个图片对于的embedding的相似程度（使用L1-distance），如果属于同一类就输出1，否则输出0。在测试的时候，让测试样本和support set中所有的图片计算相似度，最相似的那一个图片对应的类就是测试样本的类别。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220330213424764.png" alt="image-20220330213424764" style="zoom:30%;" /></p>
<h3 id="matching-networks">Matching Networks</h3>
<p><a href="http://papers.nips.cc/paper/6385-matching-networks-for-one-shot-learning.pdf">Vinyals et al., 2016</a>提出。对于K-shot任务，使用下面的方法：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220330213849574.png" alt="image-20220330213849574" style="zoom:30%;" /></p>
<p>核心公式：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220330213923274.png" alt="image-20220330213923274" style="zoom:40%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220330213937734.png" alt="image-20220330213937734" style="zoom:40%;" /></p>
<h3 id="relation-network">Relation Network</h3>
<p><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers_backup/Sung_Learning_to_Compare_CVPR_2018_paper.pdf">Sung et al., 2018</a>提出。样本的相似度计算是使用一个CNN方法<span class="math inline">\(g_{\phi}\)</span>来实现的。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220330214037937.png" alt="image-20220330214037937" style="zoom:40%;" /></p>
<h3 id="prototypical-networks">Prototypical Networks</h3>
<p><a href="http://papers.nips.cc/paper/6996-prototypical-networks-for-few-shot-learning.pdf">Snell, Swersky &amp; Zemel, 2017</a>提出。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220330214215177.png" alt="image-20220330214215177" style="zoom:40%;" /></p>
<h2 id="model-based">Model-Based</h2>
<p>让模型本身拥有快速学习的能力。</p>
<h3 id="mann-for-meta-learning">MANN for Meta-Learning</h3>
<p><a href="http://proceedings.mlr.press/v48/santoro16.pdf">Santoro et al., 2016</a>提出。以<a href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#neural-turing-machines">NTM模型</a>为基础让拥有外部存储单元的MANN（Memory-Augmented Neural Networks）模型（注意，仅仅是GRU和LSTM这些不属于MANN）适用于元学习。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220330215049680.png" alt="image-20220330215049680" style="zoom:50%;" /></p>
<h3 id="metanet">MetaNet</h3>
<p><a href="https://arxiv.org/abs/1703.00837">Munkhdalai &amp; Yu, 2017</a>提出。这里的fast weights和slow weights需要进一步了解。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220330215357448.png" alt="image-20220330215357448" style="zoom:40%;" /></p>
<h2 id="optimization-based">Optimization-Based</h2>
<p>不再依赖于具体的模型，而是直接从梯度下降的原理出发进行设计。</p>
<h3 id="lstm-meta-learner">LSTM Meta-Learner</h3>
<p>适用LSTM来显式建模梯度下降的过程。<a href="https://openreview.net/pdf?id=rJY0-Kcll">Ravi &amp; Larochelle (2017)</a>提出。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220330215638810.png" alt="image-20220330215638810" style="zoom:40%;" /></p>
<p>把梯度下降，看做是一步LSTM中的状态更新：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220330215906741.png" alt="image-20220330215906741" style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220330215923820.png" alt="image-20220330215923820" style="zoom:50%;" /></p>
<p>然后使用LSTM来显式的建模梯度下降步骤：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220330220104006.png" alt="image-20220330220104006" style="zoom:50%;" /></p>
<h3 id="maml">MAML</h3>
<p><strong>Model-Agnostic Meta-Learning</strong> 简称 <strong>MAML</strong> (<a href="https://arxiv.org/abs/1703.03400">Finn, et al. 2017</a>)，是一种通用的优化算法，</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/maml-algo.png" alt="MAML Algorithm" style="zoom:40%;" /></p>
<p>使用不同task，分别计算梯度，然后再使用prediction set进行实际的参数更新。</p>
]]></content>
      <categories>
        <category>Blog</category>
      </categories>
      <tags>
        <tag>meta-learning</tag>
      </tags>
  </entry>
  <entry>
    <title>NN-extrapolate</title>
    <url>/ml/NN-extrapolate/</url>
    <content><![CDATA[<h1 id="how-neural-networks-extrapolate-from-feedforward-to-graph-neural-networks">HOW NEURAL NETWORKS EXTRAPOLATE: FROM FEEDFORWARD TO GRAPH NEURAL NETWORKS</h1>
<p>ICLR 2021</p>
<p>这篇文章主要从理论和实验角度研究了MLP和GNN的外推（extrapolate）性能。</p>
<span id="more"></span>
<blockquote>
<p><strong>We study how neural networks trained by gradient descent extrapolate, i.e., what they learn outside the support of the training distribution.</strong> Previous works report mixed empirical results when extrapolating with neural networks: while feedforward neural networks, a.k.a. multilayer perceptrons (MLPs), do not extrapolate well in certain simple tasks, Graph Neural Networks (GNNs) – structured networks with MLP modules – have shown some success in more complex tasks. Working towards a theoretical explanation, we identify conditions under which MLPs and GNNs extrapolate well. First, we quantify the observation that ReLU MLPs quickly converge to linear functions along any direction from the origin, which implies that ReLU MLPs do not extrapolate most nonlinear functions. But, they can provably learn a linear target function when the training distribution is sufﬁciently “diverse”. Second, in connection to analyzing the successes and limitations of GNNs, these results suggest a hypothesis for which we provide theoretical and empirical evidence: the success of GNNs in extrapolating algorithmic tasks to new data (e.g., larger graphs or edge weights) relies on encoding task-speciﬁc non-linearities in the architecture or features. Our theoretical analysis builds on a connection of over-parameterized networks to the neural tangent kernel. Empirically, our theory holds across different training settings.</p>
</blockquote>
<h2 id="introduction">Introduction</h2>
<p>什么是模型的外推性能？</p>
<blockquote>
<p>We say a neural network extrapolates well if it learns a task outside the training distribution.</p>
</blockquote>
<p>作者的两点贡献：</p>
<ul>
<li>分析并证明了MLP外推的结果以及什么情况下MLP外推效果好</li>
<li>解释了为什么GNN能够在一些算法任务上（比如动态规划DP）外推效果好，并且提出了合适的改进方法</li>
</ul>
<p>一些相关的关键工作。</p>
<ul>
<li>有研究者证明了ReLU MLP最后学习到的是分段线性函数，例如《Complexity of linear regions in deep networks》</li>
<li>有研究者在更大的graph上测试GNN的外推性能，发现在找最短路径等任务上外推性能好，但是没有使用理论分析</li>
</ul>
<h2 id="how-feedforward-neural-networks-extrapolate">HOW FEEDFORWARD NEURAL NETWORKS EXTRAPOLATE</h2>
<p>外推效果的定义，通过定义外推loss：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220411211241283.png" alt="image-20220411211241283" style="zoom:50%;" /></p>
<p>一个ReLU MLP是如何外推的：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220411211349989.png" alt="image-20220411211349989" style="zoom:50%;" /></p>
<p>ReLU MLP不会顺着灰色的期望进行外推，而是很快就外推成为一个线性function。看一看下面的定理（MLP是使用NTK来训练的）：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220411211822939.png" alt="image-20220411211822939" style="zoom:50%;" /></p>
<p>怎么样让ReLU MLP能够外推结果好？</p>
<ol type="1">
<li>让MLP拟合的目标函数是线性的</li>
<li>让训练集足够的diverse，这样训练好的模型能够学到足够多合适“方向”便于外推</li>
</ol>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220411211558986.png" alt="image-20220411211558986" style="zoom:50%;" /></p>
<p>其它激活函数MLP什么时候效果好？</p>
<ul>
<li>当目标函数的分布和激活函数大致相似的时候</li>
</ul>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220411211727352.png" alt="image-20220411211727352" style="zoom:50%;" /></p>
<p>图中MAPE是指平均绝对误差比例，越小越好（原论文没提）。</p>
<h2 id="how-graph-neural-networks-extrapolate">HOW GRAPH NEURAL NETWORKS EXTRAPOLATE</h2>
<p>GNN实际是在MLP拟合线性function的基础上，通过让模型本身就编码了task-specific的非线性，来获得好的外推性能。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220411212409057.png" alt="image-20220411212409057" style="zoom:50%;" /></p>
<p>上面第一个图是使用GNN进行最短路径寻找的任务，如果使用sum的聚合方法，外推效果就差；如果是使用min的聚合方法，外推效果就比较好。这是因为此时的GNN实际上是在拟合BF最短路径算法：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220411212547418.png" alt="image-20220411212547418" style="zoom:50%;" /></p>
<p>GNN拟合BF算法：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220411212610549.png" alt="image-20220411212610549" style="zoom:50%;" /></p>
<p><span class="math inline">\(d[k][u]\)</span>是指第<span class="math inline">\(k\)</span>轮迭代，到节点<span class="math inline">\(u\)</span>的最短路径。</p>
<p>此时GNN只是使用MLP来拟合一个线性函数<span class="math inline">\(d[k-1][v]+w(v,u)\)</span>，因此外推效果较好。</p>
<p>同样的道理，可以使用max聚合方法，让GNN在计算graph的最大度任务上，外推效果好。</p>
<p>可以拓展来看，很多可以使用DP算法解决的问题，由于DP算法和GNN的聚合思想很像，或许可以从算法的角度改进GNN，让GNN外推效果好：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220411213021249.png" alt="image-20220411213021249" style="zoom:50%;" /></p>
<p>作者还提出了另外一种让GNN能够外推效果好的方法，就是提前获得某些合适的非线性的表示，然后让GNN只需要使用MLP拟合线性部分即可，最后结合非线性的表示就可以逼近理想的函数。</p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>GNN</category>
      </categories>
      <tags>
        <tag>GNN</tag>
        <tag>Theory</tag>
      </tags>
  </entry>
  <entry>
    <title>TPE</title>
    <url>/ml/TPE/</url>
    <content><![CDATA[<h1 id="the-intuitions-behind-tree-structured-parzen-estimator">The intuitions behind Tree-structured Parzen estimator</h1>
<p>TPE：一种基于贝叶斯推断的超参数调优方法。</p>
<span id="more"></span>
<h2 id="reference">Reference</h2>
<p>[1] Frazier P I. A tutorial on bayesian optimization[J]. arXiv preprint arXiv:1807.02811, 2018.</p>
<p>[2] Bergstra J S, Bardenet R, Bengio Y, et al. Algorithms for hyper-parameter optimization[C]//Advances in neural information processing systems. 2011: 2546-2554.</p>
<p>[3] Xia Y, Liu C, Li Y Y, et al. A boosted decision tree approach using Bayesian hyper-parameter optimization for credit scoring[J]. Expert Systems with Applications, 2017, 78: 225-241.</p>
<p>[4] <a href="https://towardsdatascience.com/a-conceptual-explanation-of-bayesian-model-based-hyperparameter-optimization-for-machine-learning-b8172278050f">A Conceptual Explanation of Bayesian Hyperparameter Optimization for Machine Learning</a></p>
<h2 id="bayesian-hyper-parameter-optimization">1. Bayesian hyper-parameter optimization</h2>
<p>贝叶斯超参数优化是一种为序列模式的模型提供的求全局优化的方法。</p>
<p>针对的问题：求解object function：</p>
<p><span class="math display">\[ min_{x\in A} f(x) \]</span></p>
<p>主要适用的情景：</p>
<ul>
<li>x的维度不是太大，一般会限制在<span class="math inline">\(d\lt20\)</span>，x可以理解为一个超参数序列</li>
<li><span class="math inline">\(f(x)\)</span>是一个计算起来很消耗时间的函数，例如损失函数</li>
<li>对<span class="math inline">\(f(x)\)</span>很难求导</li>
<li>......</li>
</ul>
<p>举例：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20191110144728090.png" style="zoom:50%;" /></p>
<p>纵坐标是<span class="math inline">\(f(x)\)</span>，很明显如果是我们人工去选择下一组超参数的话，会在左下角score=20的这几个点中间去搜索，而不会还是进行一个全局的抽样。这实际就是一个贝叶斯优化过程。</p>
<p>但是对于Random search，grid search这些方法，并不会利用到历史的信息来进行选择。</p>
<h2 id="sequential-model-based-global-optimization-smbo">2. Sequential model-based global optimization (SMBO)</h2>
<h3 id="伪代码">2.1 伪代码</h3>
<p>序列化模型全局优化(SMBO)是把贝叶斯优化的一个形式化的定义。具体的伪代码如下：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20191110161036552.png" style="zoom:50%;" /></p>
<p>伪代码中出现的符号含义：</p>
<ul>
<li><span class="math inline">\(\lambda\)</span>：超参数</li>
<li><p><span class="math inline">\(L\)</span>：object function，通常是损失函数</p></li>
<li><p><span class="math inline">\(H\)</span>：记录了所有(超参数<span class="math inline">\(\lambda\)</span>, <span class="math inline">\(L\)</span>)的历史集合</p></li>
<li><p>$ S_{t} <span class="math inline">\(：一个从超参数\)</span>L $的映射概率模型</p></li>
</ul>
<p>循环一共进行T次：</p>
<ol type="1">
<li>每次先根据已有的历史试验集合找出一个更可能拥有更小objective function的超参数集合<span class="math inline">\(\lambda\)</span></li>
<li>之后计算实际的object function的值</li>
<li>加入历史集合</li>
<li>更新<span class="math inline">\(S\)</span></li>
</ol>
<h3 id="一些粗浅的理解">2.2 一些粗浅的理解</h3>
<ol type="1">
<li><p>不断的利用历史试验(trial)记录，构建出了一个下面的概率模型：</p>
<p><span class="math display">\[ P(objective\ function\ score\quad |\quad hyperparameters) \]</span></p></li>
<li><p>叫做贝叶斯的原因是出现了先验概率的思想</p></li>
<li><p>核心想法是<strong>在更有可能得到更好结果的超参数范围内选择新的超参数</strong></p></li>
</ol>
<h3 id="核心要素">2.4 核心要素</h3>
<h4 id="domain">2.4.1 Domain</h4>
<p>不同的超参数有自己的取值范围，以及先验分布，例如均匀分布，log均匀分布等</p>
<h4 id="objective-function">2.4.2 Objective Function</h4>
<p>Objective Function是我们想要优化的目标。接受超参数作为输入，输出的值可以是超参数实例化后拟合了训练集的经验风险函数，如均方误差。</p>
<p>我们不会直接拿这个作为优化目标，而是使用下面的替代函数Surrogate Function。</p>
<h4 id="surrogate-function">2.4.3 Surrogate Function</h4>
<p>替代函数，也叫做响应面(response surface)，是基于之前的历史记录的一种关于objective function的概率表示。</p>
<p>叫做响应面是因为它是在高维层次的objective function score的关于超参数的概率。如下图：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20191110145710920.png" style="zoom:50%;" /></p>
<p>存在几种不同的替代函数，如：</p>
<ul>
<li>Gaussian Processes</li>
<li>Random Forest regression</li>
<li><strong>Tree-structured Parzen Estimator</strong></li>
</ul>
<h4 id="selection-function">2.4.4 Selection Function</h4>
<p>选择函数是如何根据前面的Surrogate Function，来选择新的超参数集合进行试验。</p>
<p>关于选择函数也有不同的表示，例如：</p>
<ul>
<li><strong>Expected Improvement (EI)</strong></li>
<li>Probability of Improvement</li>
</ul>
<h2 id="tree-structured-parzen-estimatortpe">3 Tree-structured Parzen estimator(TPE)</h2>
<h3 id="基础认识">3.1 基础认识</h3>
<ol type="1">
<li><p>Tree：超参数优化问题可以理解为在图结构的参数空间上不断寻找objective function最优解的问题。所谓tree，是提出TPE的作者将该优化问题限制在了树状结构上，例如：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20191110163042989.png" style="zoom:50%;" /></p>
<p>一些超参数只有在其它的超参数确定后才能够进行确认，例如网络的层数与每一层的节点数量，当然这不意味着这两个超参数是相关的。<strong>实际上在TPE中，要求所估计的超参数必须是相互独立的</strong>。</p></li>
<li><p>Parzen：<a href="https://en.wikipedia.org/wiki/Kernel_density_estimation">Parzen–Rosenblatt window</a>是在核密度估计问题中，由 <a href="https://en.wikipedia.org/wiki/Emanuel_Parzen">Emanuel Parzen</a> 和 <a href="https://en.wikipedia.org/wiki/Murray_Rosenblatt">Murray Rosenblatt</a>提出的能够根据当前的观察值和先验分布类型，估计估计值的概率密度。一般的函数如下：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20191110163827025.png" style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20191110163935502.png" style="zoom:50%;" /></p>
<p><strong>在TPE中，假设了核函数都是高斯核函数</strong></p></li>
</ol>
<h3 id="具体过程推演">3.2 具体过程推演</h3>
<h4 id="expected-improvement-ei">3.2.1 Expected Improvement (EI)</h4>
<p>对于选择一组新的超参数后的objective function的提升可以表示为：</p>
<p><span class="math display">\[ I(\lambda) = max(c^{*}-c(\lambda), 0) \tag{1}\]</span></p>
<p><span class="math inline">\(c^{*}\)</span>是当前记录的H中所有c的一个分割点，我们这里直接认为c表示的就是风险函数。</p>
<p>通过这样的转换，我们就把原来的objective function最优问题转换成了一个新的问题。</p>
<p>如果新的<span class="math inline">\(\lambda\)</span>对应的<span class="math inline">\(c(\lambda)\)</span>更小，则它是更好的超参数设置，但是这需要训练，然后求出风险函数，并不是我们想要的。</p>
<p>那么如果一个新的<span class="math inline">\(\lambda\)</span>对应的提升的期望是大于0的，那么可认为这个<span class="math inline">\(\lambda\)</span>是有较大可能使得风险减小的。故有：</p>
<p><span class="math display">\[ \begin{split}EI(\lambda) &amp;= \int\limits_{-\infty}^{c^{*}}(c^{*}-c)P_{S_{t-1}}(c|\lambda)dc \\ &amp;= \int\limits_{-\infty}^{c^{*}}(c^{*}-c)\frac{p(\lambda|c)p(c)}{p(\lambda)}dc \end{split} \tag{2} \]</span></p>
<p>式子中的<span class="math inline">\(p(\lambda|c)\)</span>定义为：</p>
<p><span class="math display">\[ p(\lambda|c) = \left\{ \begin{aligned} &amp;l(\lambda) \quad c \lt c^{*} \\ &amp;g(\lambda) \quad c \ge c^{*} \end{aligned} \right. \tag{3} \]</span></p>
<p><span class="math inline">\(c^{*}\)</span>是一个阈值，通常是在H所有的c中，满足<span class="math inline">\(p(c&lt;c^{*})=\gamma\)</span>，<span class="math inline">\(\gamma\)</span>默认可设置为0.15。这样所有的历史记录就分成了两部分，即风险较小的那部分和风险相对较大的那部分，<span class="math inline">\(l(\lambda)\)</span>是由所有的风险较小的那部分的超参数集合形成的分布，<span class="math inline">\(g(\lambda)\)</span>是由所有的风险较大的那部分的超参数集合形成的分布。如下图：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20191110191535439.png" style="zoom: 33%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20191110191405221.png" style="zoom:33%;" /></p>
<p>直觉上我们从风险较小的分布中抽样新的超参数，应该是比较合理的，下面的数学推理也证明了这一点。</p>
<p>继续推导公式2：</p>
<p><span class="math display">\[\begin{split} p(\lambda) &amp;= \int_{R}p(\lambda|c)p(c)dc \\ &amp;= \int_{-\infty}^{c^{*}}p(\lambda|c)p(c)dc + \int_{c^{*}}^{+\infty}p(\lambda|c)p(c)dc \\ &amp;= \gamma l(\lambda)+(1-\gamma) g(\lambda) \end{split}\]</span></p>
<p><span class="math display">\[\begin{split} \int_{-\infty}^{c*} (c^{*}-c)p(\lambda|c)p(c)dc &amp;= l(\lambda)\int_{-\infty}^{c*}(c^{*}-c)p(\lambda|c)p(c)dc \\ &amp;=\gamma c^{*}l(\lambda) -l(\lambda) \int_{-\infty}^{c*} cp(c)dc \end{split}\]</span></p>
<p>最终得到：</p>
<p><span class="math display">\[\begin{split} EI(\lambda) &amp;= \frac{\gamma c^{*}l(\lambda) -l(\lambda) \int_{-\infty}^{c*} cp(c)dc}{\gamma l(\lambda)+(1-\gamma) g(\lambda)} \\ &amp; \propto \big( \gamma+\frac{g(\lambda)}{l(\lambda)}(1-\gamma) \big)^{-1} \end{split}\]</span></p>
<p>这说明在<span class="math inline">\(l(\lambda)\)</span>的分布下取样得到的参数更有可能让<span class="math inline">\(EI(\lambda)\)</span>有更大的值。</p>
<h4 id="估计超参数的分布">3.2.2 估计超参数的分布</h4>
<p>由 <span class="math inline">\(c^{*}\)</span>将历史记录分为了两部分，以 $ c c^{*} $ 的那部分超参数集合为例。</p>
<p>假设存在n个取值 $ (x_1,x_2, , x_n) $ ，那么概率密度的估计是</p>
<p><span class="math display">\[ \hat{p}(x) = \frac{1}{nh}\sum\limits_{i=1}^{n}K\big( \frac{x-x_i}{h} \big) \]</span></p>
<p>其中</p>
<p><span class="math display">\[ K\big( \frac{x-x_i}{h} \big) = \frac{1}{\sqrt{2\pi}} exp\big( -\frac{1}{2}(\frac{x-x_i}{h})^2 \big) \]</span></p>
<h4 id="求解的过程">3.2.3 求解的过程</h4>
<p>利用TPE，首先对于所有的超参数形成的那棵树进行设置，因为TPE要求所有的超参数要相互独立，即一种正交的概念，即效果不能相互影响，例如学习率，提升树方法的迭代次数，和树的最大深度就存在一个权衡，所以需要先固定一些不能一起用于优化的超参数，如固定学习率和迭代次数。</p>
<p>由树从根结点从上而下的利用Parzen estimator，对某一个节点，在 <span class="math inline">\(l(\lambda)\)</span>分布下采样，比如采样100次。</p>
<p>每一次的采样得到的超参数集合可以分别得到它们的概率，然后相乘得到联合概率，算出 $ l()$和 <span class="math inline">\(g(\lambda)\)</span>。选取其中 <span class="math inline">\(l(\lambda)/g(\lambda)\)</span> 的最大值，作为这一次迭代选取的超参数。</p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>ML</category>
      </categories>
  </entry>
  <entry>
    <title>BatchNorm</title>
    <url>/ml/batchnorm/</url>
    <content><![CDATA[<h1 id="batch-normalization-accelerating-deep-network-training-by-reducing-internal-covariate-shift">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</h1>
<p>ICML 2015</p>
<p>谷歌团队的经典论文，batchnorm操作。题目中的Internal Covariate Shift是论文中提出的一个名词，主要是指在网络结构中，由于各层模型参数不同，每一层接受的输入的分布都会改变。这种现象被称作<strong>internal covariate shift</strong>。这篇文章通过对把每层的激活值做归一化处理，提升模型训练速度与效果。</p>
<p>归一化处理会增大feature之间的相对差异，排除绝对差异，因此可能更好训练。另外，归一化操作能够让激活值处于激活函数类似sigmoid的梯度较大的区域，能够缓解梯度消失问题。</p>
<span id="more"></span>
<p>pytorch中的核心公式：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210708161603064.png" style="zoom:50%;" /></p>
<p><span class="math inline">\(\gamma\)</span>和<span class="math inline">\(\beta\)</span>是两个很重要的可学习的参数，它从理论上保证归一化后的值<span class="math inline">\(y\)</span>通过学习合适的<span class="math inline">\(\gamma\)</span>和<span class="math inline">\(\beta\)</span>可以还原原来的<span class="math inline">\(x\)</span>。比如：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210708161844290.png" style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210708161902153.png" style="zoom:50%;" /></p>
<p>在mini-batch训练策略下的核心算法，对第<span class="math inline">\(i\)</span>维的激活值<span class="math inline">\(x\)</span>。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210708161527909.png" style="zoom:50%;" /></p>
<p>从<a href="https://blog.csdn.net/qq_25737169/article/details/79048516">博客</a>上找的python代码实现方便理解。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Batchnorm_simple_for_train</span>(<span class="params">x, gamma, beta, bn_param</span>):</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">param:x    : 输入数据，设shape(B,L)</span></span><br><span class="line"><span class="string">param:gama : 缩放因子  γ</span></span><br><span class="line"><span class="string">param:beta : 平移因子  β</span></span><br><span class="line"><span class="string">param:bn_param   : batchnorm所需要的一些参数</span></span><br><span class="line"><span class="string">	eps      : 接近0的数，防止分母出现0</span></span><br><span class="line"><span class="string">	momentum : 动量参数，一般为0.9， 0.99， 0.999</span></span><br><span class="line"><span class="string">	running_mean ：滑动平均的方式计算新的均值，训练时计算，为测试数据做准备</span></span><br><span class="line"><span class="string">	running_var  : 滑动平均的方式计算新的方差，训练时计算，为测试数据做准备</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">	running_mean = bn_param[<span class="string">&#x27;running_mean&#x27;</span>]  <span class="comment">#shape = [B]</span></span><br><span class="line">  running_var = bn_param[<span class="string">&#x27;running_var&#x27;</span>]    <span class="comment">#shape = [B]</span></span><br><span class="line">	results = <span class="number">0.</span> <span class="comment"># 建立一个新的变量</span></span><br><span class="line">    </span><br><span class="line">	x_mean=x.mean(axis=<span class="number">0</span>)  <span class="comment"># 计算x的均值</span></span><br><span class="line">  x_var=x.var(axis=<span class="number">0</span>)    <span class="comment"># 计算方差</span></span><br><span class="line">  x_normalized=(x-x_mean)/np.sqrt(x_var+eps)       <span class="comment"># 归一化</span></span><br><span class="line">  results = gamma * x_normalized + beta            <span class="comment"># 缩放平移</span></span><br><span class="line"></span><br><span class="line">  running_mean = momentum * running_mean + (<span class="number">1</span> - momentum) * x_mean</span><br><span class="line">  running_var = momentum * running_var + (<span class="number">1</span> - momentum) * x_var</span><br><span class="line"></span><br><span class="line">  <span class="comment">#记录新的值</span></span><br><span class="line">  bn_param[<span class="string">&#x27;running_mean&#x27;</span>] = running_mean</span><br><span class="line">  bn_param[<span class="string">&#x27;running_var&#x27;</span>] = running_var </span><br><span class="line">    </span><br><span class="line">	<span class="keyword">return</span> results , bn_param</span><br></pre></td></tr></table></figure>
<p>论文实际没有完整读一遍，只看了核心算法方便实验，以后找时间从头看一遍。</p>
]]></content>
      <categories>
        <category>Paper</category>
      </categories>
  </entry>
  <entry>
    <title>entropy-softmax</title>
    <url>/ml/entropy-softmax/</url>
    <content><![CDATA[<h1 id="机器学习中的sigmoidsoftmax与entropy">机器学习中的Sigmoid、Softmax与entropy</h1>
<p>这篇文章期望总结与讨论机器学习中常见的sigmoid、softmax函数与entropy熵。</p>
<p>参考资料：</p>
<ol type="1">
<li><a href="https://zh.wikipedia.org/wiki/%E7%86%B5_(%E4%BF%A1%E6%81%AF%E8%AE%BA)">熵，维基百科</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/55016125">sigmoid函数推导，知乎</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/105722023">一文详解Softmax函数，知乎</a></li>
<li><a href="https://zh.wikipedia.org/zh-tw/S%E5%9E%8B%E5%87%BD%E6%95%B0">S型函数，维基百科</a></li>
<li><a href="https://www.zhihu.com/question/274997106/answer/1055696026">信息熵越大，信息量到底是越大还是越小？，知乎</a></li>
<li><a href="https://www.zhihu.com/question/294679135/answer/885285177">softmax和cross-entropy是什么关系？</a></li>
</ol>
<p>总结：</p>
<ol type="1">
<li>sigmoid可以看做是神经网络输出<span class="math inline">\([p,0]\)</span>的softmax变形<span class="math inline">\([e^x/(e^x+1), 1/(e^x+e^0)]\)</span>，只不过由于对应标签1的概率<span class="math inline">\(p\)</span>是我们的期望值，另外一个0不做过多讨论。</li>
<li>softmax+交叉熵基本是绑定的，这是因为会使得loss的计算和求导都更简单。</li>
<li>我们经常使用交叉熵，是因为它作为KL散度的核心变化部分，能够衡量输出分布和真实分布之间的差异。</li>
<li>使用softmax而不是hardmax的目的是期望能够让模型从不同类的预测值上获得更多的梯度。</li>
</ol>
<span id="more"></span>
<h2 id="sigmoid函数">Sigmoid函数</h2>
<p>在机器学习领域，如果在了解完线性回归（linear regression）后，发现线性回归很难拟合非线性的分布；那么你很快能看到一个强大的分类器，逻辑斯蒂回归。</p>
<p>逻辑斯蒂回归，logistics regression，就是在线性回归的输出加上了一个特殊的非线性函数，sigmoid函数（在很多文章，也把sigmoid函数叫做S型函数，而把逻辑斯蒂回归中使用的非线性函数单独称作logistic function）：</p>
<p><span class="math display">\[
f(x)=\frac{1}{1+e^{-x}}=\frac{e^x}{e^x+1}
\]</span> <img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/v2-24758bffbd6a9a5d243ff226cb1e3306_1440w.jpg"  style="zoom:40%;" /></p>
<p>该函数是S型函数的一种，指其函数形状类似于S。S型函数在实数范围内可微，并且只有一个拐点（指函数凹凸发生变化的点）。S型函数还包括了很多其它的函数形式。</p>
<p>sigmoid函数取值在<span class="math inline">\([0,1]\)</span>，常被用来输出单类预测区间在<span class="math inline">\([0,1]\)</span>的任务。sigmoid函数的导数是以他自身为因变量的函数，<span class="math inline">\(f^\prime(x)=F(f(x))\)</span>：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220923161828071.png"  style="zoom:50%;" /></p>
<h2 id="softmax函数">Softmax函数</h2>
<p>如果我们期望进行单标签多类预测，比如某篇文章/某张图片属于什么主题，最后输出的一个序列<span class="math inline">\([0,0,1,0,0,\dots]\)</span>。注意这种情况下，所有类的和是1，仍然是单标签预测。如果是多标签预测，那么会出现多个同时成立的<span class="math inline">\(1\)</span>。</p>
<p>在这种情景中，在模型不变的情况下，试想下我们还可以使用sigmoid函数来预测吗？</p>
<p>模型此时的输出<span class="math inline">\(\bf{x}\)</span>是一个实数向量<span class="math inline">\([0.23472,11.78,-99.99,0.0,\dots]\)</span>，我们可以对每个element分别应用sigmoid函数，那么它可以转化成期望的01预测序列。</p>
<p>但这样做有什么问题？</p>
<p>每个element是独立判别的，比如每个主题都会得到自己的<span class="math inline">\(0-1\)</span>估计，它们的和不能保证是<span class="math inline">\(1\)</span>。这种做法适用于多标签的情况，但不适用于单标签多分类。单标签多分类的概率和应该是1，并且从直觉角度看，不同类之间应该存在信息的互相影响。</p>
<p>为了解决上述问题，softmax是对于sigmoid函数的拓展： <span class="math display">\[
softmax(x_i)=\frac{e^{x_i}}{\sum_{j=1}e^{x_j}}
\]</span> 上述形式和sigmoid进行对比后可以发现，sigmoid函数的分母部分是两个元素和，除了<span class="math inline">\(e^x\)</span>之外多了<span class="math inline">\(1\)</span>。而softmax函数是所有预测元素/概率的<span class="math inline">\(e\)</span>指数和作为总的分母。</p>
<p>从值的角度来看，softmax通过平均，保证了输出值在<span class="math inline">\([0,1]\)</span>。</p>
<p><strong>为什么叫做soft的max？</strong></p>
<p>想一下，我们完全可以直接把最大的那个实数拿出来作为预测结果（这就叫做hard max）。我们为什么非要求和以后，再计算最大实数在和中的占比呢？</p>
<p>因为在很多情况下，我们并不想直接丢掉其它类的预测值，我们往往希望能够获得神经网络对所有类的预测概率。</p>
<p>从优化的角度讲，直接把最大的实数挑出来，那么就只会依据这个实数对应的类进行优化，比如它对应的类不是真实标签，那么优化器会强迫神经网络在接下来对这个类的预测值减小，但是不会同时强迫神经网络对其它标签（包括真实标签）的预测值增大/减小。如果它对应的类是真实标签的话，那么优化器会会强迫神经网络在接下来对这个类的预测值增大，但是不会同时强迫神经网络对其它标签的预测值更小。这种做法不是一种很理想的决策。</p>
<p>另外，softmax对于目标标签的概率输出考虑到了其它类（作为分母）。这样在优化的时候，其它类对应的神经元也能够得到对应的梯度。相反，直接hardmax把最大的挑出来，那就只有最大值对应的神经元可以得到优化了。</p>
<p>接下来讨论<strong>为什么引入指数底<span class="math inline">\(e\)</span></strong>？而不是直接求和？下面解答来自<a href="https://zhuanlan.zhihu.com/p/105722023">一文详解Softmax函数，知乎</a>。</p>
<p><span class="math inline">\(e^x\)</span>的斜率逐渐增加，随着<span class="math inline">\(x\)</span>越来越大，斜率也越来越大。这就导致了，引入<span class="math inline">\(e^x\)</span>会拉大不同预测概率之间的差距，这实际相当于增加了马太效应，即强者越强，一个输出值<span class="math inline">\(z_i\)</span>增加很小的幅度，也会被<span class="math inline">\(e^x\)</span>放大。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(tf.__version__) <span class="comment"># 2.0.0</span></span><br><span class="line">a = tf.constant([<span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>], dtype = tf.float32)</span><br><span class="line"></span><br><span class="line">b1 = a / tf.reduce_sum(a) <span class="comment"># 不使用指数</span></span><br><span class="line"><span class="built_in">print</span>(b1) <span class="comment"># tf.Tensor([0.2 0.3 0.5], shape=(3,), dtype=float32)</span></span><br><span class="line"></span><br><span class="line">b2 = tf.nn.softmax(a) <span class="comment"># 使用指数的Softmax</span></span><br><span class="line"><span class="built_in">print</span>(b2) <span class="comment"># tf.Tensor([0.04201007 0.11419519 0.8437947 ], shape=(3,), dtype=float32)</span></span><br></pre></td></tr></table></figure>
<p>同时，<span class="math inline">\((e^x)^\prime=e^x\)</span>，求导比较方便。</p>
<p>引入指数就没有缺点吗？</p>
<p>当然有，指数函数在<span class="math inline">\(x\)</span>比较大时，会输出过于大的值：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">scores = np.array([<span class="number">123</span>, <span class="number">456</span>, <span class="number">789</span>])</span><br><span class="line">softmax = np.exp(scores) / np.<span class="built_in">sum</span>(np.exp(scores))</span><br><span class="line"><span class="built_in">print</span>(softmax) <span class="comment"># [ 0.  0. nan]</span></span><br></pre></td></tr></table></figure>
<p>在深度学习框架TensorFlow中，因为softmax和交叉熵通常是一起的，因此设置了额外的loss函数同时实现了softmax和交叉熵的计算，避免出现上述情况。</p>
<p>接下来我们要讨论<strong>softmax函数的求导</strong>。</p>
<p><span class="math inline">\(p_i=softmax(x_i)\)</span>函数，分母包括了所有的<span class="math inline">\(x_j\)</span>，而分子只包括<span class="math inline">\(x_i\)</span>。所以我们要分类讨论。</p>
<p>当<span class="math inline">\(j==i\)</span>时，对<span class="math inline">\(x_j\)</span>也就是<span class="math inline">\(x_i\)</span>进行求导，此时分子要参与求导（下面的<span class="math inline">\(z\)</span>就是前面的<span class="math inline">\(x\)</span>）：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220924151830047.png"   style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220924151900166.png"   style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220924152207932.png"  style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220924152223554.png"   style="zoom:50%;" /></p>
<p>上述公式可以写成，<span class="math inline">\(p_i\times (1-p_j)\)</span>，由于<span class="math inline">\(i==j\)</span>，因此最终结果为<span class="math inline">\(p_i-p_i^2\)</span>。</p>
<p>当<span class="math inline">\(j\ne i\)</span>时，对<span class="math inline">\(x_j\)</span>进行求导，分子导数是<span class="math inline">\(0\)</span>：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220924152709867.png"   style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220924152740503.png"  style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220924152803996.png"   style="zoom:50%;" /></p>
<p>最终结果为，<span class="math inline">\(-p_j\times p_i\)</span>。</p>
<p>即，<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220924152907614.png"   style="zoom:50%;" /></p>
<p>softmax的导数形式意外的简单，可以直接利用前馈过程中计算出的结果算出导数。</p>
<p>在使用了softmax之后，我们得到了预测序列<span class="math inline">\([0.11,0.43,0.006,\dots]\)</span>，那么怎么样计算loss呢？</p>
<p>我们首先可以给softmax输出结果加上一个<span class="math inline">\(log\)</span>，这样不改变它的单调性：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220924153634383.png"   style="zoom:50%;" /></p>
<p>那么接下来假设<span class="math inline">\(i\)</span>的真实标签就是<span class="math inline">\(1\)</span>，如果我们让<span class="math inline">\(log(p_i)\)</span>不断增大不就可以了吗？当然，loss一般是越小越好，所以有：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220924154005224.png"  style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220924154019920.png"   style="zoom:50%;" /></p>
<p>记住上面的式子，在推导交叉熵的时候，两者会统一起来。</p>
<h2 id="entropy熵">Entropy熵</h2>
<p>信息论中的熵的概念，由1948年，<a href="https://zh.wikipedia.org/wiki/克劳德·艾尔伍德·香农">克劳德·艾尔伍德·香农</a>將<a href="https://zh.wikipedia.org/wiki/熱力學">熱力學</a>的熵引入，因此也叫做香农熵。熵是对不确定性的度量，不确定性越大，熵越大。</p>
<p>熵的数学定义为： <span class="math display">\[
H(X)=E[I(X)]=E[-ln(P(X))]=E[ln(\frac{1}{P(X)})]
\]</span> 即随机事件/变量，概率的平均期望。</p>
<p>对于有限样本：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220924155220119.png"   style="zoom:50%;" /></p>
<p>在这里<span class="math inline">\(b\)</span>是对数所使用的底，通常是2,自然常数e，或是10。当<span class="math inline">\(b = 2\)</span>，熵的单位是bit；当<span class="math inline">\(b = e\)</span>，熵的单位是nat；而当<span class="math inline">\(b\)</span> = 10,熵的单位是Hart。</p>
<p>投一次硬币，出现的花纹（正反面）这个事件的不确定性是1 bit。</p>
<p>熵和信息量有什么区别？</p>
<p>不能简单的把熵就认为是信息量。事实上熵减才能衡量信息量的增加。我们往一个事件/随机变量当中注入新的信息，比如额外事件的发生，不确定性才会减小。</p>
<p>在信息世界，熵越高，则能传输越多的信息，熵越低，则意味着传输的信息越少。这句话表达的是随机变量能够容纳/表达的信息量的大小和熵是有关的。</p>
<p>香农对于某个确定的事件发生后的信息量的定义，核心是发生概率越小，一旦发生后，信息量越大： <span class="math display">\[
h(x)=-log_2(p(x))
\]</span> 然后介绍下交叉熵，用来衡量两个独立变量的分布差异：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220930164612247.png"   style="zoom:50%;" /></p>
<p>评估变量Q和变量P分布差异的大小，如果两者分布完全一致，KL散度值为0；KL散度值越大，分布差异越大；KL散度值越小，分布差异越小。</p>
<p>在机器学习中，如果我们把P看做是真实分布，Q是模型预测的分布，那么KL散度可以衡量机器学习模型的预测性能。在这种情况下，对KL散度进一步推导：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220930165237326.png"  style="zoom:50%;" /></p>
<p>公式的前半部分是真实分布P的负熵，后半部分就是真实分布P做系数、log预测分布Q的交叉熵（同时包括了真实和预测分布，所以叫做交叉）。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220930173153047.png"   style="zoom:50%;" /></p>
<p>前半部分是个固定常量，只要后半部分越小，KL散度就越小。</p>
<p>在了解到什么是交叉熵之后，我们再回到使用softmax推导出的式子：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220924154019920.png"   style="zoom:50%;" /></p>
<p>对于常常使用one-hot编码标签值的机器学习算法来说，只有正确类标签值是1，其它是0：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220930174029787.png"   style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220930174059045.png"   style="zoom:50%;" /></p>
<p>也就是两者完全等价。</p>
<p>然后使用交叉熵进行求导：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220930174650609.png"   style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220930174806703.png"   style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220930174914329.png"   style="zoom:50%;" /></p>
<p>最后的求导结果，只需要预测值和实际标签就能得到导数。</p>
<p>这就是当拿交叉熵和softmax一起做loss时候的优点，求导更加简单。</p>
<p>另外一点是，当计算出softmax之后，再计算交叉熵： <span class="math display">\[
S= \sum_j y_k\times log(S_j)
\]</span> 如果<span class="math inline">\(S_j\)</span>是softmax输出结果，那么，可以一步到位直接计算<code>logSoftmax</code>：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220930212116078.png"   style="zoom:50%;" /></p>
<p>在pytorch的<code>nn.CrossEntropyLoss()</code>函数实现中，就是直接输入神经网络计算得到的激活值<span class="math inline">\(a_j\)</span>（无需经过<code>Softmax</code>）即可，<code>nn.CrossEntropyLoss()</code>会按照<code>logSoftmax</code>来计算最终的loss</p>
]]></content>
      <categories>
        <category>ML</category>
        <category>Theory</category>
      </categories>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title>prototypical-network</title>
    <url>/ml/prototypical-network/</url>
    <content><![CDATA[<h1 id="prototypical-networks-for-few-shot-learning">Prototypical Networks for Few-shot Learning</h1>
<p>作者为少次学习和零次学习提出了一种新的网络Prototypical network。核心思想是为不同的class定义不同的prototype的表示。这个prototype是有相同class下的所有实例求平均得到的。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220310164356012.png" alt="image-20220310164356012" style="zoom:50%;" /></p>
<span id="more"></span>
<p>直接看核心公式，</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220310164441739.png" alt="image-20220310164441739" style="zoom:50%;" /></p>
<p><span class="math inline">\(S_k\)</span>就是class <span class="math inline">\(k\)</span>下的所有实例。<span class="math inline">\(f_{\phi}\)</span>是某种编码函数，或者叫embedding function，可以为任意合适的方法来产生最后的向量。例如作者就使用了CNN，作为在图像数据集下，few-shot的编码函数。</p>
<p>因此，如果要求某个新的实例<span class="math inline">\(x\)</span>是否属于class <span class="math inline">\(k\)</span>，通过定义距离函数<span class="math inline">\(d(\cdot, \cdot)\)</span>，经过<span class="math inline">\(softmax\)</span>就可求出：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220310164703633.png" alt="image-20220310164703633" style="zoom:50%;" /></p>
<p>距离越大，当然成为class <span class="math inline">\(k\)</span>的概率就越小。</p>
<p>作者在训练的时候，使用了之前工作采用的采样batch的方法，叫做<em>episodes</em>，核心思想是模拟少次学习在test时候的情况，每次train的时候，也只采样几个class，几个shot。具体作者的做法如下：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220310170614007.png" alt="image-20220310170614007" style="zoom:40%;" /></p>
<p>作者额外的证明了一些其它的性质，比如如果距离函数是属于regular Bregman divergences（布雷格曼发散），推测一个点属于class的概率就是上面的softmax结果。简单查了一下，这个Bregman divergences的含义是说，它满足空间中距离所有点最小“距离”的点，就是所有点的平均值。这个条件是当且仅当的。</p>
<p>作者还证明了，如果使用欧式距离作为距离函数的话，求解属于哪个class的公式就等价于一个线性的模型：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220310170123470.png" alt="image-20220310170123470" style="zoom:50%;" /></p>
<p>上面公式中的第一项对于不同的class都是固定的，而对于后面两项：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220310170201612.png" alt="image-20220310170201612" style="zoom:50%;" /></p>
<p>求<span class="math inline">\(x\)</span>属于class <span class="math inline">\(k\)</span>的概率就等价于一个拥有参数<span class="math inline">\(w_k\)</span>和<span class="math inline">\(b_k\)</span>的线性模型。</p>
]]></content>
      <categories>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>theory</tag>
      </tags>
  </entry>
  <entry>
    <title>KGAT</title>
    <url>/recommendation/KGAT/</url>
    <content><![CDATA[<h1 id="kgat-knowledge-graph-attention-network-for-recommendation">KGAT: Knowledge Graph Attention Network for Recommendation</h1>
<p>在结合了知识图谱之后，就形成了一个关系更加丰富的graph neural network，使用GNN的方法来进行最后的预测。这篇论文就是在结合了知识图谱的基础上使用作者之前(2019)发表的neural graph collaborative filtering(ngcf)算法。理解了ngcf这篇论文就很好理解。</p>
<span id="more"></span>
<h2 id="现在结合知识图谱的推荐存在的问题">1. 现在结合知识图谱的推荐存在的问题</h2>
<h3 id="知识图谱出现解决的问题">1.1 知识图谱出现解决的问题</h3>
<p>推荐算法中的协同过滤考虑的根据某个用户的历史数据，寻找可能兴趣相投的群体，来推荐物品。比如在下图中，要给用户推荐物品，那么但从交互的物品来看，都与有交互，那么就可以认为是兴趣相投的用户，根据他们的历史数据来给推荐物品。但如果表示某部电影，的导演也是这部电影的演员，与之间是存在属性上的联系的，这种联系是单纯协同过滤无法解决的。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20191214102753089-8557739.png" style="zoom:50%;" /></p>
<p>为了解决这个问题，加入知识图谱(knowledge graph)，形成了下面的结构。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20191229105855957-8557739.png" style="zoom:50%;" /></p>
<p>在这样的图中，与就关联起来了。同样给推荐物品，就能够找到更多的相似用户来进行推荐。</p>
<p>考虑之前的用户和物品交互矩阵，矩阵当中只存在一种关系，就是用户和物品的交互，并且物品和物品之间是没有直接交互的，如果结合了知识图谱，物品和物品之间就出现了直接的关系，增加了新的实体，矩阵中的关系也就由一个变为了多个。</p>
<p>在这样的条件下，就可以继续利用GNN来进行propagation。</p>
<h3 id="使用gnn的思路">1.2 使用GNN的思路</h3>
<p>在结合了知识图谱之后，就形成了一个关系更加丰富的graph neural network，使用GNN的方法来进行最后的预测。这篇论文就是在结合了知识图谱的基础上使用作者之前(2019)发表的neural graph collaborative filtering(ngcf)算法。理解了ngcf这篇论文就很好理解。</p>
<h2 id="模型结构">2 模型结构</h2>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20191214103645598-8557739.png" style="zoom:50%;" /></p>
<p>主要包括三层：</p>
<ol type="1">
<li>Embedding layer</li>
<li>Attentive embedding propaggation layer</li>
<li>Prediction layer</li>
</ol>
<h3 id="embedding-layer">2.1 Embedding layer</h3>
<p>对于用户-物品可以表示为， <span class="math display">\[
G_1 = \{(u, y_{ui}, i)|u\in U, i\in I, y_{ui}=1\}
\]</span> 知识图谱使用下面的三元组来表示， <span class="math display">\[
G_2 = \{(h, r, t)|h,t\in \epsilon , r\in R\}
\]</span> 这样整个结构也就都成为了一个三元组， <span class="math display">\[
G = \{(h, r, t)|h,t\in \epsilon^{&#39;} , r\in R^{&#39;}\}
\]</span> 在新的结构当中，由于每一个物品都对应到了知识图谱中的实体。所以现在的矩阵变为了<span class="math inline">\(N用户\times\ M^{&#39;}实体\)</span>。</p>
<p>在embeding layer这一层，存在一个loss，知识图谱embeding表示的loss。论文里使用了TransR，来获得知识图谱的embeding。</p>
<p>TransR的原理是实体使用d维embeding，关系表示使用k为embeding，一个连接<span class="math inline">\(&lt;h, r, t&gt;\)</span>在数学上的含义可以是h投影到关系r的k维空间上，加上r的k维表示，得到t的k维投影： <span class="math display">\[
W_re_h+e_r\approx W_re_t,\\ e_h, e_t\in R^d, e_r\in R^k, W_r\in R^{k\times d}
\]</span> <span class="math inline">\(W_r\)</span>是关系r的转换矩阵。这个等式两边越接近越好，这样可以定义一个相似性得分的函数： <span class="math display">\[
g(h,r,t)={\lVert W_re_h+e_r-W_re_t \rVert}^2_2
\]</span> 在整个的矩阵上，对于有关系的<span class="math inline">\(&lt;h, t&gt;\)</span>score越小越好，对于没有关系的<span class="math inline">\(&lt;h, t^{&#39;}&gt;\)</span>score越大越好。</p>
<p>因此可以得到一个损失函数： <span class="math display">\[
L_{KG}=\sum_{(h,r,t,t^{&#39;})}{-ln\sigma (g(h,r,t^{&#39;})-g(h,r,t)) }
\]</span> 在论文的代码实现当中，对于每一个head，取了1个positive，1个negative来计算loss。</p>
<h3 id="attentive-embeding-propagation-layers">2.2 Attentive Embeding Propagation Layers</h3>
<p>实质是在ngcf上加上了attention机制。</p>
<p>对于一个实体<span class="math inline">\(h\)</span>，对于和 <span class="math inline">\(h\)</span> 有关系 <span class="math inline">\(r\)</span> 的集合 <span class="math inline">\(N_h=\{ (h,r,t)|(h,r,t)\in G \}\)</span> 使用propagation： <span class="math display">\[
e_{N_h}=\sum_{(h,r,t)\in N_h} \pi (h,r,t)e_t
\]</span> Attention: <span class="math display">\[
\pi (h,r,t)=(W_re_t)^Ttanh(W_re_h+e_r) \\
\pi (h,r,t)=\frac{exp(\pi (h,r,t))}{\sum_{(h,r^{&#39;},t^{&#39;})\in N_h} \pi (h,r,t)e_t}
\]</span> 理解：</p>
<p><span class="math inline">\(W_re_t\)</span>是实体<span class="math inline">\(e_t\)</span>在关系<span class="math inline">\(r\)</span>空间内的投影，<span class="math inline">\(W_re_h+e_r\)</span>是<span class="math inline">\(e_h\)</span>投影到关系<span class="math inline">\(r\)</span>的k维空间上，加上r的k维表示。如果这两个embeding点积越大，表示这两个向量越相似，对应的权重应该更大。</p>
<p>之后的propagation和aggregation具体方式参考ngcf。</p>
<h3 id="训练方式">2.3 训练方式</h3>
<p>具体的，在训练之前，因为加入知识图谱相当于增加了新的物品，增加了大量的待训练参数，为了加快训练过程，首先利用BPR-MF的方式预训练好用户、物品的embeding，之后再进行kgat的训练。</p>
<p>在训练的时候，在一个epoch里，</p>
<ol type="1">
<li>分batch(1024)训练完embeding propagation，更新所有实体的embeding</li>
<li>分batch‘(2048)训练TransR，更新关系的转换矩阵<br />
</li>
<li>更新attentive embeding</li>
</ol>
<h2 id="experiments">3 Experiments</h2>
<p>研究问题：</p>
<ol type="1">
<li>KGAT和其它模型的比较</li>
<li>KGAT不同模型设置对结果的影响</li>
<li>KGAT对于用户偏好解释性的影响</li>
</ol>
<p>可以发现，这三个问题与NGCF中探究的问题一致。</p>
<p>数据集：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20191214110411611-8557739.png" alt="image-20191229105855957" style="zoom:50%;" /></p>
<p>可以发现，如果加入知识图谱，物品的数量就会极大的增加，这也是为什么建议预训练好用户和物品的embeding。</p>
<p>评估指标： <span class="math display">\[
recall@K,\ ndcg@K,\ K=20
\]</span> 超参数设置：</p>
<ul>
<li>embeding size：64</li>
<li>batch size：1024</li>
<li>early stopping</li>
<li>KGAT layer size：[64, 32, 16]</li>
</ul>
<p>比较结果：</p>
<p>效果比较：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20191229110432499-8557739.png" alt="image-20191229110432499" style="zoom:50%;" /></p>
<p>不同模型结构设置的影响：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20191229111040431-8557739.png" alt="image-20191229110623067" style="zoom:50%;" /></p>
<p>对于用户偏好的解释：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20191229110623067-8557739.png" alt="image-20191229111040431" style="zoom:50%;" /></p>
<p>从这张图上可以看出来通过加入知识图谱，获得了更多的额外信息。</p>
<h2 id="总结">4 总结</h2>
<p>加入知识图谱导致了更加丰富的物品信息</p>
<p>缺点：</p>
<p>在知识图谱中，存在一些常见的属性，例如图4中的English，这些属性有很多物品都具备，但是在attention的计算中，attention的计算仅在当前用户的交互实体下进行，没有考虑全局的情况。</p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>RS</category>
      </categories>
  </entry>
  <entry>
    <title>KGIN</title>
    <url>/recommendation/KGIN/</url>
    <content><![CDATA[<h1 id="learning-intents-behind-interactions-with-knowledge-graph-for-recommendation">Learning Intents behind Interactions with Knowledge Graph for Recommendation</h1>
<p>KGIN，WWW 2021</p>
<p>结合知识图谱，结合图神经网络。在user-item中加入了intent，在KGIN中的intent表现为知识图谱中关系relation的基于自注意力的组合，同时加入差异性约束，使得不同intent的差异增大。Intent的想法很有趣，直观上看本质上还是多头的，类似于集成学习ensemble learning的思想。</p>
<span id="more"></span>
<blockquote>
<p>Knowledge graph (KG) plays an increasingly important role in recommender systems. A recent technical trend is to develop end- to-end models founded on graph neural networks(GNNs). However, existing GNN-based models are coarse-grained in relational modeling, failing to (1) identify user-item relation at a fine-grained level of intents, and (2) exploit relation dependencies to preserve the semantics of long-range connectivity.</p>
<p>In this study, we explore intents behind a user-item interaction by using auxiliary item knowledge, and propose a new model, Knowledge Graph-based Intent Network (KGIN). Technically, we model each intent as an attentive combination of KG , relations, Negative Items : Positive Items encouraging the independence of different intents for better model capability and interpretability. Furthermore, we devise a new information aggregation scheme for GNN, which recursively integrates the relation sequences of long-range connectivity (i.e., relational paths). This scheme allows us to distill useful information about user intents and encode them into the representations of users and items. Experimental results on three benchmark datasets show that, KGIN achieves significant improvements over the state-of-the-art methods like KGAT [41], KGNN-LS [38], and CKAN [47]. Further analyses show that KGIN offers interpretable explanations for predictions by identifying influential intents and relational paths. The implementations are available at <a href="https://github.com/huangtinglin/Knowledge_Graph_based_Intent_Network" class="uri">https://github.com/huangtinglin/Knowledge_Graph_based_Intent_Network</a>.</p>
</blockquote>
<p>直接看方法：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210428152321247.png" style="zoom:50%;" /></p>
<p>整个模型的训练分成两个部分，一个是user-item上对于user embedding的训练，一个是在KG上对于entity/item的训练。</p>
<h2 id="在user-item-graph上的训练">在user-item graph上的训练。</h2>
<p>首先引入intent，定义一个集合<span class="math inline">\(\mathcal{P}\)</span>对于所有的用户是共享的，然后针对user-item pair <span class="math inline">\((u,i)\)</span>，插入intent，变为<span class="math inline">\(\{ (u,p,i)\ p \in \mathcal{P} \}\)</span>。</p>
<p><strong>Representation Learning of Intents</strong>：</p>
<p>每一个intent <span class="math inline">\(p\)</span>对应一个表示embedding，同时为了让intent能够与KG中的relation联系起来，考虑不同relation的组合作用，将intent表示为基于注意力的relation的组合：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210428153437851.png" style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210428153453247.png" style="zoom:50%;" /></p>
<p>KGIN为每个relation针对不同的intent <span class="math inline">\(p\)</span>定义了一个weight，<span class="math inline">\(w_{rp}\)</span>。不是简单的直接weighted sum而是经过了一层softmax。</p>
<p>另外，为了保证intent embedding之间存在差异性，对于不同的intent <span class="math inline">\(e_p\)</span>，使用了两种方法作为惩罚项加入loss function中。</p>
<ol type="1">
<li>Mutual information</li>
</ol>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210428153029551.png" style="zoom:50%;" /></p>
<ol start="2" type="1">
<li>Distance correlation</li>
</ol>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210428152948599.png" style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210428153949318.png" style="zoom:50%;" /></p>
<p>以上的思想是精髓，需要之后细看，并且类似的思想在<a href="#">Post not found: AM-GCN</a>中也有出现。</p>
<p>在得到了intent <span class="math inline">\(p\)</span>之后，聚合item，得到user的表示：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210428153959942.png" style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210428153813909.png" style="zoom:50%;" /></p>
<p>注意力的聚合，计算每个intent与目标用户embedding的相似程度。</p>
<h2 id="在kg上的训练">在KG上的训练</h2>
<p>不使用注意力，也没有intent，甚至也没有<span class="math inline">\(W\)</span>，直接聚合</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210428152740113.png" style="zoom:50%;" /></p>
<p>需要注意：<span class="math inline">\(e_r\)</span>在intent embedding计算中出现，并且看公式的话，它不会随着layer的增加而改变。</p>
<h2 id="model-prediction">Model Prediction</h2>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210428154032654.png" style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210428153420725.png" style="zoom:50%;" /></p>
<h2 id="model-optimization">Model Optimization</h2>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210428154045082.png" style="zoom:50%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210428152227995.png" style="zoom:50%;" /></p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>RS</category>
      </categories>
  </entry>
  <entry>
    <title>MKR</title>
    <url>/recommendation/MKR/</url>
    <content><![CDATA[<h1 id="multi-task-feature-learning-for-knowledge-graph-enhanced-recommendation">Multi-Task Feature Learning for Knowledge Graph Enhanced Recommendation</h1>
<p>WWW 2019</p>
<p>MKR将KG和RS看做两个不同的学习任务，提出了<em>cross&amp;compress</em> unit。能够显式的捕获item和entities之间的高阶交互，并且自动在两个学习任务中控制cross knowledge。</p>
<span id="more"></span>
<blockquote>
<p>Collaborative filtering often suffers from sparsity and cold start problems in real recommendation scenarios, therefore, researchers and engineers usually use side information to address the issues and improve the performance of recommender systems. In this paper, we consider knowledge graphs as the source of side information. We propose MKR, a Multi-task feature learning approach for Knowledge graph enhanced Recommendation. MKR is a deep end-to-end framework that utilizes knowledge graph embedding task to assist recommendation task. The two tasks are associated by cross&amp;compress units, which automatically share latent features and learn high-order interactions between items in recommender systems and entities in the knowledge graph. We prove that cross&amp;compress units have sufficient capability of polynomial approximation, and show that MKR is a generalized framework over several representative methods of recommender systems and multi-task learning. Through extensive experiments on real-world datasets, we demonstrate that MKR achieves substantial gains in movie, book, music, and news recommendation, over state-of-the-art baselines. MKR is also shown to be able to maintain a <strong>decent</strong> performance even if user-item interactions are sparse.</p>
</blockquote>
<p><strong>motivation</strong>：对于以前的很多基于KG的推荐方法，在处理KG的时候存在很多问题。比如PER和FMG，将KG作为一个异质信息网络，需要基于metapath进行推荐；对于RippleNet，没有办法很好的捕获KG中relation的重要性；对于CKE，利用了TransR进行训练，但是TransR更适合in-graph的任务而不是更适用于RS的任务。</p>
<p><strong>Method</strong>：为了解决上面的问题，MKR，将基于KG的推荐问题看做多任务学习问题。multi-task learning (MTL)。在KG上的学习和在推荐上的学习任务不是完全独立的。在文献Learning Multiple Tasks with Multilinear Relationship Networks中提到了，一个item和它对应的entities，在RS和KG上可能具有相近的结构，并且可能在任务无关的特征空间中共享相似的特征。</p>
<blockquote>
<p>Therefore, an item and its corresponding entity are likely to have a similar proximity structure in RS and KG, and share similar features in low-level and non-task-specific latent feature spaces.</p>
</blockquote>
<p>MKR提出了<em>cross&amp;compress</em> unit。能够显式的捕获item和entities之间的高阶交互，并且自动在两个学习任务中控制cross knowledge。</p>
<p>来看一下模型结构。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210502182518838.png" style="zoom:50%;" /></p>
<p>总体而言，两个学习任务，两个学习任务之间的item和entities经过<em>cross&amp;compress</em> unit进行学习，用户user和KG中的relation都是经过MLP进行学习。两个学习任务意味着两个loss。</p>
<p>在训练时，迭代一次，多次训练推荐任务，最后训练一次KG链路预测任务。</p>
<p>那么核心创新点就是<em>cross&amp;compress</em> unit，看一下式子。</p>
<p>Cross部分：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210502182833566.png" style="zoom:50%;" /></p>
<p>对于item和entities进行tensor dot，不同维度之间相乘，捕获pairwise的interaction。</p>
<p>Compress部分：</p>
<p>将feature interaction matrix，投影到item和entities的不同feature space中：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210502183100668.png" style="zoom:50%;" /></p>
<p><strong>需要注意的点</strong>：需要看到，它在投影时，即压缩时，在horizontal和vertical两个方向上同时进行投影。也就是说对于<span class="math inline">\(C_L\)</span>和<span class="math inline">\(C_l^T\)</span>同时投影。对于horizontal方向的压缩，不同行之间的区别在于<span class="math inline">\(\mathbf{v}_l\)</span>，即item的信息；对于vertical方向的压缩，不同列之间的区别在于<span class="math inline">\(\mathbf{e}_l\)</span>，即entities提供的信息。</p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>RS</category>
      </categories>
  </entry>
  <entry>
    <title>NGCF</title>
    <url>/recommendation/NGCF/</url>
    <content><![CDATA[<h1 id="neural-graph-collaborative-filtering">Neural Graph Collaborative Filtering</h1>
<blockquote>
<p><a href="https://arxiv.org/abs/1905.08108">Wang X, He X, Wang M, et al. Neural Graph Collaborative Filtering[J]. arXiv preprint arXiv:1905.08108, 2019.</a></p>
</blockquote>
<p>论文的主要贡献是提出了一种embeding propagation的方式，能够利用high order范围内的实体，训练得到用户和物品的embeding。结合知识图谱做推荐。</p>
<span id="more"></span>
<h2 id="介绍">1 介绍</h2>
<p>协同过滤(CF)有两个关键的点:</p>
<p>一个是如何表示用户和物品(embeding)，embeding的表示在各种方法里都不相同，可以直接使用用户/物品ID表示embeding，也可以利用各种特征，经过神经网络MLP，获得embeding表示。 另一个是如何表示两者的交互(interaction)，在MF中，直接使用用户与物品vector的点积表示交互（即一个鼠标点击广告的动作，或者购买该物品的历史）。</p>
<p>但是多数模型都没有通过利用user-item的交互来训练得到embeding，只使用了用户属性等基本描述性的特征。</p>
<p>如果要利用交互来获得embeding，存在的问题在于若使用user-item矩阵的形式表示交互，这样矩阵规模就非常的大，常常达到几百万量级，而且非常稀疏。</p>
<p>为了解决这个问题，论文中将交互转换为graph的形式，集中注意力在有过交互的物品上，例子如下图：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20191203225032817.png" style="zoom:50%;" /></p>
<p>图中的用户u1是推荐的目标用户。右边的图看成是以<span class="math inline">\(u_1\)</span>为根结点形成的一个树，这样对于<span class="math inline">\(u_1\)</span>的预测，就由原来的<span class="math inline">\(i_1, i_2, i_3\)</span>(first order)拓展到<span class="math inline">\(i_4, i_5\)</span>(third order)这样的high order范围。</p>
<p>论文的主要贡献是提出了一种embeding propagation的方式，能够利用high order范围内的实体，训练得到用户和物品的embeding。</p>
<h2 id="model结构">2 Model结构</h2>
<p>NGCF一共包括三层，</p>
<ol type="1">
<li>Embeding layer：表示user-item的look-up table</li>
</ol>
<p><span class="math display">\[
E=[\underbrace{e_{u_1}, e_{u_2}, \dots}_{N\ users},\underbrace{e_{i_1}, e_{i_1}}_{M\ items} ]^T
\]</span></p>
<ol start="2" type="1">
<li>Embedding Propagation Layers: 利用embeding进行多层的propagation</li>
<li>Prediction layer：预测<span class="math inline">\(&lt;u, i&gt;\)</span>的概率</li>
</ol>
<p>整个网络的结构如图：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20191215094628886.png" style="zoom:50%;" /></p>
<h3 id="embedding-layer">2.1 Embedding layer</h3>
<p>这一层与主流的推荐模型的embeding layer一样，对于N个用户u，M个物品i，使用d维的向量来表示，形成下面的矩阵(<span class="math inline">\((N+M)\times d\)</span>)： <span class="math display">\[
E = [e_{u_1}, \dots, e_{u_N},e_{i_1}, \dots, e_{u_M},]^T, e_u, e_i\ \in R^d
\]</span> 在实现中首次训练使用xavier initializer初始化。</p>
<h3 id="embedding-propagation-layers">2.2 Embedding Propagation Layers</h3>
<h4 id="first-order-propagation">2.2.1 First-order Propagation</h4>
<h5 id="message-construction">2.2.1.1 Message Construction</h5>
<p>对于一个用户u，我们首先可以直接使用那些与用户有直接交互的物品计算用户的embeding，表示为 <span class="math display">\[
m_{u\leftarrow i} = f(e_i, e_u, p_{ui})
\]</span> <span class="math inline">\(m_{u\leftarrow i}\)</span>称为物品i对于用户u的message embeding，<span class="math inline">\(f\)</span>称为message encoding function，<span class="math inline">\(p_{ui}\)</span>是系数，控制<span class="math inline">\(&lt;u, i&gt;\)</span>这条path的权重。</p>
<p>具体的，<span class="math inline">\(m_{u\leftarrow i}\)</span>表现为： <span class="math display">\[
m_{u\leftarrow i} = \frac{1}{\sqrt{|N_{u}||N_{i}|}} \big( W_1 e_i + W_2 (e_i\odot e_u)\big),\ \ W_1, W_2 \in R^{d^{&#39;}\times d}
\]</span> <span class="math inline">\(\odot\)</span>表示element wise乘法，<span class="math inline">\(N_{u},\ N_{i}\)</span>分别表示与用户u和物品i直接有交互的物品或者用户数量。</p>
<p><span class="math inline">\(\frac{1}{\sqrt{|N_{u}||N_{i}|}}\)</span>就是 <span class="math inline">\(p_{ui}\)</span>， 被称作graph Laplacian norm，它表示物品传递给用户的message的weight，可以这样理解，如果某个物品的<span class="math inline">\(N_i\)</span>越小，表示这个物品越”独特“，越能够体现用户的个性偏好， <span class="math inline">\(p_{ui}\)</span>增大；用户的<span class="math inline">\(N_u\)</span>越小表示该用户的兴趣越”集中“，那么他的历史数据中的每个物品的 <span class="math inline">\(p_{ui}\)</span>都应该增大，表示每个物品都能够较大的反映该用户偏好。</p>
<h5 id="message-aggregation">2.2.1.2 Message Aggregation</h5>
<p>对于一个用户u的多个物品i，得到了多个传递过来的message embeding，需要使用一种聚合起来形成最终的用户embeding的方式， <span class="math display">\[
e_u^{(1)} = LeakRelu\big( m_{u\leftarrow u} + \sum_{i \in N_u} m_{u\leftarrow i} \big),\\
m_{u\leftarrow u}=W_1 e_u
\]</span> <span class="math inline">\(e_u^{(1)}\)</span>是用户u经过第一次embeding propagation之后的embeding，可以看到最终聚合的方式是直接对所有的message embeding相加，最后联合原来的表示<span class="math inline">\(e_u\)</span>，经过一个leak-relu就得到了最后的表示。</p>
<p>上面过程以单个用户u为例，介绍了一次embeding propagation的过程。这个过程对于物品i也是一样的。</p>
<p>单个用户进行一次propagation，与用户u直接相连的所有物品的”信息“传递到了用户u上。但这个过程是同时在所有的用户u和物品i都进行的。一次propagation，让每个用户和物品都得到了从与它们直接相连的实体的信息。如果在进行一次propagation，用户和物品目前包含了自己下层直连的信息，就又会传递给上级。也就实现了获取high order连接信息的目的。</p>
<h4 id="high-order-propagation">2.2.2 High-order Propagation</h4>
<p>在first order propagation的基础上，得到多次propagation的表示， <span class="math display">\[
\begin{cases}
e_u^{(l)} = LeakRelu\big( m_{u\leftarrow u}^{l} + \sum_{i \in N_u} m_{u\leftarrow i}^{l} \big),\\
m_{u\leftarrow u}^{l}=W_1^{l} e_u^{(l-1)},\\
m_{u\leftarrow i}^{l} = \frac{1}{\sqrt{|N_{u}||N_{i}|}} \big( W_1^{l} e_i^{(l-1)} + W_2^{l} (e_i^{(l-1)} \odot e_u^{(l-1)})^{(l-1)} \big)
\end{cases}
\]</span></p>
<h4 id="propagation-rule-in-matrix-form">2.2.3 Propagation Rule in Matrix Form</h4>
<p>之前的例子作用于所有的用户和物品，就得到了矩阵形式的表达， <span class="math display">\[
E^{(l)} = LeakRelu \big( (L+I)E^{(l-1)} W_1^{l} + LE^{(l-1)} \odot E^{(l-1)} W_2^{l}\big)
\]</span> 其中<span class="math inline">\(L \in R^{(N+M)\times(N+M)}\)</span>，<span class="math inline">\(L_{ui}=\frac{1}{\sqrt{|N_{u}||N_{i}|}}\)</span>。</p>
<h3 id="model-prediction">2.3 Model Prediction</h3>
<p>经过l次propagation，一个用户u，得到<span class="math inline">\(\{e_u^{1}, \dots ,e_u^{l}\}\)</span>，在本论文里，直接将l个d维的embeding concat到一起。 <span class="math display">\[
e^*_u =e_u^{0}|| \dots ||e_u^{l},\qquad e^*_i =e_i^{0}|| \dots ||e_i^{l}
\]</span> 那么最后对于用户u，物品i的得分通过求内积得到， <span class="math display">\[
\hat{y}_{NGCF}(u,i) = (e^*_u)^T e^*_i
\]</span></p>
<h3 id="optimization">2.4 Optimization</h3>
<p>损失函数为BPR(Bayesian Personalized Ranking) loss， <span class="math display">\[
Loss = \sum_{(u, i, j)\in O}-ln\sigma(\hat{y}_{ui}-\hat{y}_{uj}) + \lambda {\lVert \theta \rVert}_2^2 \\
O = \{ (u, i, j)|(u, i)\in R^+,\ (u, j)\in R^- \}
\]</span> 使用Adam，early stoping。</p>
<p>为了防止过拟合，类似dropout方法，使用了两种dropout：</p>
<ol type="1">
<li>Message dropout：以一定的概率<span class="math inline">\(p_1^{l}\)</span>，在进行第l次embeding propagation时，丢弃一些用户或物品message embeding 。message dropou作用于<span class="math inline">\(E^{(l)}\)</span>。</li>
<li>Node dropout：以一定的概率<span class="math inline">\(p_2^{l}\)</span>，在进行第l次embeding propagation之前，丢弃上次产生的一些用户或物品的embeding 。实际是都使用了0.1概率。node dropout作用于<span class="math inline">\(L\)</span>.</li>
</ol>
<h2 id="eexperiments">3 Eexperiments</h2>
<p>在3个数据集上讨论了3个问题：</p>
<ol type="1">
<li>本文提出的NGCF和其它CF模型的比较</li>
<li>NGCF不同超参数的影响</li>
<li>能够利用high order connectivity信息，对于用户和物品的表示的影响</li>
</ol>
<h3 id="dataset">3.1 Dataset</h3>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20191214230017160.png" style="zoom:50%;" /></p>
<p>使用10-core形式，每个用户至少有10个历史交互数据。</p>
<p>80%的interaction为训练集，20%为测试集。</p>
<h3 id="experimental-settings">3.2 Experimental Settings</h3>
<p>评估指标针对每个用户推荐K个物品，然后计算 <span class="math inline">\(recall@K,\ ndcg@K\)</span>，默认情况下K设置为了20。</p>
<p>一些实验的超参数如下：</p>
<ul>
<li>batch size：1024</li>
<li>embeding size：64</li>
<li>ndcf layer：3，[64, 64, 64]</li>
<li>dropout: 0.1</li>
<li>message out: 0.1</li>
</ul>
<h3 id="rq1-comparison">3.3 RQ1: comparison</h3>
<h4 id="overall-comparison">3.3.1 Overall Comparison</h4>
<p>对比了几个不同的CF算法如下</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20191203155628521.png" style="zoom:50%;" /></p>
<h4 id="comparison-w.r.t.-interaction-sparsity-levels.">3.3.2 Comparison w.r.t. Interaction Sparsity Levels.</h4>
<p>一个用户的推荐效果和这个用户的历史数据数量有很大的关系，如果交互的数量越少，越难推荐合适的物品，针对不同交互量用户分组进行了下图的研究。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20191215095752801.png" style="zoom:50%;" /></p>
<p>图上能够看到在不同的分组下，NGCF都有最好的ndcg@20结果。</p>
<h3 id="rq1-study-of-ngcf">3.4 RQ1: Study of NGCF</h3>
<h4 id="effect-of-layer-numbers">3.4.1 Effect of Layer Numbers</h4>
<p>针对NGCF不同层数产生的效果的研究，NGCF-4虽然在两个数据集上得到了较好的结果，但是提升并不大，而且参数数量增多，训练成本增加，也容易过拟合。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20191215100237965.png" style="zoom:50%;" /></p>
<h4 id="effect-of-embedding-propagation-layer-and-layeraggregation-mechanism">3.4.2 Effect of Embedding Propagation Layer and LayerAggregation Mechanism</h4>
<p>对于embeding propagation的方式，进行了研究，</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20191215095100798.png" style="zoom:50%;" /></p>
<h4 id="effect-of-dropout">3.4.3 Effect of Dropout</h4>
<p>研究在不同数据集下，node和message dropout不同数值对于结果的影响</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20191215100632501.png" style="zoom:50%;" /></p>
<p>结果显示多数情况下，相同概率的node dropout方式好于message dropout，而且node dropout方式得到的最好效果要优于message dropout。</p>
<p>一个可能的原因是node dropout会直接丢弃原来的node，这些node不会产生任何的效果，具有更强的鲁棒性。</p>
<h3 id="rq3-effect-of-high-order-connectivity">3.5 RQ3: Effect of High-order Connectivity</h3>
<p>为了研究利用high-order connectivity是否有效果，在Gowalla测试数据集中，截取6个用户和它们的物品在NGCF-1和NGCF-3下的embeding，利用t-SNE进行探究。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20191215102635413.png" style="zoom:50%;" /></p>
<p>从图上可以看出来，在3-order下，一个用户和它的物品更加倾向形成一聚类，即通过它的物品，能够更好的反映用户的实际情况。这表示利用high-order起到了作用，能够更好的捕获协同信息。</p>
<h2 id="conclusion">4 Conclusion</h2>
<p>论文的主要成果</p>
<ul>
<li>一种新的embeding propagation方式</li>
<li>在三个数据集上进行的不同的研究</li>
</ul>
<p>下一步方向</p>
<ul>
<li>现在每层的neighbor的权重都是一样的，可以考虑加入attention(在作者的下一篇论文KGAT中实现了)</li>
<li>结合知识图谱（KGAT）</li>
<li>结合social network，cross-feature等</li>
</ul>
<p>不足</p>
<ul>
<li>单纯的使用了用户的历史交互信息，用户和物品的其它特征并没有利用，能否结合FM, NFM，得到更加丰富的embeding？</li>
</ul>
]]></content>
      <categories>
        <category>Paper</category>
        <category>RS</category>
      </categories>
  </entry>
  <entry>
    <title>ONN</title>
    <url>/recommendation/ONN/</url>
    <content><![CDATA[<h2 id="背景">1 背景</h2>
<p><a href="https://www.sciencedirect.com/science/article/pii/S0893608019302850?via%3Dihub">ONN(Operation-aware Neural Networks for user response)</a>是2018年腾讯广告算法比赛最优的推荐算法。主要任务是预测用户点击推荐广告的概率(click-through rate, CTR)或者进行其它期望的行为(conversion rate, CVR)。在基本的通用的Base model上，<strong>将PNN与FFM结合起来</strong>，实现了在embedding层的每一个feature对于不同operation（内积或者外积）有不同的表示，之后进入MLP，得到更好的预测结果</p>
<span id="more"></span>
<h2 id="创新点">2 创新点</h2>
<h3 id="针对的问题">2.1 针对的问题</h3>
<p>目前的大多数的模型对于一个feature上的embedding vector进行不同的operation都是使用相同的表示。但是<strong>对于不同的操作，一个feature的最好表示不总是相同的</strong>(<a href="https://dl.acm.org/citation.cfm?doid=2959100.2959134">Juan et al., 2016</a>; <a href="https://dl.acm.org/citation.cfm?doid=1718487.1718498">Rendle &amp; Schmidt-Thieme, 2010</a>)</p>
<h3 id="解决思路">2.2 解决思路</h3>
<p>在基本的通用的Base model上，<strong>将PNN与FFM结合起来</strong>，实现了在embedding层的每一个feature对于不同operation（内积或者外积）有不同的表示，之后进入MLP，得到更好的预测结果</p>
<h2 id="相关的模型">3. 相关的模型</h2>
<h3 id="ffm">3.1 FFM</h3>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/884D03E3-A229-4894-AF6A-177AAE3FE851.jpg" style="zoom:50%;" /></p>
<center>
The architecture of the FFM model.
</center>
<h3 id="base-model">3.2 Base Model</h3>
<p>深度学习引入推荐系统之后，深度学习的优势在于可以获取到特征的高维信息，一般的过程可以概括成以下三步：</p>
<ol type="1">
<li>一个embedding layer，负责将离散的feature映射到更低维的表示形式。
<ul>
<li>对于一个样本的特征表示为<span class="math inline">\(f=[x_0, x_1, \ldots x_m]\)</span></li>
<li>embedding矩阵表示为<span class="math inline">\(M = [V^0, V^1, \ldots V^m]\)</span></li>
<li>两者作积得到的这一步的输出概括为： <span class="math display">\[ e = [V^0x_0, V^1x_1, \ldots V^mx_m] \]</span><br />
其中的<span class="math inline">\(V^i\)</span>是中对应<span class="math inline">\(i\)</span>th feature的那一列，<span class="math inline">\(x_i\)</span>是<span class="math inline">\(i\)</span>th feature的one hot表示。</li>
</ul></li>
<li>对于上一步得到的结果进行operation，这一步可以表示为<span class="math inline">\(f = [o_1(e), o_2(e), \ldots o_l(e)]\)</span>，<span class="math inline">\(o_i\)</span>是表示<span class="math inline">\(i\)</span>th operation，这个operation可以是两个向量内积或者外积，在多数的结构中，这个操作只是一个copy操作，暨不进行任何的操作。这种在embedding向量上进行的操作可以看成是一种初期的特种工程。</li>
<li>第二步得到的结果输入MLP(multi-layer perceptron)，最终输出<span class="math inline">\(\hat{y} = \sigma(\Phi(f))\)</span>，<span class="math inline">\(\sigma\)</span>是sigmoid函数，<span class="math inline">\(\Phi\)</span>是MLP的非线性转换</li>
</ol>
<h3 id="pnn">3.3 PNN</h3>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/8B9A5379-797C-4D1B-AF09-4C4F7D6DFE55.jpg" style="zoom:50%;" /></p>
<center>
The architecture of the PNN model.
</center>
<h2 id="onn结构">4 ONN结构</h2>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/389A9CB8-2500-4FBE-8FC8-5571A91C8D22.jpg" style="zoom:50%;" /></p>
<center>
The architecture of the ONN model.
</center>
<p>可以看到最大的特点在于对于<span class="math inline">\(i\)</span>th feature的one-hot表示转换为embedding后拥有多个表示，在进行不同的operation时，采取了不同的表示形式。</p>
<h3 id="operation-aware-embedding">4.1 Operation-aware embedding</h3>
<p>下面说一下对于<span class="math inline">\(i\)</span>th feature的具体转换过程。 <img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/31572F4F-DF1E-4FEE-A231-B17B92F090C3.jpg" style="zoom:50%;" /></p>
<center>
The structures of the normal embedding layer and the operation-aware embedding layer.
</center>
<p>在上图的左边部分中，<span class="math inline">\(e_i\)</span>是feature的转换后的embedding，<span class="math inline">\(V^i\)</span>代表对应<span class="math inline">\(i\)</span>th feature的embedding matrix，这样得到的<span class="math inline">\(e_i\)</span>就只会有一种表示。<br />
而在ONN改进后的右边部分中，设置了<span class="math inline">\([V^{i,l}, V^{i,l}, \ldots V^{i,l}]\)</span>，其中的<span class="math inline">\(V^{i,l}\)</span>表示对于<span class="math inline">\(i\)</span>th feature的<span class="math inline">\(l\)</span>项操作。这里要注意的是同一种操作比如求内积，<span class="math inline">\(i\)</span>和<span class="math inline">\(j\)</span> 求内积与 <span class="math inline">\(i\)</span>和<span class="math inline">\(p\)</span> 求内积是不一样的操作。 最终的到的<span class="math inline">\(e_i\)</span>就是一个矩阵，<span class="math inline">\([e^1_i, e^2_i, \dots, e^l_i]=[V^{i,l}x_i, V^{i,l}x_i, \ldots V^{i,l}x_i]\)</span>，在进行<span class="math inline">\(k\)</span>项操作时，取出<span class="math inline">\(e^k_i\)</span>表示进行操作。 为了表示在<span class="math inline">\(i\)</span>th feature上进行操作<span class="math inline">\(o\)</span>时要使用的<span class="math inline">\(k\)</span>表示定义为：<span class="math inline">\(\Gamma(o, i)=k\)</span><br />
在实现当中，实际求内积只在用户特征和推荐广告的特征之间进行，</p>
<h3 id="incipient-feature-extraction-layer">4.2 Incipient feature extraction layer</h3>
<p>在上一个的基础上，来看一下对于一个样例 <span class="math inline">\(f\)</span>的最终输出形式是什么。 <span class="math display">\[ f = [e_f, i_f] \]</span> 其中第一项<span class="math inline">\(e_f\)</span>是<span class="math inline">\(f\)</span>的所有特征的embedding vector，表示为: <span class="math display">\[ e_f=[e_1^{\Gamma(o(c, 1), 1)}, e_2^{\Gamma(o(c, 2), 2)}, \dots, e_m^{\Gamma(o(c, m), m)}] \]</span><br />
公式中的<span class="math inline">\(o(c, i)\)</span>是指对<span class="math inline">\(i\)</span>th feature进行copy操作 第二项<span class="math inline">\(i_f\)</span>是表示feature interactions，具体的说是两个feature embedding vector的内积。只求两个向量间的内积是再多的就过于复杂，求内积是在之前的实验中证明了内积效果比外积要好<a href="https://ieeexplore.ieee.org/document/7837964">(Qu et al., 2016）</a>。公式为： <span class="math display">\[ i_f=[p_{1, 2}, p_{1, 3}, \dots, p_{m-1, m}] \]</span> <span class="math inline">\(p_{i, j}\)</span>是指在<span class="math inline">\(i\)</span>th feature和<span class="math inline">\(j\)</span>th feature之间求内积，<span class="math inline">\(p_{i, j}=\big \langle e_i^{\Gamma(o(p, i, j), i)}, e_j^{\Gamma(o(p, i, j), j)} \big \rangle\)</span></p>
<h3 id="mlp">4.3 MLP</h3>
<p>两个hidden layer，最后一个sigmoid输出 两个hidden layer的输出表示为： <span class="math display">\[ l_1=BN(relu(W_1\hat{f}+b_1)) \]</span> loss函数是交叉熵</p>
<h2 id="与其它模型pnn-ffm的关系">5 与其它模型(PNN, FFM)的关系</h2>
<ul>
<li>回顾一下PNN模型的结构，和ONN的主要区别在于embedding layer，ONN实现了operation aware，即一个feature有了多种embedding vector，这样对于不同操作可以选择不同的feature表示。这样在训练时就得到了更好的特征表示。</li>
<li>和FFM模型最大的区别在于ONN加入了MLP，加入了深层网络后，深度网络能够更好的挖掘特征深层次的依赖，能够学习到复杂的特征依赖关系</li>
</ul>
<h2 id="实验">6 实验</h2>
<h3 id="数据集">6.1 数据集</h3>
<ol type="1">
<li>Criteo: 包含45million条用户点击广告的记录，使用了最后5million作为测试集(8:1)，数据集中包括13个连续特征和26个分类特征。通过<span class="math inline">\(discrete(x)=\lfloor 2 \times log(x) \rfloor\)</span>将连续量离散化</li>
<li>腾讯广告数据集: 包含了14天内app推荐广告的记录，用户信息，广告信息以及安装的app的记录。论文使用了39个分类特征，去掉了最后2天噪声过大的数据，使用前11天数据作为训练，第12天的数据作为数据。最终22million训练集，2million测试集(11:1)</li>
</ol>
<h3 id="对比的方面">6.2 对比的方面</h3>
<p>分别从AUC，cross entropy，pearson‘s R以及root mean squared error在线下，线上以及采用不同的operation来进行试验</p>
<h4 id="offline-training-performance-comparison">6.2.1 Offline-training performance comparison</h4>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/AF76B24C-D512-4773-9889-07101C1075D9.png" style="zoom:50%;" /></p>
<p>从上面这两张表格可以看出来的有：</p>
<ul>
<li>PNN这些加入了深度网络的模型效果要优于FM, FFM，说明了深层的模型效果是要优于浅层的网络</li>
<li>FFM优于FM，ONN优于PNN，说明采用了operation aware embedding是优于一般的embedding层的</li>
<li>PNN，DeepFM，ONN优于了DNN，说明了进行求积的操作是有效的。</li>
</ul>
<h4 id="online-training-performance-comparison">6.2.2 Online-training performance comparison</h4>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/75087804-9474-4E93-AA7E-755B4C585275.png" style="zoom:50%;" /></p>
<p>在线上的测试中，每一个测试样例只能够被训练一次，对于FM, PNN这些只有一种表示形式的模型来说，一次epoch就学到比较好的表示是比较难的。ONN依旧取得了最好的效果。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/81E8040F-874A-4D9C-89D8-A7D87F654C5B.jpg" style="zoom:50%;" /></p>
<center>
Model convergence curves on the Criteo dataset.
</center>
<p>从上面的收敛趋势可以看到FFM，ONN这样使用了aware的模型，logloss收敛速度是由于其它模型的。</p>
<h4 id="analysis-of-different-operations">6.2.3 Analysis of different operations</h4>
<p>默认情况下ONN是使用内积作为operation，论文中就inner-product, outer-product, sub-network, inner+outer-product四种operation进行了比较。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/2E4E4961-55CF-49DE-9AD3-B67660A93A59.png" style="zoom:50%;" /></p>
<p>可以看到inner+outer-product获得了最好的结果，但是优势并不明显，考虑到时间和空间复杂性，它并不是一个很好的选择。所以依旧是使用了inner product。 但需要注意的是，sub-network取得的效果也是非常有竞争性的。而且它在Criteo数据集上的AUC指标上取得了很好的效果，这个可以考虑为下一步的研究方向。</p>
<h2 id="总结">7 总结</h2>
<ul>
<li>线上测试的结果表明ONN比较适合于线上的环境。</li>
<li>operation aware这种想法可以考虑应用在其它地方</li>
</ul>
]]></content>
      <categories>
        <category>Paper</category>
        <category>RS</category>
      </categories>
  </entry>
  <entry>
    <title>Foundation-models</title>
    <url>/nlp/Foundation-models/</url>
    <content><![CDATA[<h1 id="on-the-opportunities-and-risks-of-foundation-models">On the Opportunities and Risks of Foundation Models</h1>
<p>2021 斯坦福大学 arxiv</p>
<p>在这篇论文里，斯坦福大学的研究者提出了一个概念“foundation models”用来指代在大规模数据上进行训练，可以用于大范围应用的模型。</p>
<p>下面仅仅是基础概念和发展的笔记，具体请参考论文。</p>
<blockquote>
<p>AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that <strong>are trained on broad data at scale and are adaptable to a wide range of downstream tasks</strong>. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles (e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities, and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.</p>
</blockquote>
<span id="more"></span>
<p>论文中定义的foundation model：</p>
<blockquote>
<p>A foundation model is any model that is trained on broad data at scale and can be adapted (e.g., fine-tuned) to a wide range of downstream tasks; current examples include BERT [Devlin et al. 2019], GPT-3 [Brown et al. 2020], and CLIP [Radford et al. 2021].</p>
</blockquote>
<p>foundation的命名（没有使用大语言模型、预训练模型等名字）主要是想强调模型的影响范围，并且想强调这些model不是能够直接进行各种下游任务，而是需要adaptation（fine-tuning、prompt、 architecture reusing、 embedding reusing等等）。这些模型的一点点改进，几乎可以推进所有NLP领域的进展，甚至是跨研究社区的领域进展，对于社会的法律和道德等方面也有影响，作者在论文称之为“强杠杆作用”（high leverage）。foundation模型的好的方面和坏的方面会被所有采用它的下游任务方法所集成，同时它还具有可解释性弱、可能产生不可预计的错误预测场景等问题。</p>
<p>作者提出，foundation models的出现使得AI的发展进入了新的阶段：</p>
<figure>
<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230208111627037.png" alt="image-20230208111627037" /><figcaption>image-20230208111627037</figcaption>
</figure>
<ul>
<li>machine learning：20世纪90年代开始到2015年，机器学习的算法/模型可以在不同应用通用，但是特征的导出依赖于领域专家的特征工程。机器学习取代了之前的专家知识库等概念，开始引领AI的发展。</li>
<li>deep learning：2015年左右，Yann LeCun提倡的深度学习“deep learning”[Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep Learning. Nature 521, 7553 (2015).] 让模型架构能够在不同应用通用，用来实现自动特征导出，极大的降低了对领域专家特征工程的需求。只需要很少的数据预处理，深度学习模型就可以自动学习high level的feature。</li>
<li>foundation model：2019年开始，随着BERT，GPT-2，T5等模型的出现，证明了模型在大规模数据集上进行训练，可以通过很小的改变适应到一系列和预训练任务独立的下游任务。不仅仅是deep learning模型的架构通用，而是model本身（参数、输出等）就可以在不同任务中通用。同时，跨研究社区、跨模态等应用也开始出现。</li>
</ul>
<figure>
<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230208112855998.png" alt="image-20230208112855998" /><figcaption>image-20230208112855998</figcaption>
</figure>
<p>foundation models的出现依赖于模型结构（Transformer）、大规模在线数据的产生和收集、计算资源的指数级增加、训练方法（自监督学习）等多方面的进展。foundation model体现出的另一个不一样的点是，会直接面向社会公开/部署，让各个领域研究者/公司/个人/政府都可以尝试，因而对社会也产生了影响（环境/偏见/歧视/不可解释等）。</p>
<p>后面更多的论文内容没有记录，前面对于foundation model的探讨讲的不错。</p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>GPT-1</title>
    <url>/nlp/GPT-1/</url>
    <content><![CDATA[<h1 id="improving-language-understanding-by-generative-pre-training">Improving Language Understanding by Generative Pre-Training</h1>
<p>2018-06年，OpenAI，GPT-1</p>
<blockquote>
<p>Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classiﬁcation. Although large unlabeled text corpora are abundant, labeled data for learning these speciﬁc tasks is scarce, making it challenging for discriminatively trained models to perform adequately. <strong>We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative ﬁne-tuning on each speciﬁc task.</strong> In contrast to previous approaches, we make use of task-aware input transformations during ﬁne-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures speciﬁcally crafted for each task, signiﬁcantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9% on commonsense reasoning (Stories Cloze Test), 5.7% on question answering (RACE), and 1.5% on textual entailment (MultiNLI).</p>
</blockquote>
<span id="more"></span>
<p>参考：</p>
<ul>
<li>原论文《<em>Improving Language Understanding by Generative Pre-Training</em>》</li>
<li><a href="https://www.bilibili.com/video/BV1AF411b7xQ/?spm_id_from=333.788">沐神的GPT讲解视频</a></li>
</ul>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220513101404397.png" style="zoom:25%;" /></p>
<p>GPT把Transformer的解码器拿出来在大规模数据集上进行预训练；BERT把transformer的编码器拿过来。BERT-base维度和结构层数和GPT-1是一样的，并且也使用了BooksCorpus数据集进行预训练。</p>
<h2 id="introduction">1 Introduction</h2>
<p>作者希望解决什么问题？</p>
<p>深度学习的模型往往需要足够的标注数据进行训练，但是这往往代表着高昂的人工代价和成本。同时在很多领域没有这么多的标注数据。但是直接利用无标签数据进行信息捕获的方法，是一直以来期望能够实现的方向。即便是有足够标注数据的领域，如果引入通过无监督方式学习到的表示，也可能进一步提升模型效果，比如使用预训练好的word embedding来提升NLP任务结果。</p>
<p>但是目前在如何利用无标签数据这个方向上的方法存在两个问题：</p>
<ol type="1">
<li>从无标签文本当中学习什么样的优化目标是最适合之后进行task transfer的，没有明确的答案。</li>
<li>对如何把学习到的表示转化到特定的任务，没有统一的方法/共识，很多方法往往会针对特定任务进一步改造模型架构，引入更多的参数。</li>
</ol>
<p>作者是用什么方法/思路解决上面的问题的？</p>
<p><strong>思路</strong>：半监督预训练+有监督的任务相关的微调，unsupervised pre-training+supervised ﬁne-tuning</p>
<blockquote>
<p>In this paper, we explore a semi-supervised approach for language understanding tasks using a combination of unsupervised pre-training and supervised ﬁne-tuning.</p>
</blockquote>
<p>最终期望能够在大规模语料上训练好，对于不同的任务（甚至是和语料表示的领域不相关的任务）只需要很小的改动即可。</p>
<blockquote>
<p>Our goal is to learn a universal representation that transfers with little adaptation to a wide range of tasks.</p>
</blockquote>
<p><strong>设计</strong>：预训练（Transformer decoder）+微调（把输入采用traversal-style approaches，变为序列化token的输入，输出在Transformer后加一个线性层和softmax）</p>
<p>使用transformer作为总体的架构，因为它更能够处理长期依赖（想想它是所有token都一起经过attention的），因此在不同的task之间，提供了较好的鲁棒性（设想下不同的task需要的信息是不同的，可能存在于语料的不同地方，那么当然需要预训练的模型尽量把所有的信息都能够记住）。</p>
<p><em>有研究者认为预训练实际上是一个regularization scheme，因此能够让深度学习有更好的泛化性</em>，这里如有兴趣，可以参考paper《Why does unsupervised pre-training help deep learning?》</p>
<h2 id="framework">2 Framework</h2>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220510154323724.png" style="zoom:50%;" /></p>
<p>从上图可以看到，GPT会把各个子任务变为序列的形式，在开始加入一个<span class="math inline">\(Start\)</span>词元，在最后加入一个抽取<span class="math inline">\(Extract\)</span>词元。整个序列输入transformer解码器，然后使用抽取词元的表示经过线性层，作为具体的预测目标。</p>
<ul>
<li>Entailment：指在premise中是不是支持hypothesis，三分类问题</li>
<li>Similarity：两段文本是不是相似，构造两个序列</li>
<li>Multiple Choice：n个答案，构造n个序列输入</li>
</ul>
<h3 id="unsupervised-pre-training">2.1 Unsupervised pre-training</h3>
<p>优化目标，标准的语言模型优化目标，已知前<span class="math inline">\(k\)</span>个token，尝试预测当前token <span class="math inline">\(i\)</span>，<span class="math inline">\(k\)</span>就是窗口大小：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220510154617674.png" style="zoom:50%;" /></p>
<p>已知前面几个token的情况下，预测第i个token出现的概率；这点和BERT中的不太一样；BERT的预训练是采用了带掩码的语言模型，会mask掉任意的word然后进行了预测，做完形填空，这样来看BERT的预训练的语义方向是双向的。</p>
<p>优化目标<span class="math inline">\(L_1\)</span>是整个语料<span class="math inline">\(U\)</span>各个token出现的概率，一般情况下概率之间应该是相乘的，这里使用<span class="math inline">\(log\)</span>之后，就变成了多个概率相加。相加的操作比相乘更容易并行，计算也更简单。<span class="math inline">\(k\)</span>越大的话，整个模型会月倾向于使用更长的文本中寻找信息。</p>
<p>输入是context token（不太清楚具体指什么vector，难道就是简单的token编码，比如one-hot？）。</p>
<p>使用的模型是transformer的解码器，因为transformer的编码器是可以看到整个文本的，而解码器是只会看到前面的词，因此在GPT里还是使用了transformer的解码器。结构是12层+768 dimensional states+12 attention heads，输出是预测是哪个token的概率：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220510154934018.png" style="zoom:50%;" /></p>
<p>其中的<span class="math inline">\(W_e\)</span>是指token embedding组成的矩阵。</p>
<h3 id="supervised-ﬁne-tuning">2.2 Supervised ﬁne-tuning</h3>
<p>输入是token，对于某些任务，比如问题-回答等，需要把原来结构化的输入变为与序列化的输入。</p>
<p>对于输入<span class="math inline">\(&lt;x^1,\cdots,x^m&gt;\)</span>，放入transformer里，获得最后一层的位置<span class="math inline">\(m\)</span>输出的表示。</p>
<p>参数就是前一步train好的Transformer，后面再加一线性层，最后经过softmax获得预测的标签<span class="math inline">\(y\)</span>：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220510155720266.png"  style="zoom:50%;" /></p>
<p>优化的目标函数：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220510155835380.png"  style="zoom:50%;" /></p>
<p>作者同样是有了辅助目标函数来增加有监督模型泛化性（让有监督模型不仅仅是只看重一个优化目标），加速收敛（不仅仅依赖于微调对于pre-train model的updating）。但是作者在后面的实验部分发现，辅助函数主要是对规模比较大的有监督的数据集效果比较好，而对于规模比较小的数据集效果不如移除辅助目标函数。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220510160042465.png" style="zoom:50%;" /></p>
<p>在优化task相关的目标loss的同时，也进一步优化之前pretrain的目标loss。</p>
<h2 id="experiments">3 Experiments</h2>
<p>在BooksCorpus数据集上进行了预训练，7000篇没有被发表的书。</p>
<p>作者用到了大量的训练上的trick，这里不提。有一部分是我不了解的，也没有使用过。</p>
<p>总的来说，在4个任务atural language inference，question answering，semantic similarity， text classiﬁcation，一共12个数据集上的9个数据集取得了最好的结果。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220510160500198.png" style="zoom:50%;" /></p>
<p>具体的结果也不粘贴了，参考论文。</p>
<p>主要看一下消融实验：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220510160616495.png" style="zoom:50%;" /></p>
<p>证明了：</p>
<ul>
<li>加入辅助目标函数，帮助模型在规模更大的数据集上取得了更好的效果</li>
<li>使用Transformer而不是LSTM，帮助模型取得了更好的效果（除了在MRPC数据集上）</li>
<li>如果不使用预训练，在所有数据集下都出现了严重的效果下降</li>
</ul>
<h2 id="conclusion">Conclusion</h2>
<blockquote>
<p><strong>Our work suggests that achieving signiﬁcant performance gains is indeed possible, and offers hints as to what models (Transformers) and data sets (text with long range dependencies) work best with this approach.</strong> We hope that this will help enable new research into unsupervised learning, for both natural language understanding and other domains, further improving our understanding of how and when unsupervised learning works</p>
</blockquote>
]]></content>
      <categories>
        <category>Paper</category>
        <category>Pretrain</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>GPT-2</title>
    <url>/nlp/GPT-2/</url>
    <content><![CDATA[<h1 id="language-models-are-unsupervised-multitask-learners">Language Models are Unsupervised Multitask Learners</h1>
<p>GPT-2 OpenAI，15亿参数量，2019-02</p>
<blockquote>
<p>Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspeciﬁc datasets. <strong>We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText.</strong> When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underﬁts WebText. Samples from the model reﬂect these improvements and contain coherent paragraphs of text. These ﬁndings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.</p>
</blockquote>
<p>遗憾的是尽管比BERT-large的3.5亿参数量还要大，但是效果并没有超过BERT。因此在GPT-2主要在zero-shot设置下进行探究。</p>
<span id="more"></span>
<h2 id="introduction">1 Introduction</h2>
<p><strong>问题</strong>：之前出现的在大规模数据集上训练大模型的方法，对数据集的分布以及特定的任务很敏感。这些系统的适应面积还比较狭窄。作者希望能够进一步推动这些大模型的泛化性，往最终的理想目标更进一步：不需要给每个任务都创建和人工标注数据集。</p>
<p><strong>假设</strong>：作者认为在特定领域下进行task相关的训练，是造成目前众多系统泛化性受到约束的原因。比如目前出现的利用多任务学习提升模型泛化性的方法，即在训练的时候可以同时看到不同任务相关的数据集，从元学习的角度看，每个任务(dataset, objective)可以看做是一个training sample。如果要让一个ML系统的泛化性足够好，可能需要成百上千的多任务。当然这是很难一直拓展的下去的。</p>
<p>另外，对于已有的预训练+微调的模式，对于不同的任务仍然需要有标注好的数据。</p>
<blockquote>
<p>Our suspicion is that the prevalence of single task training on single domain datasets is a major contributor to the lack of generalization observed in current systems.</p>
</blockquote>
<p><strong>方法</strong>：作者直接让训练好的语言模型能够在下游的zero-shot的任务下进行预测，不需要任何结构和参数上的调整。核心在于两点：（1）足够大、足够多样化、质量较可靠的大规模数据集WebText（2）整体参数量达到了15亿的基于Transformer的模型（和GPT-1整体架构一样，只不过是层数更多，并且采用了一些新的训练trick）。</p>
<h2 id="approach">2 Approach</h2>
<p>在GPT-1里，对于要预测的任务加入了词元（START、EXTRACT和delimiter），这些词元是在无监督的时候没有见过的词元，但是因为会进行task-specific的微调，所以这些词元回去尝试学到合适的表示。但是现在GPT-2要做zero-shot的设置，那么就不能在不同task下，加入没见过的词元。因此在GPT-1中的词元的引入就不再合适了。</p>
<p>也就是说要让输入的序列，变得更像自然语言。</p>
<p>比如做机器翻译：(translate to french, english text, french text)。第一个输入<em>translate to french</em>，就叫做提示prompt。</p>
<p>为什么加入这样自然语言描述的prompt就能够让机器执行相应的任务？作者认为训练好的大模型应该有能力学习到这样的推理能力。</p>
<blockquote>
<p>Our speculation is that a language model with sufficient capacity will begin to learn to infer and perform the tasks demonstrated in natural language sequences in order to better predict them, regardless of their method of procurement.</p>
</blockquote>
<p>比如下面在数据集里关于法语翻译成英语的例子：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220513151520436.png" style="zoom:30%;" /></p>
<p>可以猜想，如果语料库确实质量好，数据量足够大，那么对于不同的任务应该可以理解到不同的含义。</p>
<h3 id="training-dataset">2.1 Training Dataset</h3>
<p>作者构建了尽可能大，尽可能包括更多领域的数据。</p>
<blockquote>
<p>Our approach motivates building as large and diverse a dataset as possible in order to collect natural language demonstrations of tasks in as varied of domains and contexts as possible.</p>
</blockquote>
<p>作者自己从Reddit上导出了网页内容，为了保证数据质量，人工的过滤/审核抓取的网页内容是不实际的。因此作者从Reddit上爬取的数据是至少有3个karma的帖子（karma是佛教里业力、报应值的意思，在用户对Reddit的帖子进行点赞投票的时候，会获得相应的karma），表明至少这些内容是有意义或者是有趣的。最后的WebText，在2017年12月之后，经过了数据清洗，去重之后，包括了超过800万的文档，总共40G的数据。</p>
<h3 id="input-representation">2.2 Input Representation</h3>
<p>模型对输入文本的要求会限制模型的泛化性。</p>
<p>作者采用了Byte Pair Encoding (BPE)的方法在word-level和byte-level上进行平衡。word-level的编码效果比较好，而byte-level的泛化性强。</p>
<p><em>这一部分没看懂</em>。</p>
<h3 id="model">2.3 Model</h3>
<p>继续沿着OpenAI GPT model的架构，但是做了几个稍稍的改进：</p>
<ul>
<li>把layer normalization移到每个block的输入部分；并且在最后的self-attention block输出加了一个额外的layer normalization、</li>
<li>残差层的参数初始化值考虑到了网络的深度，乘以系数<span class="math inline">\(1/\sqrt{N}\)</span>，<span class="math inline">\(N\)</span>代表残差层的深度。</li>
</ul>
<p>根据网络的深度，设计了四个大小不同的模型：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220511193814757.png" style="zoom:50%;" /></p>
<p>最大的模型参数量比GPT-2大了14倍。</p>
<h2 id="experiments">3 Experiments</h2>
<p>涉及到的方面很多，很多任务具体不是很了解，看一下总体的情况：</p>
<p>在zero-shot设置下，不需要经过有监督的训练，直接在8个数据集中的7个数据集获得了SOTA：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220511194124158.png" style="zoom:50%;" /></p>
<p>GPT-2在Children’s Book Test（类似于完形填空）、LAMBADA（文本长依赖建模能力，预测长句子的最后一个单词）、Winograd Schema challenge（对含糊不清的文本进行常识推理）等task都取得了SOTA的结果。</p>
<p>但是在一些其它的task表现不太好，特别是翻译和QA，表现出来的效果并没有比之前非常粗浅的方法效果好，距离已有的SOTA方法差距很远。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220513151837927.png" style="zoom:40%;" /></p>
<p>需要注意的是，即便是这么大的模型，作者认为仍然是欠拟合（underfit）WebText的：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220511195003857.png" style="zoom:30%;" /></p>
<p>随着模型增大，在WebText上的效果一直在变好，如果继续增大模型，效果应该会进一步增加。纵轴是困惑度（perplexity），越小越好，下面的它的常用的对数形式（来自<a href="https://www.zhihu.com/question/58482430">知乎</a>）：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220511195728092.png" style="zoom:50%;" /></p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>Pretrain</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>GPT-3</title>
    <url>/nlp/GPT-3/</url>
    <content><![CDATA[<h1 id="language-models-are-few-shot-learners">Language Models are Few-Shot Learners</h1>
<p>GPT-3，NIPS 2020 技术报告63页，不是投稿的论文，OpenAI，2020-05</p>
<blockquote>
<p>Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by ﬁne-tuning on a speciﬁc task. While typically task-agnostic in architecture, this method still requires task-speciﬁc ﬁne-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions – something which current NLP systems still largely struggle to do. <strong>Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art ﬁne-tuning approaches.</strong> Speciﬁcally, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or ﬁne-tuning, with tasks and few-shot demonstrations speciﬁed purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-ﬂy reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3’s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we ﬁnd that GPT-3 can generate samples of news articles which human evaluators have difﬁculty distinguishing from articles written by humans. We discuss broader societal impacts of this ﬁnding and of GPT-3 in general.</p>
</blockquote>
<p>GPT-3比GPT-2强调的zero-shot的设置，稍微回退了一点，变为强调few-shot的设置。</p>
<span id="more"></span>
<h2 id="introduction">1 Introduction</h2>
<p>最近NLP领域热衷于研究pre-trained的语言表示，寻找能够实现task-agnostic的方法。</p>
<p>一开始研究者学习word的表示，然后作为task-specific的模型的输入。</p>
<p>最近出现了预训练+fine-tunning的模式，取得了很大的进展。</p>
<p><strong>问题</strong>：进行任务相关的微调，可能带来下面几个缺点：</p>
<ol type="1">
<li><p>fine-tuning需要为不同的任务收集有监督的数据。每一个新任务都需要收集新的数据，这当然限制了模型的表达能力</p></li>
<li><p>fine-tuning的操作，会使得之前pre-training好的model向着一个狭隘的训练分布拟合，导致实际上更大的模型不一定会在训练分布之外表现出更好的泛化性，也就没有办法真正的评估出预训练模型的好坏</p></li>
<li><p>从人类的角度来看，学习特定的语言任务并不需要太多的数据，比如一个简单的描述就可以让人类进行对应的行为。这使得人类可以很快的学会大量不同的任务，并且同时无缝的执行不同的语言任务。我们期望语言模型也能够拥有这样的易变性（ﬂuidity）和普遍性（generality）</p></li>
</ol>
<p><strong>解决思路</strong>：</p>
<p>一方面近期出现了一些在NLP上进行元学习的方法，比如“in-context learning”，把预训练模型的输入，构造出不同task的形式，让模型能够在训练的时候学习到广泛的模式识别能力。</p>
<blockquote>
<p>Recent work [RWC +19] attempts to do this via what we call “in-context learning”, using the text input of a pretrained language model as a form of task speciﬁcation: the model is conditioned on a natural language instruction and/or a few demonstrations of the task and is then expected to complete further instances of the task simply by predicting what comes next.</p>
</blockquote>
<p>下面这张图是对于语言模型可能在进行潜在的元学习的说明，并不是GPT-3真正的训练过程。比如不同的document有不同的context，是在讨论不同的领域。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220512194903403.png" style="zoom:50%;" /></p>
<p>但是这一方向的元学习方法效果还远远不够好。</p>
<p>另一方面，最近出现了一系列基于transformer的大模型，随着规模增大，效果逐渐提升。</p>
<p>那么作者的思路就是结合两者：</p>
<p>如果也将元学习的模型规模扩大，可能它的能力也会得到提高。</p>
<blockquote>
<p>Since in-context learning involves absorbing many skills and tasks within the parameters of the model, it is plausible that in-context learning abilities might show similarly strong gains with scale.</p>
</blockquote>
<p>虽然作者提到了元学习的概念，但是这里的n-shot的example，并不会像一般的元学习方法一样去更新参数，在GPT-3的n-shot的example仅仅是作为输入，用来提示必要的信息。</p>
<p>这篇文章作者进行了以下的探究和贡献：</p>
<ul>
<li>提出了拥有1750亿参数量的GPT-3，重点在于探究它的元学习能力。参数量是GPT-2的116倍；GPT-1的1600倍左右。</li>
<li>对GPT-3的性能，分别在zero-shot、one-shot和few-shot的设置下进行了大量的实验。讨论了GPT-3表现优越的地方，和表现不好的情况。</li>
<li>研究了在大规模语料中出现的数据污染问题（data contamination），即测试集中的内容可能在训练集中出现，特别是因为很多数据是收集于web上。发展出一套评估数据污染情况和后果的工具。</li>
</ul>
<h2 id="approach">2 Approach</h2>
<p>GPT-3使用的几种不同的实验设置：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220512193531024.png" style="zoom:40%;" /></p>
<p>最终效果显示，GPT-3在少次任务下表现相对最好，甚至在某些情况下超过了SOTA的模型；在one-shot和zero-shot的情况下，很多的任务表示promising，没有达到SOTA。</p>
<blockquote>
<p>Broadly, on NLP tasks GPT-3 achieves promising results in the zero-shot and one-shot settings, and in the the few-shot setting is sometimes competitive with or even occasionally surpasses state-of-the-art (despite state-of-the-art being held by ﬁne-tuned models).</p>
</blockquote>
<p>在一个移除随机符号的NLP任务下，不同设置的GPT-3的性能：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220512195107205.png" style="zoom:40%;" /></p>
<p>作者也发现，GPT-3在一些任务，比如自然语言推理以及阅读理解的部分数据集下表现不好。</p>
<blockquote>
<p>At the same time, we also ﬁnd some tasks on which few-shot performance struggles, even at the scale of GPT-3. This includes natural language inference tasks like the ANLI dataset, and some reading comprehension datasets like RACE or QuAC.</p>
</blockquote>
<h3 id="model-and-architectures">2.1 Model and Architectures</h3>
<p>沿用GPT-2的架构。区别是使用了Sparse Transformer中的一些机制：</p>
<blockquote>
<p>we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer.</p>
</blockquote>
<p>实现的不同大小的模型：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220512194115923.png" style="zoom:50%;" /></p>
<p>更大的模型，使用了更大的batch size和更小的学习率。猜测是大模型拟合能力更强，容量更大，因此扩大batch size、减小learning rate都是避免模型拟合太快</p>
<h3 id="training-dataset">2.2 Training Dataset</h3>
<p>大规模语料，来自多个不同的数据集，由于规模大，整个语料基本上一个sequence只需要训练一次即可。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220512194411891.png" style="zoom:50%;" /></p>
<p>这里需要特别注意Common Crawl，原始版本包括了万亿级别单词，但是数据质量比较低。因此作者对它进行了一系列的数据清洗，最后得到了大概4100亿token，大小570 GB。</p>
<ol type="1">
<li>把Common Crawl数据集下载下来，然后使用一个简单的线性回归分类器，正类是高质量的数据集，比如GPT-2的WebText里的数据；负类是Common Crawl中原始的数据。然后对所有的Common Crawl中数据进行预测，如果分类到了正，就认为质量还可以，否则的话就过滤掉。</li>
<li>去重，使用LSH算法判断两个document是不是重合的。</li>
<li>把多个高质量的数据集加进去</li>
</ol>
<p>在训练的时候，根据数据集的质量来采样，而不是根据数据集的大小。</p>
<p>在训练大规模model的时候，大规模语料可能在无意间包含了下游任务的测试信息，因此作者根据所有测试任务的验证和测试集，对训练语料进行了数据清洗。</p>
<blockquote>
<p>To reduce such contamination, we searched for and attempted to remove any overlaps with the development and test sets of all benchmarks studied in this paper.</p>
</blockquote>
<p>但是GPT-3的初始版本训练的数据集没有完全过滤掉污染的数据，并且由于训练代价，没法再次进行训练。</p>
<blockquote>
<p>Unfortunately, a bug in the ﬁltering caused us to ignore some overlaps, and due to the cost of training it was not feasible to retrain the model.</p>
</blockquote>
<h2 id="results">3 Results</h2>
<p>不同大小的模型，计算量和验证集上的损失下降的趋势。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220513160142526.png" style="zoom:50%;" /></p>
<p>可以看到，对于单个模型，最好的方案是在每个loss和计算量相对平衡的点，继续进行运算，loss基本处于收敛的情况。如果把各个模型的平衡点相连，大致上是一个power-law的分布。也就是说，找到一个合适的模型，不过度训练的情况下，随着计算量呈指数增长，模型精度大概是线性下降的趋势。</p>
<h2 id="limitations">5 Limitations</h2>
<ol type="1">
<li>虽然效果比GPT-2要好很多，但是在文本生成任务上还是较弱。GPT-3擅长描述一段的文本，但是不擅长写更多、更长的文本。</li>
<li>结构和算法上的局限性，比如只能从前向后看。然后比如是在预测的时候，对于每一个token都是一样的处理，没有提取最重要的token。并且也只能学习文本的信息，并没有对物理世界的各种其它信息的感知。</li>
<li>样本有效性不够，使用了过多过大的语料。</li>
<li>不确定性，不确定GPT-3到底是在推理的时候真的从头学习到了怎么进行新的任务，还是说只是识别出了之前在预训练的时候见过的模式。</li>
<li>训练代价昂贵。</li>
<li>无法解释。</li>
</ol>
<h2 id="broader-impacts">6 Broader Impacts</h2>
<ol type="1">
<li>可能会被用来做坏事，假新闻造谣、论文造假等等</li>
<li>公平、偏见（性别、种族、宗教等等）。比如GPT-3更可能认为某个角色是男性</li>
<li>能耗</li>
</ol>
<h2 id="conclusion">Conclusion</h2>
<blockquote>
<p>We presented a 175 billion parameter language model which shows strong performance on many NLP tasks and benchmarks in the zero-shot, one-shot, and few-shot settings, in some cases nearly matching the performance of state-of-the-art ﬁne-tuned systems, as well as generating high-quality samples and strong qualitative performance at tasks deﬁned on-the-ﬂy. We documented roughly predictable trends of scaling in performance without using ﬁne-tuning. We also discussed the social impacts of this class of model. Despite many limitations and weaknesses, these results suggest that very large language models may be an important ingredient in the development of adaptable, general language systems.</p>
</blockquote>
]]></content>
      <categories>
        <category>Paper</category>
        <category>Pretrain</category>
      </categories>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>LM-as-KB-Survey</title>
    <url>/nlp/LM-as-KB-Survey/</url>
    <content><![CDATA[<h1 id="a-review-on-language-models-as-knowledge-bases">A Review on Language Models as Knowledge Bases</h1>
<p>arxiv 2022，Meta AI的一篇关于把预训练语言模型看做是知识库KB的综述。</p>
<blockquote>
<p>Recently, there has been a surge of interest in the NLP community on the use of pretrained Language Models (LMs) as Knowledge Bases (KBs). Researchers have shown that LMs trained on a sufﬁciently large (web) corpus will encode a signiﬁcant amount of knowledge implicitly in its parameters. The resulting LM can be probed for different kinds of knowledge and thus acting as a KB. This has a major advantage over traditional KBs in that this method requires no human supervision. In this paper, we present a set of aspects that we deem an LM should have to fully act as a KB, and review the recent literature with respect to those aspects.1</p>
</blockquote>
<span id="more"></span>
<h2 id="introduction">1. Introduction</h2>
<p>在大规模语料上训练得到的语言模型LM已经展现出了拥有以下几种知识：</p>
<ul>
<li>world knowledge</li>
<li>relational knowledge</li>
<li>commonsense knowledge</li>
<li>linguistic knowledge</li>
<li>actionable knowledge</li>
</ul>
<p>但是LM的知识是隐式的，导致它的知识可访问性和可解释性都比较差。和之相对的，知识库knowledge base使用显式的符号知识表达现实世界，它的知识访问、更新和可解释性都更强。因此，研究者开始探究怎么样能够像KB一样控制LM中学习到的知识呢？该问题在论文《Language Models as Knowledge Bases? 》（EMNLP 2019）中首次提出。</p>
<p>这篇survey从LM-as-KBs的6个角度进行了综述：Access, Edit, Consistency, Reasoning, and Explainability and Interpretability。</p>
<h2 id="accessing-knowledge">2. Accessing Knowledge</h2>
<p>KB可以使用查询语言或者查询工具很简单的找到对应的knowledge，但是对于LM来说要找到对应的knowledge更难。有两种主要的方式访问LM中的知识：finetuning和prompting。</p>
<h3 id="finetuning">2.1 Finetuning</h3>
<p>在下游任务上直接微调预训练模型的参数，当然是最直接使用预训练隐式知识的方法。有研究者发现，大多数的知识还是在预训练过程中学习到的，而finetuning仅仅是学习到了一个访问该知识的接口（<em>Analyzing commonsense emergence in few-shot knowledge models. 21</em>）。</p>
<h3 id="prompting">2.2 Prompting</h3>
<p>prompting是一种让任务来适应模型的方法，不需要改变LM的模型参数，只需要找到合适的任务提示。作者把prompting分为discrete prompt和soft prompt两种。</p>
<p>discrete prompting是指直接使用自然语言/token进行描述；</p>
<p>soft prompting是指使用词向量进行提示；有研究者发现soft prompts的表达能力比discrete prompting更强（<em>Learning how to ask: Querying lms with mixtures of soft prompts. 21</em>）。<em>之前没有仔细了解过prompting，此结论不确定是否正确</em>。</p>
<h2 id="consistency">3. Consistency</h2>
<p>一致性是LM作为KB要面临的重要挑战之一。在下面三个情况下都需要考虑一致性：</p>
<h3 id="paraphrase">3.1 Paraphrase</h3>
<p>改写paraphrase，相同的意思使用不同句子/词去表达。LM对于不同paraphrase prompt的输出结果应该是一致的。</p>
<blockquote>
<p>Bhagat and Hovy (2013) [<em>What Is a Paraphrase? 13</em>] deﬁne the term quasi-paraphrases as ‘sentences or phrases that convey approximately the same meaning using different words’.</p>
</blockquote>
<h3 id="commonsense">3.2 Commonsense</h3>
<p>LM的另一个一致性体现在对于学习到的知识的一致性。研究者发现LM可能对于negation词（如not）是不鲁棒的。比如一个LM能够同时学习到“Birds can fly”和“Birds cannot fly”两个矛盾知识（<em>Negated and misprimed probes for pretrained language models: Birds can talk, but cannot ﬂy. 20</em>）。</p>
<p>LM对于蕴含entailment知识也应该是一致的（个人理解，entailment就是指当我们提到了一个知识成立的时候，它内部包括的知识也应该都成立），比如蛇是脊椎动物“A viper is a vertebrate”蕴含了另一个知识蛇有大脑“A viper has a brain”。（<em>Do Language Models Have Beliefs? Methods for Detecting, Updating, and Visualizing Model Beliefs. 21</em>）</p>
<h3 id="multilingual">3.3 Multilingual</h3>
<p>LM对于不同语言描述的同一个查询，应该给出相同的输出。</p>
<h2 id="model-editing">4. Model Editing</h2>
<p>知识库中的知识很简单的就可以被修改和更新，但是LM学习到的知识要更新/编辑就比较困难了。De Cao等人提出一个editing方法应该满足以下三点：</p>
<ul>
<li>Generality：editing方法不应该局限在某个具体的LM模型</li>
<li>Reliability：editing方法应该只影响要修改的知识，不能影响其它的知识。</li>
<li>Consistency：editing方法修改之后，应该保证对于各种语义相同的输入给出相同的输出，也就是要保证前面说的一致性要求。这就要求editing方法既要修改不正确的隐式知识，和修改的事实关联的所有事实也需要被修改，同时其它的事实保持不变。</li>
</ul>
<p>现在的editing方法主要有三类：</p>
<ul>
<li>finetuning：最粗暴的方法是直接让模型针对新的知识进行从头学习，但这由于LM的训练成本基本上是不现实的。另外一种方法是构造一个支持新知识的evidence collection，让模型进行学习。但是这种持续学习的方法，要特别注意灾难性遗忘的问题，也就是会迅速忘记之前的旧知识（由于所有的参数都要更新，也不知道哪个参数需要修改，修改幅度有多大）。</li>
<li>hyper networks：另一种方法是通过训练一个外部网络，让它输出要修改知识所需要的weight shift，从而能够编辑知识（<em>Editing Factual Knowledge in Language Models. 21</em>）。</li>
<li>direct editing：Meng等人提出可以直接把Transformer block看做是key-value对，通过追踪相关的weight，直接修改对应的weight即可（<em>Locating and editing factual knowledge in gpt. 22</em>）。</li>
</ul>
<h2 id="reasoning">5. Reasoning</h2>
<p>LM模型已经表现出了一定的推理能力，比如常识推理、自然语言推理、数学推理、归纳推理等等。并且如果输入些推理过程的提示，LM模型也可以模仿着给出推理过程。</p>
<p>但是LM到底有没有推理能力，还没有定论（<em>Chain of thought prompting elicits reasoning in large language models. 22</em>）。</p>
<h2 id="interpretability">6. Interpretability</h2>
<p>作者区分了两个“可解释”：</p>
<ul>
<li>interpretability指对模型内部机理的探究；</li>
<li>explainability指模型的输出是否可解释，比如让模型自己给出输出答案的原因，属于事后解释。</li>
</ul>
<p>可解释性可能是影响大规模语言模型真正落地到实际应用中最大的问题了。研究者从不同的角度进行了探究，但是个人认为目前的进展还不能充分解释LM内部机理。</p>
<ul>
<li>Probing：研究者将LM内部的表示和外部属性进行关联，从而辅助理解LM到底学习到了什么信息。</li>
<li>Attention：自注意力是Transformer中的重要组成，研究者对attention进行了许多探究，包括不同层attention学习到的模式有什么区别、同一层不同attention head学习到的模式有什么区别、attention在不同情况下会更加关注什么样的token输入等等。</li>
<li>Mechanistic Interpretability：Elhage等人提出了一个解释Transformer的数学视角（<em>A mathematical framework for transformer circuits. 21</em>）。</li>
<li>Causal tracing：Meng等人尝试追踪模型输出和参数之间的路径关联（<em>Locating and editing factual knowledge in gpt. 22</em>）。</li>
</ul>
<h2 id="explainability">7. Explainability</h2>
<p>作者提到，有人使用influence function来尝试提高输出的可解释性，对此方法不了解（<em>Explaining black box predictions and unveiling data artifacts through inﬂuence functions. 20</em>）。</p>
<p>注意力本身的结果也常常被用来提供可解释输出。有研究者认为attention可以提供必要的explainability，但是有研究者认为attention不能够提供真正的explainability（<em>Is attention interpretable? 19</em>，<em>Attention is not Explanation 19</em>）。另外有研究者认为这个和具体模型相关（<em>Attention is not not explanation. 19</em>）。 所以attention能否被用来作为输出解释的一部分，目前在学术界还有争议。</p>
<p>另外一种方式是直接让LM给出它们做决策的解释，比如可以让它指出输入文本中支持输出的fragment（<em>Probing across time: What does roberta know and when? 16</em>）。</p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>NLP</category>
      </categories>
      <tags>
        <tag>Survey</tag>
        <tag>NLP</tag>
        <tag>Language Model</tag>
      </tags>
  </entry>
  <entry>
    <title>LSTM-CRF</title>
    <url>/nlp/LSTM-CRF/</url>
    <content><![CDATA[<h1 id="neural-architectures-for-named-entity-recognition">Neural Architectures for Named Entity Recognition</h1>
<p>NAACL 2016，CMU</p>
<p>作者针对NER问题，提出了基于bi-LSTM和CRF（条件随机场）的模型以及transition-based的方法s-LSTM（该模型为详细阅读）。</p>
<blockquote>
<p>State-of-the-art named entity recognition systems rely heavily on hand-crafted features and domain-speciﬁc knowledge in order to learn effectively from the small, supervised training corpora that are available. In this paper, we introduce two new neural architectures—one based on bidirectional LSTMs and conditional random ﬁelds, and the other that constructs and labels segments using a transition-based approach inspired by shift-reduce parsers. Our models rely on two sources of information about words: character-based word representations learned from the supervised corpus and unsupervised word representations learned from unannotated corpora. Our models obtain state-of-the-art performance in NER in four languages without resorting to any language-specific knowledge or resources such as gazetteers.</p>
</blockquote>
<span id="more"></span>
<p>作者使用双向LSTM学习sentence的context信息。</p>
<p>输入层拼接了pretrained好的word embedding以及character-level的embedding。</p>
<p>输出层采用CRF，主要原因是合理NER的标注序列是满足某些内部约束的。也就是说不同token的tag之间不是完全独立的，某个token的tag的标注会对其它token的tag标注产生影响。</p>
<p>整体模型架构：</p>
<figure>
<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221130203736133.png" alt="image-20221130203736133" /><figcaption>image-20221130203736133</figcaption>
</figure>
<p>输入层word embedding的产生：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221130203815945.png"   style="zoom:40%;" /></p>
<p>图中的<span class="math inline">\(e_{Mars}\)</span>是来自于前人基于skip-n-gram，从语料无监督学习到的word embedding。用于捕获distributional evidence（比如常常是表示实体的单词倾向于出现在什么位置？）。</p>
<p><span class="math inline">\(l_{Mars}\)</span>和<span class="math inline">\(r_{Mars}\)</span>是使用一个新的character bi-LSTM建模得到的character-level的word embedding。用于捕获语言可能具备的orthographic evidence，拼写层次的特征（比如常常成为一个name的单词通常长什么样子？）。</p>
<p>两个embedding拼接，再经过dropout，得到了最后输入到LSTM-CRF模型的final word embedding。</p>
<p>输出预测序列标签的时候，最简单的方法是使用LSTM输出的hidden state为每个token单独进行预测。但是在NER任务中，实际上在不同token输出的标签之间，存在内部的依赖，比如I-PER这个tag不可能紧跟着出现在B-LOC后面。因此，作者考虑使用Conditional Random Field对不同token的预测标签进行联合建模，而不是单独建模。</p>
<p>对于输入序列：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221130205734969.png"   style="zoom:50%;" /></p>
<p>计划输出序列：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221130205807567.png"   style="zoom:50%;" /></p>
<p>首先，会输出一个矩阵<span class="math inline">\(P\in R^{n\times k}\)</span>，<span class="math inline">\(n\)</span>表示<span class="math inline">\(n\)</span>个token，<span class="math inline">\(k\)</span>表示一共有<span class="math inline">\(k\)</span>个不同的tag。<span class="math inline">\(P_{ij}\)</span>就表示第<span class="math inline">\(i\)</span>个token成为第<span class="math inline">\(k\)</span>个tag的概率。</p>
<p>为了建模不同token的tag之间的依赖，还定义了一个转移矩阵transition matrix <span class="math inline">\(A\in R^{k+2\times k+2}\)</span>。<span class="math inline">\(k+1\)</span>是因为新增了表示句子start和end的tag。<span class="math inline">\(A_{ij}\)</span>表示从tag <span class="math inline">\(i\)</span>转移到tag <span class="math inline">\(j\)</span>的score。</p>
<p>因此，可以定义下面的输出NER序列score：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221130210358207.png"   style="zoom:40%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221130210450926.png"   style="zoom:40%;" /></p>
<p>最大化概率目标：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221130210650954.png"  style="zoom:30%;" /></p>
<p>最后，选择最大score的<span class="math inline">\(y\)</span>就是预测的NER序列。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221130210504762.png"   style="zoom:50%;" /></p>
<p>NER的输出标签，常用的IOB格式（Inside, Outside, Beginning），Beginning表示当前token是某个entity的开始，Inside表示当前token是某个entity的中间，Outside表示当前token不属于任何token。在这篇论文中，作者采用了IOB格式的拓展IOBES，除了I，O，B外还包括了singleton entities（S）和the end of named entities（E）。作者在实验中没有发现IOB和IOBES有太大差距。</p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>NER</category>
      </categories>
      <tags>
        <tag>KG</tag>
        <tag>NER</tag>
      </tags>
  </entry>
  <entry>
    <title>Transformer</title>
    <url>/nlp/Transformer/</url>
    <content><![CDATA[<h1 id="attention-is-all-you-need">Attention Is All You Need</h1>
<p>NIPS 2017</p>
<p>本文提出了一个新的简单的网络结构，Transformer，只依赖于注意力机制。</p>
<p><a href="http://jalammar.github.io/illustrated-transformer/">可参考的博客</a></p>
<span id="more"></span>
<h2 id="introduction">1 Introduction</h2>
<p>RNN等模型已经取得了很大的成功，但是计算循环网络的代价通常会与输入输出序列的符号位置紧密相关。序列的符号位置决定了循环网络中输入的步数，导致很难并行化，就在时间和计算资源（内存）上限制了模型的训练。</p>
<p>注意力机制在sequence modeling 和 transduction models任务上已经取得了很大的成就，但是很少有模型会将注意力机制与RNN联系在一起。</p>
<p>本文就提出了一个新的方法Transformer，避免了循环，相反的是基于注意力机制完全依赖于输入和输出的整体。</p>
<blockquote>
<p>Transformer is the ﬁrst transduction model relying entirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution.</p>
</blockquote>
<h2 id="model-architecture">2 Model Architecture</h2>
<p>整个Transformer的结构是encoder和decoder的结构，如下图所示。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20201011202918546.png" style="zoom:50%;" /></p>
<p>整体堆叠6层encoder，然后经过6层的decoder。6层encoder的最终输入会输入到每一层的decoder中。</p>
<p>每一层encoder包括两个sublayer，一个接受input的输入，然后经过<span class="math inline">\(h\)</span>个多头注意力，残差加上原来的输入，之后norm（这叫做post-normalization，指layernorm放在残差之后，实际上后来很多工作使用的是pre-normalization，也就是先norm，再经过attention或FFN，最后残差，可参考ViT的结构），第二层经过一个前馈网络，还是残差加上原来的输入，经过norm。encoder的初始输入是全部的原始输入。</p>
<p>每一层的decoder包括三个sublayer，有两个与encoder一样，但是多了一层会接受encoder的输出作为keys和values。decoder的初始输入是<span class="math inline">\(t-1\)</span>时刻的预测结果以及encoder的输出。</p>
<p>下面详细解析：</p>
<h3 id="multi-head-attention">2.1 Multi-head attention</h3>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20201011204804340.png" style="zoom:50%;" /></p>
<p>首先计算单个attention，使用query，keys和values来计算。使用query和其它所有embedding的keys计算出权值，然后不同的权值与values相乘求和。querys和keys的维度是<span class="math inline">\(d_k\)</span>，values的维度是<span class="math inline">\(d_v\)</span>。 <span class="math display">\[
Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V
\]</span> 需要注意的是其中多了一个常量<span class="math inline">\(\sqrt{d_k}\)</span>，这是因为作者在实验中发现加入除数常量的效果很好，原因可能是因为softmax的输入在值较大的时候梯度会变小，因此加入了一个除数常量减小值。</p>
<blockquote>
<p>4 To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, <span class="math inline">\(q\cdot k=\sum^{d_k}_{i=1}q_ik_i\)</span> , has mean 0 and variance<span class="math inline">\(d_k\)</span> .</p>
</blockquote>
<p>除以<span class="math inline">\(\sqrt{d_k}\)</span>是为了防止在d_k特别大的时候，也就是hidden embedding维度比较大的时候，计算出来的注意力weight呈现出只有一个值非常靠近<span class="math inline">\(1\)</span>，其它值靠近<span class="math inline">\(0\)</span>的情况，这会导致bp的时候的梯度就很小，几乎是0。</p>
<p>除以<span class="math inline">\(\sqrt{d_k}\)</span>能够把输入softmax的absolute attention weight的值都scale的小一点； 减低指数函数<span class="math inline">\(e(\cdot)\)</span>带来的放大效应/马太效应。详细的数学解释可以参考<a href="https://towardsdatascience.com/transformer-networks-a-mathematical-explanation-why-scaling-the-dot-products-leads-to-more-stable-414f87391500">Transformer Networks: A mathematical explanation why scaling the dot products leads to more stable gradients</a></p>
<p>计算完成单个attention之后，再计算多头注意力，拼接起来之后再乘以一个权值矩阵： <span class="math display">\[
MultiHead(Q,K,V)=Concat(head_1,\dots,head_h)W^O
\]</span> 在实践中，使用了8个头，每个维度64，一共维度512。每一个头都可以看做是好比CNN中的不同的卷积通道，每个head独立训练，有自己的参数，期望每个head能够学习到不同的pattern。高层和底层、同一层的不同head有可能学习到不同的知识（这一点有相关文章探讨，发现不同注意力层会捕获不同层次的信息，但是每一层的不同head可能只有几个会学习到不同的pattern，比如不同的attention分布）。</p>
<p>decoder的结构与encoder类似，但是它多了一层encoder和decoder。</p>
<h3 id="self-attention">2.2 self-attention</h3>
<p>第一步：对于输入的每一个vector创建3个新的vector， a Query vector, a Key vector, and a Value vector。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/self-attention_softmax.png" style="zoom:50%;" /></p>
<p>第二步：计算单个的score，比如说计算第一个词Thinking，需要计算整个序列当中所有的vector对于Thinking的vector的重要程度，使用Thinking的query vector和其它所有的key vector做dot product。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/transformer_self_attention_score.png" style="zoom:50%;" /></p>
<p>第三步与第四步：实际是归一化socre，相当于产生relative score。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/self-attention-output.png" style="zoom:50%;" /></p>
<p>第五步：各个word vector与relative score相乘，求和。这样编码后的某个位置的新的embedding是由前一步所有输入的embedding共同决定的。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/self-attention-matrix-calculation.png" style="zoom:50%;" /></p>
<p>第六步：矩阵形式的实际计算情况</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/transformer_decoder_output_softmax.png" style="zoom:50%;" /></p>
<h3 id="encoder-and-decoder">2.3 Encoder and Decoder</h3>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/transformer_self_attention_vectors.png" style="zoom:50%;" /></p>
<h3 id="the-final-linear-and-softmax-layer">2.4 The Final Linear and Softmax Layer</h3>
<p>decoder的输出，经过一个全连接层，然后得到logits vector，其中每一维度对应一个word；再经过softmax，取出score最大的word。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/Screen-Shot-2020-10-13-at-8-46-23-PM.png" style="zoom:50%;" /></p>
]]></content>
      <categories>
        <category>Paper</category>
        <category>NLP</category>
      </categories>
  </entry>
  <entry>
    <title>camera-tutorial1</title>
    <url>/z-personal-taste/camera-tutorial1/</url>
    <content><![CDATA[<h1 id="相机与摄像入门笔记">相机与摄像入门笔记</h1>
<p><a href="https://www.bilibili.com/video/BV1pv411H78e/?spm_id_from=333.1007.top_right_bar_window_custom_collection.content.click&amp;vd_source=2862656caae0c18be380254a92306b47">教程来源，B站，从零开始手把手教你学摄影，20节课带你从小白到大师</a></p>
<span id="more"></span>
<h2 id="认识你的相机">1. 认识你的相机</h2>
<p>相机分类方式：</p>
<ul>
<li>可更换镜头相机</li>
<li>不可更换镜头相机</li>
</ul>
<p>作为个人业余用户，可更换镜头相机是最常用的。接下来重点讲解。</p>
<p>可更换镜头相机市面上主流有两类，单反和微单，两者的主要区别是光学结构的不同，这也导致了两种相机在体型上的差距，单反通常会更大一点。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221023164758493.png"   style="zoom:50%;" /></p>
<ul>
<li>单反：包括了一个反光镜和五棱镜。单反如果进入实时取景模式，会抬起反光镜，那么此时它和微单是一样的。</li>
<li>微单：光线可以直接照射到传感器。</li>
</ul>
<p>尽管结构更简单，但是微单并不是比单反效果要差。实际上，微单才是未来相机的趋势。</p>
<p>第二种分类方式是根据传感器的大小。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221023165146594.png" /></p>
<p>传感器的大小就是我们常说的底，或者是CMOS，它决定了我们画幅的大小。</p>
<p>大画幅作为普通人很少接触。</p>
<p>中画幅作为新手也很少接触。</p>
<p>全画幅和残画幅是新手接触比较多的。</p>
<p>相机画幅的大小可以很简单的看出来。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221023193621965.png"   style="zoom:50%;" /></p>
<p>佳能、尼康和索尼的相机产品线命名：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221023193816811.png" /></p>
<p>通常的讲，数字的位数越少，产品越高端。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221023194131080.png" /></p>
<p>同样是数字位数越少，产品越高端。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221023194304991.png" /></p>
<p>镜头的分类，大致可以分为原厂和副厂。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221023194459349.png" /></p>
<p>另外常见的分类方式是根据镜头的焦距，焦距控制了相机视野的范围。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221023194610826.png" /></p>
<p>焦距越小，视角越大，单个主体占比也就越小。</p>
<p>还有几个其它的分类方式：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221023194733150.png" /></p>
<p>光圈越大，有一个数值<code>f</code>就会越小，恒定的光圈镜头会好于浮动光圈镜头。光圈越小，镜头越贵。</p>
<p>对于变焦镜头有两种，一种是外变焦，一种是内变焦。外变焦镜头在转动变焦环的时候，镜头长度会变化；内变焦镜头，镜头长度不会发生改变。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221023195010363.png" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221023195218979.png" /></p>
<p>常见的变焦镜头黑话：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221023195325647.png" /></p>
<p>还有定焦镜头：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221023195452687.png" /></p>
<p>作为新手，还是建议使用变焦镜头。同时，视频作者建议，可以选择中焦焦段的镜头。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221023195705718.png" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221023195746989.png" /></p>
<p>选购建议：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221023195853378.png" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20221023195959749.png" /></p>
]]></content>
      <categories>
        <category>interest</category>
        <category>camera</category>
      </categories>
      <tags>
        <tag>camera</tag>
      </tags>
  </entry>
  <entry>
    <title>Basic-Info</title>
    <url>/tutorial/cs224n-2019/resourse/</url>
    <content><![CDATA[<h1 id="cs224n-natural-language-processing-with-deep-learning">CS224n: Natural Language Processing with Deep Learning</h1>
<p><a href="https://web.stanford.edu/class/cs224n/index.html">Offical website</a></p>
<p><a href="https://www.youtube.com/playlist?list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z">Youtube video</a></p>
]]></content>
      <categories>
        <category>Class</category>
        <category>CS224N</category>
      </categories>
  </entry>
  <entry>
    <title>14-transformers</title>
    <url>/tutorial/cs224n-2019/14-transformers/</url>
    <content><![CDATA[<h1 id="transformers-and-self-attention">Transformers and Self-Attention</h1>
<p>序列化的模型类似于RNN，存在几个问题：</p>
<ul>
<li>Sequential computation的计算限制了并行计算</li>
<li>没有对于short和long dependencies的显式建模</li>
<li>我们希望能够建模层级</li>
</ul>
<span id="more"></span>
<p>对于迁移不变性的解释。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210329210505679.png" /></p>
<h2 id="section"></h2>
]]></content>
      <categories>
        <category>Class</category>
        <category>CS224N</category>
      </categories>
  </entry>
  <entry>
    <title>1-Interoduction-word-vectors</title>
    <url>/tutorial/cs224n-2019/1-Interoduction-word-vectors/</url>
    <content><![CDATA[<h1 id="introduction-and-word-vectors">1 Introduction and Word Vectors</h1>
<p><a href="https://www.youtube.com/watch?v=8rXD5-xhemo&amp;list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z&amp;index=2">video</a></p>
<span id="more"></span>
<h2 id="human-language-and-word-meaning">Human language and word meaning</h2>
<p>human language的特征：</p>
<ul>
<li>不确定性。我们尝试使用语言来描述世界，表达自己的想法，但是自己表述的语言能否被其它的人接受实际是不确定的，可能是一种概率的问题。</li>
<li>”human language is a pathetically slow network“，说话这种方式能够表达的能力是很有限的，因此在人类的交流中，实际语言是实现了一种对于信息的压缩，能够理解语言的背后是我们的大脑中已经拥有了很多先验知识。</li>
</ul>
<p>Represent the meaning of word</p>
<p>Definition: <strong>meaning</strong></p>
<ul>
<li>the idea that is represented by a word</li>
</ul>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/word2vec-prob.png" /></p>
<p>在早期的NLP中，人们将不同的词word表示为独立的符号（discrete symbol），这叫做localist representation，例如one-hot编码。</p>
<p>这样表示的问题：</p>
<ul>
<li>word的数量很大，甚至可以构造出无限多的word，导致one-hot编码的dim越来越大</li>
<li>对于one hot编码来说，不同的编码之间是独立的，和正交的，无法保留word原本的含义，也无法衡量两个word之间的相似程度</li>
</ul>
<p>如何获取一个word的meaning？</p>
<p>一个著名的观点是：</p>
<blockquote>
<p>”You shall know a word by the company it keeps“</p>
<p>——J. R. Firth 1957: 11</p>
<p>A word’s meaning is given by the words that frequently appear close-by.</p>
</blockquote>
<p>针对one hot的问题，我们尝试为每个word建立更dense vector，即使word的distributed representation，word vector。</p>
<h2 id="word2vec">Word2vec</h2>
<p>借助于前面的观点，word2vec出现了。</p>
<blockquote>
<p>Word2vec (Mikolov et al. 2013) is a framework for learning word vectors</p>
</blockquote>
<p>核心思想：</p>
<ul>
<li>一个大的语料库</li>
<li>每个word都使用固定长度的vector表示</li>
<li>选中center word，计算它周围的outside word/context word出现的概率，并且不断更新参数让这个概率最大</li>
</ul>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/idea-word2vec.png" /></p>
<p>核心问题在于如何计算<span class="math inline">\(P(w_{t+j}|w_t)\)</span>？</p>
<p>在word2vec中，对于每个word建立两个vector：</p>
<ul>
<li><span class="math inline">\(v_w\)</span> when word <span class="math inline">\(w\)</span> is a center word</li>
<li><span class="math inline">\(u_w\)</span> when word <span class="math inline">\(w\)</span> is a context word</li>
</ul>
<p>核心公式： <span class="math display">\[
P(o|c)=\frac{\text{exp}(u_o^T v_c)}{\sum_{w\in V} \text{exp}(u_w^T v_c)}
\]</span> 解释</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/meaning-of-words.png" /></p>
<p>使用点积来衡量相似度，当我们得知了context的时候，中心词的meaning应该也能知悉，即context与center word的meaning此时应该接近。对于在语料中经常出现的word，赋予它们比较大的概率。</p>
<p>上面的公示实际是softmax</p>
<ul>
<li>soft：指对于所有的预测目标都有一个估计概率，哪怕它可能很小</li>
<li>max：指softmax的输出是概率最大的值</li>
</ul>
]]></content>
      <categories>
        <category>Class</category>
        <category>CS224N</category>
      </categories>
  </entry>
  <entry>
    <title>01-intro-graph</title>
    <url>/tutorial/cs224w/01-intro-graph/</url>
    <content><![CDATA[<h1 id="introduction-of-graphs">Introduction of graphs</h1>
<h2 id="introduction">Introduction</h2>
<p>为什么network/graph很重要？</p>
<blockquote>
<p>Networks are a general language for describing complex systems of interacting entities</p>
</blockquote>
<p>当我们谈论network的时候，经常讨论两种图：</p>
<ol type="1">
<li>Natural graph：对于现实事物的直接描述，例如社交网络、大脑神经元的链接网络等</li>
<li>Information graph：经过处理之后，带有信息的图，例如链接知识的图等</li>
</ol>
<span id="more"></span>
<p>实际上在某些情况下上面两种network的分界线是很模糊的。</p>
<p>很多事物都拥有图的结构，利用这些图的结构能够帮助我们更好的预测。</p>
<p>Why networks?</p>
<ul>
<li>Universal language for describing complex data</li>
<li>Shared vocabulary between fields</li>
<li>Data availability &amp; computational challenges</li>
</ul>
<p>Ways to analyze networks:</p>
<ul>
<li>Predict the type/color of a given node
<ul>
<li>Node classification</li>
</ul></li>
<li>Predict whether two nodes are linked
<ul>
<li>Link prediction</li>
</ul></li>
<li>Identify densely linked clusters of nodes
<ul>
<li>Community detection</li>
</ul></li>
<li>Measure similarity of two nodes/networks
<ul>
<li>Network similarity</li>
</ul></li>
</ul>
<h2 id="structure-of-graphs">Structure of Graphs</h2>
<p>graph的component：</p>
<ul>
<li>Objects: nodes, edges <span class="math inline">\(N\)</span></li>
<li>Interactions: links, edges <span class="math inline">\(E\)</span></li>
<li>System: network, graph <span class="math inline">\(G(N, E)\)</span></li>
</ul>
<figure>
<img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210309193044665.png" alt="image-20210309193044665" /><figcaption>image-20210309193044665</figcaption>
</figure>
<blockquote>
<p>We will try to make this distinction whenever it is appropriate, but in</p>
<p>most cases we will use the two terms interchangeably</p>
</blockquote>
<p>graph的基本概念：</p>
<ul>
<li>无向图</li>
<li>有向图</li>
</ul>
<p>node degree：对于无向图来说就是一个节点连接的边，因此一个无向图的平均度就是<span class="math inline">\(2E/N\)</span>。对于有向图来说度分为入度和出度，一个节点的in-degree就是有多少条箭头指向该节点；out-degree就是多少条边末端链接到该节点上。</p>
<p>几种特殊的graph：</p>
<ul>
<li>complete graph：对于无向图，一个complete graph指所有节点之间都存在边：<span class="math inline">\(E=E_{max}=\frac{N(N-1)}{2}\)</span></li>
<li>Bipartite graph：a graph whose nodes can be divided into two disjoint sets <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> such that every link connects a node in <span class="math inline">\(U\)</span> to one in <span class="math inline">\(V\)</span>; that is, <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> are independent sets</li>
<li>Weighted graph：在邻接矩阵中的非0值不再只是1，而是其它衡量重要程度的实值，比如道路图</li>
<li>Self-edge graph：边的起始点都是同一个节点，比如蛋白质图protein graph</li>
<li>Multigraph：在节点和节点当中存在多条边，比如Communication graph、Collaboration graph</li>
</ul>
<p>在数学上表示一个图可以使用adjacent matrix表示。</p>
<ul>
<li>对于无向图，行和列求和相等，并且是对应节点的degree</li>
<li>对于有向图，行求和是out-degree，列求和是in-degree</li>
</ul>
<p>对于这种表示方式，需要在脑海里保持的一种直觉观点是，邻接矩阵是非常稀疏的。矩阵的稠密度计算：<span class="math inline">\(E/N^2\)</span>。</p>
<p>还可以使用edge list和adjacent list表示：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210309203007866.png" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210309203034763.png" /></p>
<p>graph的连通性connectivity：</p>
<ol type="1">
<li>对于无向图，如果说一个graph是Connected graph，这意味着任意两个节点都可以通过某个路径连接起来</li>
<li>对于有向图，分为强连接性图和弱连接性图</li>
</ol>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210309204211861.png" /></p>
<p>在研究图的连通性当中，可能存在关键的边或者节点，如果把这些关键点或边删除整个图就不再连通。</p>
<ul>
<li>Bridge edge: If we erase the edge, the graph becomes disconnected</li>
<li>Articulation node: If we erase the node, the graph becomes disconnected</li>
</ul>
]]></content>
      <categories>
        <category>Class</category>
        <category>CS224W</category>
      </categories>
  </entry>
  <entry>
    <title>02-prop-of-graph-and-rand-model</title>
    <url>/tutorial/cs224w/02-prop-of-graph-and-rand-model/</url>
    <content><![CDATA[<h1 id="properities-of-networks-random-graph-model">Properities of networks, Random Graph Model</h1>
<p>在下面所提到的图默认是无向图。介绍了graph的四种属性</p>
<span id="more"></span>
<h2 id="network-properties">Network Properties</h2>
<p>4 key network properties</p>
<ol type="1">
<li>Degree distribution <span class="math inline">\(P(k)\)</span></li>
</ol>
<p>具有不同度的节点数量在所有graph node中的比例</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210309220725684.png" /></p>
<ol start="2" type="1">
<li>Paths in a graph</li>
</ol>
<p>在图中的路径path是指节点的序列。在有向图中的path需要遵循edge的direction。</p>
<p>有了path就可以衡量距离distance，两个节点的distance是最短路径shortest path。</p>
<p>定义了distance之后，可以定义graph的直径diameter。graph的diameter是所有节点对的distance中最长的值。</p>
<ol start="3" type="1">
<li>Clustering coefficient</li>
</ol>
<p>聚类系数clustering coefficient衡量了节点的邻居之间的连接性。clustering coefficient针对的是graph中的每一个node。具体算法是计算邻居之间的边/理想中最多的领居间边的数量。 <span class="math display">\[
C_i= \frac{2e_i}{k_i(k_i-1)}
\]</span> 其中，<span class="math inline">\(e_i\)</span>是邻居间的边的数量，<span class="math inline">\(k_i\)</span>是节点<span class="math inline">\(i\)</span>的度。<span class="math inline">\(k_i(k_i-1)\)</span>计算了所有邻居节点之间都存在一个边的上限情况。聚类系数在0-1区间。</p>
<ol start="4" type="1">
<li>Connectivity</li>
</ol>
<p>定义为最大连通单元中节点的数量。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210309222007993.png" /></p>
<h2 id="random-graph-model">Random Graph Model</h2>
<p>我们可以设定一些条件，产生人造的随机graph，来促进我们对于现实graph的理解。</p>
<p>可以有两种random graph</p>
<ol type="1">
<li><span class="math inline">\(G_{np}\)</span>：n个node，node之间产生edge的概率是<span class="math inline">\(p\)</span>。</li>
<li><span class="math inline">\(G_{nm}\)</span>：n个节点，随机产生<span class="math inline">\(m\)</span>个边。</li>
</ol>
<p>研究这两种random graph的properties。</p>
<p>对于<span class="math inline">\(G_{np}\)</span>：</p>
<h3 id="degree-distribution">Degree Distribution</h3>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210322215556478.png" /></p>
<h3 id="clustering-coefficient">Clustering Coefficient</h3>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210322214919097.png" /></p>
<h3 id="path-length">Path Length</h3>
<p>首先定义Expansion，核心思想是随机选一个node集合，有多少的边会离开这个集合。</p>
<p>式子定义：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210323170924449.png" /></p>
<p>式子下面的分母表示对于一个划分来说，如果划分的S越大，节点越多，如果离开集合S的edge数量不变，那么expansion应该小；如果划分的集合S的node数量不变，那么离开S的edge数量越多，expansion越大。随机的划分S，能得到V-S，对于S和V-S都可以计算出一个expansion，离开这两个集合的edge数量一样，但是如果拿较大的集合来算的话，计算出来的expansion就会偏小。因此，总是以数量较少的集合作为考虑的点。</p>
<p>随机的划分集合，能得到很多的expansion，为了衡量整个graph的expansion，考虑expansion的下限，即最小的那个expansion。在这种情况下，如果graph的expansion比其它graph的expansion更大，可以理解为这个graph的expansion更大，locality更弱。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210323163010845.png" /></p>
<p>一个random graph的path length是<span class="math inline">\(O(log\ n)\)</span>。diameter是:</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210323163900359.png" /></p>
<p>也就是说对于random graph来说，随着node数量增加，diameter并不会增加很多。</p>
<h3 id="lagest-connected-components">Lagest Connected Components</h3>
<p>在random graph中，随着平均degree增加，node链接到giant component的概率增加。</p>
<h2 id="the-smallest-world-model">The Smallest-World Model</h2>
<p>仔细观察下network的properties会发现，聚类系数与graph的直径似乎是两个有点冲突的属性。如果聚类系数比较高，说明一个graph的locality强，那么node与较远的node之间就比较难有直接的链接，这会造成graph的路径很大。</p>
<p>一个graph的diameter衡量了graph的“shortcut”，如果diameter比较小，意味着对于一个node，可以在较小的step内链接到其它node。但是直径变小的话，一个graph的locality似乎会被破坏，一个node的很多邻居之间不相连，而是有更多的edge链接到其它的node上。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210323105306472.png" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210323105329615.png" /></p>
<p>那么，是否有办法让graph同时具有high clustering和small diameter？</p>
<p>解决方案是the smallest-world model。1998。</p>
<p>核心是对于一个已经具有high clustering的graph，引入randomness，新增/删减edge来创建更多的“shortcut”。</p>
<p>示例图：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20210323110103346.png" /></p>
]]></content>
      <categories>
        <category>Class</category>
        <category>CS224W</category>
      </categories>
  </entry>
  <entry>
    <title>2021-02-tradition-ml</title>
    <url>/tutorial/cs224w/2021-02-tradition-ml/</url>
    <content><![CDATA[<h1 id="traditional-mehods-for-machine-learning-in-graphs">Traditional Mehods for Machine Learning in Graphs</h1>
]]></content>
      <categories>
        <category>Class</category>
        <category>CS224W</category>
      </categories>
  </entry>
  <entry>
    <title>Basic-Info</title>
    <url>/tutorial/cs224w/resourse/</url>
    <content><![CDATA[<h1 id="machine-learning-with-graphs">Machine Learning with Graphs</h1>
<p><a href="http://snap.stanford.edu/class/cs224w-2019/">offical 2019</a></p>
<p><a href="http://web.stanford.edu/class/cs224w/">offical 2020</a></p>
<p><a href="http://networksciencebook.com/chapter/1#vulnerability">online book: network science</a></p>
]]></content>
      <categories>
        <category>Class</category>
        <category>CS224W</category>
      </categories>
  </entry>
  <entry>
    <title>2-representation</title>
    <url>/tutorial/multimodal-cmu-2022/2-representation/</url>
    <content><![CDATA[<h1 id="mmml-tutorial-challenge-1-representation">MMML Tutorial Challenge 1: Representation</h1>
<p>Challenge 1 Representation：</p>
<blockquote>
<p>Learning representations that reflect cross-modal interactions between individual elements, across different modalities.</p>
</blockquote>
<p>Representation challenge有三个sub-challenge，Fusion、Coordination和Fission。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904163842455.png"   style="zoom:33%;" /></p>
<span id="more"></span>
<h2 id="sub-challenge-1-representation-fusion">Sub-Challenge 1: Representation Fusion</h2>
<p>fusion的定义：</p>
<blockquote>
<p>Learn a joint representation that models cross-modal interactions between individual elements of different modalities.</p>
</blockquote>
<p>学习模态之间的联合表示。有两种fusion：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903195752329.png"   style="zoom:33%;" /></p>
<p>basic fusion指融合两个已经同质较多的modality；complex fusion是指融合异质性强的modality。basic fusion是很重要的，因为对于complex fusion在网络学习过程中，随着抽象层次的增加，不同模态之间的同质性是在增加的，最后进行融合的时候，可以看做是一个basic fusion。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903200130379.png"   style="zoom:33%;" /></p>
<h3 id="basic-fusion">Basic fusion</h3>
<p>首先，对于两个模态，我们可以简单的把它们的表示拼接到一起，叫做additive fusion：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903200954360.png"   style="zoom:33%;" /></p>
<p>additive fusion可以应用到更复杂的情况：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903201046736.png"   style="zoom:33%;" /></p>
<p>additive fusion可以看做是一种集成的方法，或者叫做late fusion。</p>
<p>如果认为这样加性的混合不能够满足模态交互的要求，那么可以采用乘法的交互，multiplicative fusion。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903202429932.png"   style="zoom:33%;" /></p>
<p>bilinear fusion：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903202509100.png"   style="zoom:33%;" /></p>
<p>tensor fusion，从张量角度融合表示。（<em>Zadeh et al., Tensor Fusion Network for Multimodal Sentiment Analysis, EMNLP 2017</em>）</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903202649615.png"   style="zoom:33%;" /></p>
<p>tensor fusion的结果tensor，会随着modality的数量增加而指数式增加。因此出现了降低计算成本的low-rank fusion（<em>Liu et al., Efficient Low-rank Multimodal Fusion with Modality-Specific Factors, ACL 2018</em>）。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903202928027.png"   style="zoom:33%;" /></p>
<p>如果进一步拓展加法fusion和乘法fusion，我们可以获得high-order polynomial fusion（<em>Hou et al., Deep Multimodal Multilinear Fusion with High-order Polynomial Pooling, Neurips 2019</em>）。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903203411430.png"   style="zoom:33%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903203449368.png" alt="image-20220903203449368" style="zoom:33%;" /></p>
<p>gated fusion，对于不同模态，设计融合信息的gate（<em>Gated Multimodal Units for information fusion,</em>）。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903204826371.png"   style="zoom:33%;" /></p>
<p>gate的设计可以是线性的，非线性的或者是核函数，都是在衡量模态间的相似程度。</p>
<p>Nonlinear Fusion就是利用神经网络进行融合，比如通过一个MLP。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903205016445.png"   style="zoom:33%;" /></p>
<p>对于这种方法，可以看做是一种early fusion，因为仅仅是拼接了两个向量后，然后进行融合，只不过融合的方法变成了神经网络。对于nonlinear fusion，必须注意的是，它真的学习到了nonlinear interaction吗？</p>
<p>之后有人提出了EMAP方法衡量fusion方法对nonlinear interaction的建模能力。核心思想是，通过EMAP，将nonlinear fusion投影到additive fusion上，如果得到的additive fusion的预测能力和nonlinear fusion的预测能力相近，那么就说明fusion没有很好的建模非线性的信息。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903210522579.png"   style="zoom:33%;" /></p>
<p>作者发现，部分的fusion方法表现出来的结果，并没有很好的建模非线性的交互信息。</p>
<h3 id="complex-fusion">Complex fusion</h3>
<p>仍然是非常有挑战的方法，如何直接处理异质性很强的模型？下面是一个实例，通过进行channel exchange直接进行模态fusion（<em>Deep multimodal fusion by channel exchanging</em>）。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903211104205.png"   style="zoom:33%;" /></p>
<h2 id="sub-challenge-2-representation-coordination">Sub-Challenge 2: Representation Coordination</h2>
<p>模态信息的协作coordination，定义是：</p>
<blockquote>
<p>Learn multimodally-contextualized representations that are coordinated through their cross-modal interactions.</p>
</blockquote>
<p>和fusion不同的是，它不会把所有模态信息融合到一个表示空间中，而是用某种方式保持模态信息的一致。存在两种coordination：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903212757995.png"   style="zoom:33%;" /></p>
<p>一般的，这种coordination function以loss function的形式实现：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903212934237.png"   style="zoom:33%;" /></p>
<p>下面是几种coordination function示例：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903213014577.png"  style="zoom:33%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903213034255.png"   style="zoom:33%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903213100212.png"   style="zoom:33%;" /></p>
<p>CCA是指让不同模态的latent representation保持较强的相关性，下面是一个实例（DCCAE，<em>Wang et al., On deep multi-view representation learning, PMLR 2015</em>）。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903213244674.png"  style="zoom:33%;" /></p>
<p>还有的方法，假设模型能够学习到完整的intact的表示，不同模态的表示只是intect representation在不同view下的表现（<em>Xu et al., Multi-View Intact Space Learning, TPAMI 2015</em>）：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903213414235.png"   style="zoom:33%;" /></p>
<p>再比如下面的方法，通过学习intact representation，然后设计在单个模态的degradation network（<em>Zhang et al., AE2-Nets: Autoencoder in Autoencoder Networks, CVPR 2019</em>）：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903213541449.png"   style="zoom:33%;" /></p>
<p>还有gated coordination：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903213631959.png"   style="zoom:33%;" /></p>
<p>另外一个popular的方法是对比学习Contrastive learning。它不需要像前面的方法一样，人工设计某种相似度函数，强迫latent representation互相靠近，而是定义关联的不同模态element pair，通过定义loss，让关联的不同模态的element pair互相靠近，不相关的pair互相远离，这样最后学习到的latent representation自然会表现出关联的相近，不关联的远离。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903214055573.png"   style="zoom:33%;" /></p>
<p>举例，比如CLIP（<em>Radford et al., Learning Transferable Visual Models From Natural Language Supervision, arxiv 2021</em>）：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903214142642.png" alt="image-20220903214142642" style="zoom:33%;" /></p>
<p>再比如（<em>Kiros et al., Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models, NIPS 2014</em>）：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903214223340.png" alt="image-20220903214223340" style="zoom:33%;" /></p>
<h2 id="sub-challenge-3-representation-fission">Sub-Challenge 3: Representation Fission</h2>
<p>fission定义：</p>
<blockquote>
<p>learning a new set of representations that reflects multimodal internal structure such as data factorization or clustering.</p>
</blockquote>
<p>同样有两种类似的fission：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903222515087.png"   style="zoom:33%;" /></p>
<p>Modality-Level Fission，期望能够学习只包含在modality A中的信息，学习至包含在modality B中的信息，学习同时包括A和B的信息：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903222639929.png"   style="zoom:33%;" /></p>
<p>如何学习这三种不同的表示？</p>
<p>有一种方法是从loss的角度，让不同的表示有不同的倾向（<em>Tsai et al., Learning Factoriazed Multimodal Representations, ICLR 2019</em>）。我们使用<span class="math inline">\(L_1\)</span>让不同的表示尽可能有所区别，避免信息重叠；使用<span class="math inline">\(L_2\)</span>还原原来的模态信息；使用<span class="math inline">\(L_3\)</span>进行预测任务。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903222802216.png"   style="zoom:33%;" /></p>
<p>对于上面这种做法的理解，可以从信息论的角度看（<em>Tsai et al., Self-Supervised Learning from a Multi-View Perspective, ICLR 2021</em>）：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903222949898.png"  style="zoom:33%;" /></p>
<p>让mutual information尽可能的大，让条件熵尽可能的小。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903223031841.png"   style="zoom:33%;" /></p>
]]></content>
      <categories>
        <category>tutorial</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>3-alignment</title>
    <url>/tutorial/multimodal-cmu-2022/3-alignment/</url>
    <content><![CDATA[<h1 id="mmml-tutorial-challenge-2-alignment">MMML Tutorial Challenge 2: Alignment</h1>
<p>Alignment定义：</p>
<blockquote>
<p>Identifying and modeling cross-modal connections between all elements of multiple modalities, building from the data structure.</p>
</blockquote>
<p>存在三种可能的connection：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904100856424.png"   style="zoom:33%;" /></p>
<p>equivalence表示两个不同模态的element之间是完全相等的，correspondences表示两个element信息互相补充比如图像和对图像内容的描述，dependencies表示两个element之间存在关系。</p>
<span id="more"></span>
<p>dependency示例：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904101631417.png"   style="zoom:33%;" /></p>
<p>比如说存在时间上的前后关系；或者是比较特殊的co-dependencies，指不同的元素总是同时出现。</p>
<h2 id="sub-challenge-1-explicit-alignment">Sub-Challenge 1: Explicit Alignment</h2>
<p>定义：</p>
<blockquote>
<p>Identifying explicit connections between elements of multiple modalities.</p>
</blockquote>
<p>比如把图像中的object和对应的文本描述关联起来：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904102128899.png"   style="zoom:33%;" /></p>
<p>speech的对齐：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904103040629.png"   style="zoom:33%;" /></p>
<p>在图像和文本的对齐中，对于模态element的定义是明确的。但是对于某些模态的定义就不够确切。比如如果我们希望对齐两个video。</p>
<p>比如存在有研究如何对齐两个video的方法：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904104449428.png"   style="zoom:33%;" /></p>
<h2 id="sub-challenge-2-implicit-alignment">Sub-Challenge 2: Implicit Alignment</h2>
<p>定义：</p>
<blockquote>
<p>Implicitly model connections between elements for better representation learning.</p>
</blockquote>
<p>比如期望实现下面的三个模态对齐：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904105557586.png"   style="zoom:33%;" /></p>
<p>我们无法直接提前选择好explicit connection，但是我们可以利用神经网络实现implicit connection。比如一个简单，但是efficient的方式是把所有的模态拼接到一起后，使用Transformer，通过self-attention，潜在地把不同模态的element融合到一起：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904105844754.png"   style="zoom:33%;" /></p>
<p>Transformer会把所有可能相关的element关联到一起。VisualBERT是一个关联image-language的实例：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904110121686.png"   style="zoom:33%;" /></p>
<p>上面的方法会直接把不同模态信息进行融合，最近出现了更多的pair-wise alignment：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904110811425.png"   style="zoom:33%;" /></p>
<p>仍然保持各自在不同的表示空间，但是对于modality A会尝试对齐来自modality 的信息，对于modality B尝试对齐来自modality A的信息。如果出现了三个以上的模态，同样可以进行跨模态的对齐。实现的主要思路是，比如我现在有个word embedding，使用这个word embedding作为query embedding，计算来自video slice的image embedding的相似度，然后基于attention聚合image embedding，这样就到达了衡量image对于language的重要程度。下面是一个cross-modal pairwise Transformer的实例：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904111052584.png"   style="zoom:33%;" /></p>
<p>使用到了这种cross-modal Transformer的实例，比如ViLBERT（<em>ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks</em>）：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904111448833.png"   style="zoom:33%;" /></p>
<p>再比如LXMERT：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904111539420.png"   style="zoom:33%;" /></p>
<p>另外一个最近的研究兴趣是使用GNN实现alignment，不同模态的element可以互相关联到一起，通过不断的迭代GNN，对于每一个node都能够不断的看到更大的视野。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904151545587.png"   style="zoom:33%;" /></p>
<p>下面的方法是实例MTAG（<em>Modal-Temporal Attention Graph for Unaligned Human Multimodal Language Sequences, NAACL 2021</em>）：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904151624388.png" style="zoom:33%;" /></p>
]]></content>
      <categories>
        <category>tutorial</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>1-intro</title>
    <url>/tutorial/multimodal-cmu-2022/1-intro/</url>
    <content><![CDATA[<p><a href="https://cmu-multicomp-lab.github.io/mmml-tutorial/schedule/">CMU MML Tutorial Louis-Philippe Morency</a></p>
<h1 id="mmml-tutorial-introduction">MMML Tutorial: Introduction</h1>
<h2 id="多模态介绍">多模态介绍</h2>
<p>什么是multimodal？</p>
<p>在数学上，我们描述多模态是在概率上有不同的分布趋势。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903093913110.png"   style="zoom:20%;" /></p>
<p>但是现在，我们大多提到多模态，更多是在指multiple modalities。更准确的说是sensory modalities。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903094246738.png"   style="zoom:20%;" /></p>
<span id="more"></span>
<p>不同的模态意味着拥有不同的特征或者说信号。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903094422429.png"   style="zoom:20%;" /></p>
<p>那么接下来，什么是模态Modality？</p>
<p>一种较通用的定义是，多模态是指</p>
<blockquote>
<p>Modality refers to the way in which something expressed or perceived.</p>
</blockquote>
<p>也就是指信息被表达或者感知的方式。</p>
<p>下面是对于模态理解的一种角度：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903094937416.png"   style="zoom:30%;" /></p>
<p>从这个角度出发，我研究的KG，本身就已经是经过人类处理之后的模态，已经较为抽象，脱离了一开始的原始形式。</p>
<p>什么是多模态Multimodal？</p>
<p>词典上的定义：</p>
<blockquote>
<p>Multimodal: with multiple modalities.</p>
</blockquote>
<p>研究人员的定义：</p>
<blockquote>
<p>Multimodal is the science of heterogeneous and interconnected data.</p>
</blockquote>
<p>核心解决两个问题：不同模态的差异性和不同模态如何联系到一起。</p>
<p>不同模态表示的信息，通常是异质的heterogeneous。并且，如果是更抽象的模态，表示的信息会更加趋同。（这么一想，或许这是为什么我们会尝试利用神经网络，在高层进行模态信息的融合，而不是在低层）</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903095640357.png"   style="zoom:30%;" /></p>
<p>两个在不同角度拍摄的照相机，它们的结果当然是相近的（但肯定因为角度不同有所区别）；两个来自不同语言的文本，差异性就会比较大；而语言和视觉之间的差异就更大了。</p>
<p>不同模态信息可能存在差异的几个维度实例：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903100256385.png"   style="zoom:30%;" /></p>
<p>比如结构上的差异、表达空间的差异（例如speech通常是连续的）、信息表现的特征、特征的粒度、数据的噪音、模态是否和任务相关等等方面。</p>
<p>模态的元素之间通常是如何关联到一起的，对于关联到一起的元素，我们如何让它们之间进行交互？这个通常是多模态学习需要解决的关键核心问题。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903100802997.png" alt="image-20220903100802997" style="zoom:30%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903100826507.png"   style="zoom:30%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903100851250.png"   style="zoom:30%;" /></p>
<p>不同模态交互，可能存在的情况举例：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903102403297.png"   style="zoom:30%;" /></p>
<p>从统计角度看，两个不同模态元素经常同时出现；某个模态经常依赖于另外模态的元素（时间/空间）；从语义角度看，两个模态元素都是在描述统一事物；或者两个模态元素之间存在语义联系。</p>
<p>接下来，对于关联的多模态元素，出现不同特征/信号的时候，可能出现什么样的结果？</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903102645836.png"   style="zoom:33%;" /></p>
<p>举例：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903102727947.png"   style="zoom:40%;" /></p>
<p>比如在上面的图中，不同模态出现了不同的信号，有不同的响应。不同模态响应同时作用下，可能出现响应的增强/互补、响应不变、响应倾向于某个模态、或者是出现新的响应形式。</p>
<p>对于不同模态元素的交互，通常可以从以下几个维度考虑：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903102212604.png"   style="zoom:30%;" /></p>
<h2 id="多模态研究历史">多模态研究历史</h2>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903104827261.png"   style="zoom:30%;" /></p>
<p>一开始的时候，研究人员从社会学的角度，研究不同模态之间的联系，比如David McNeill研究了手势和语言之间的联系，认为语言是speech的必要组成部分。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903104958575.png"  style="zoom:30%;" /></p>
<p>随后，出现了基于计算的研究</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903105058872.png"   style="zoom:30%;" /></p>
<p>由于在nlp方向，人们能够把token表示为向量，例如word2vec；在cv方向，人们同样能够适应cnn把image的object表示为向量。人们通过这种方法，让文本和图像之间表现出了更多的homogeneous。</p>
<p>在过去的五年中，利用深度学习，出现了大量的不同模态之间的研究方向</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903105144702.png"   style="zoom:30%;" /></p>
<h2 id="多模态机器学习">多模态机器学习</h2>
<p>什么是多模态机器学习？</p>
<blockquote>
<p>Multimodal Machine Learning (MML) is the study of computer algorithms that learn and improve through the use and experience of data from multiple modalities</p>
</blockquote>
<p>什么是多模态人工智能？</p>
<blockquote>
<p>Multimodal Artificial Intelligence (MAI) studies computer agents able to demonstrate intelligence capabilities such as understanding, reasoning and planning, through multimodal experiences, and data</p>
</blockquote>
<p>Tutorial作者认为multimodal AI是multimodal ML的超集。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903110850771.png"   style="zoom:50%;" /></p>
<p>在multimodal问题中，需要面临解决的6个核心challenge。</p>
<ol type="1">
<li>Representation</li>
</ol>
<blockquote>
<p>Learning representations that reflect cross-modal interactions between individual elements, across different modalities.</p>
</blockquote>
<p>如何表示不同模态的信息，如何表示不同模态中的单个element？这个问题几乎是multimodal learning中最基本也最核心的问题。可能存在以下几种情况：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903141449798.png"   style="zoom:30%;" /></p>
<p>fusion表示原始的模态信息，之后被融合到一个representation space中；coordination指不同模态始终有独立的表示空间；fission表示先融合，之后分裂到不同的空间中；</p>
<ol start="2" type="1">
<li>Alignment</li>
</ol>
<blockquote>
<p>Identifying and modeling cross-modal connections between all elements of multiple modalities, building from the data structure.</p>
</blockquote>
<p>对于不同模态中的所有element，如何发现它们之间存在的联系，并且利用这样的关联？</p>
<p>模态内部很可能存在内部的结构，不同模态元素之间也可能存在显式的连接，同时，利用representation，也可能找到潜在的关联。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903142254056.png"   style="zoom:33%;" /></p>
<ol start="3" type="1">
<li>Reasoning</li>
</ol>
<blockquote>
<p>Combining knowledge, usually through multiple inferential steps, exploiting multimodal alignment and problem structure.</p>
</blockquote>
<p>如何结合knowledge，进行推理？</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903142655068.png"  style="zoom:33%;" /></p>
<p>几个sub-challenge：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903142737264.png"   style="zoom:33%;" /></p>
<p>如何从结构上融合knowledge？如何定义或者使用中间概念？如何设计推理模式？如何利用外部knowledge（例如commonsense knowledge）进行推理？</p>
<ol start="4" type="1">
<li>Generation</li>
</ol>
<blockquote>
<p>Learning a generative process to produce raw modalities that reflects cross-modal interactions, structure and coherence.</p>
</blockquote>
<p>generation可能存在几个不同的challenge：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903143031642.png"   style="zoom:33%;" /></p>
<p>summarization期望进行信息缩减；translation期望信息不丢失；creation可能是最难的，它期望获得信息的拓展，能够应用到新的模态中。</p>
<ol start="5" type="1">
<li>Transference</li>
</ol>
<blockquote>
<p>Transfer knowledge between modalities, usually to help the target modality which may be noisy or with limited resources.</p>
</blockquote>
<p>对于目标modality，如何利用来自其它模态的信息？来自其它模态的信息可能是有限的，也可能是noisy的。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903143519103.png"   style="zoom:33%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903143611127.png"  style="zoom:33%;" /></p>
<p>transfer指不同的模型学习不同模态，如何把不同模态的信息迁移到目标模态；co-learning是指使用同一个模型同时处理不同模态；</p>
<ol start="6" type="1">
<li>Quantification</li>
</ol>
<blockquote>
<p>Empirical and theoretical study to better understand heterogeneity, cross-modal interactions and the multimodal learning process.</p>
</blockquote>
<p>对多模态学习进行理论上的分析。如何理解heterogeneity？如何理解interaction？以及如何理解multimodal learning的过程？</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903143838239.png"  style="zoom:33%;" /></p>
<p>以上的六个core challenge实际是关联在一起的。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220903144057552.png"   style="zoom:33%;" /></p>
<p>首先，我们需要考虑如何表示不同模态的信息，是分别在独立的空间中进行学习，还是在联合空间下进行学习；其次，我们需要考虑如何发现多模态元素的关联；在前两步基础上，我们才可以进行目标推理，如何设计合理的结构，处理heterogeneity和interaction，对预测目标采用合理的步骤进行推理；同样的，我们可以进行模态生成，完成模态转换、summarization等任务；我们还可以进行模态的迁移，让其它模态辅助、增强目标模态的预测，它和模态生成的区别是，模态生成的输入不包括要预测的模态；最后，我们需要理论上对multimodal learning的支撑。</p>
]]></content>
      <categories>
        <category>tutorial</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>4-reasoning</title>
    <url>/tutorial/multimodal-cmu-2022/4-reasoning/</url>
    <content><![CDATA[<h1 id="mmml-tutorial-challenge-3-reasoning">MMML Tutorial Challenge 3: Reasoning</h1>
<p>Reasoning的定义：</p>
<blockquote>
<p>Combining knowledge, usually through multiple inferential steps, exploiting multimodal alignment and problem structure.</p>
</blockquote>
<p>reasoning的基础是前面的representation和alignment，然后我们才可以考虑如何combine合适的不同模态的信息来得到理想的预测在值。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904153027700.png"   style="zoom:33%;" /></p>
<span id="more"></span>
<p>可以看到，reasoning和representation fusion在原理上是有相似之处的，但是reasoning比fusion更加的复杂，它可能需要multi-step实现对各种不同complex structure建模；fusion更多是指single-step的融合。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904154623849.png"  style="zoom:33%;" /></p>
<h2 id="sub-challenge-1-structure-modeling">Sub-Challenge 1: Structure Modeling</h2>
<p>定义，如何建模出现在不同模态间的复杂结构</p>
<blockquote>
<p>Defining or learning the relationships over which composition occurs.</p>
</blockquote>
<p>可能存在以下不同的结构：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904162348932.png"   style="zoom:33%;" /></p>
<p>接下来看一下如何实现对Temporal Structure的建模How can we capture cross-modal interactions across time? 一种方法是通过memory network来实现：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904155159816.png"   style="zoom:33%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904155220741.png"   style="zoom:33%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904155244531.png"   style="zoom:33%;" /></p>
<p>接下来是建模hierarchical structure，比如在visual grounding中，期望利用language的语法结构，然后能够利用这样的语法结构进行推理：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904160207597.png"   style="zoom:33%;" /></p>
<p>interactive structure，它同样是一种时间上的结构，但是和一般的temporal structure不一样的是，interactive structure中前一步的action，会影响未来的action。而在一般的temporal structure中不一定这样，temporal structure中的元素可能仅仅存在时间先后的联系，不一定存在直接的明确的影响。</p>
<p>建模interactive structure更多的依赖于reinforcement learning，这是一个很大的方向，完全可以作为一个新的tutorial，这里不进行详细的了解。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904161348542.png"   style="zoom:33%;" /></p>
<p>最后是structure discovery，我们不在自己定义complex network进行reasoning，而是通过网络结构搜索，让机器自动学习合适的reasoning structure。下面是一个实例（<em>Xu et al., MUFASA: Multimodal Fusion Architecture Search for Electronic Health Records. AAAI 2021</em>）：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904161940163.png"  style="zoom:33%;" /></p>
<p>这样做的好处是无需人工的设计网络架构，我们做的只是定义好各种building blocks，让机器自己去找合适的结构就可以。缺点是需要大量的计算，机器需要不断的尝试不同的架构，进行训练，然后评估。</p>
<h2 id="sub-challenge-2-intermediate-concepts">Sub-Challenge 2: Intermediate Concepts</h2>
<p>中间概念intermediate concepts的定义：</p>
<blockquote>
<p>The parameterization of individual multimodal concepts in the reasoning process.</p>
</blockquote>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904163524657.png"   style="zoom:33%;" /></p>
<p>引入中间概念来辅助推理，可能是的reasoning process更加可信赖，更加interpolate。</p>
<p>下面是一个借助neuro-symbolic的实例（<em>Andreas et al., Neural Module Networks. CVPR 2016]</em>），它人工设计了概念作为中间状态：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904163420441.png"   style="zoom:33%;" /></p>
<h2 id="sub-challenge-3-inference-paradigm">Sub-Challenge 3: Inference Paradigm</h2>
<p>inference paradigm challenge定义：</p>
<blockquote>
<p>How increasingly abstract concepts are inferred from individual multimodal evidences.</p>
</blockquote>
<p>粗暴一点的说，就是如何能够考虑逻辑？</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904165120144.png"   style="zoom:33%;" /></p>
<p>几种可能存在的inference模式：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904165245476.png"   style="zoom:33%;" /></p>
<p>首先是对于logical inference，以VQA举例，很多的模型实际上无法捕获逻辑联系，比如在下面的实例中（<em>Gokhale et al., VQA-LOL: Visual Question Answering Under the Lens of Logic. ECCV 2020</em>）：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904165446797.png"   style="zoom:33%;" /></p>
<p>研究者提出的一种解决方案是，建模了可微分的逻辑操作符：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904165542182.png"   style="zoom:33%;" /></p>
<p>接下来是casual inference。当我们尝试简单的改变预测目标时，现在的很多模型会出现预测错误的情况，并且它们很可能捕获了错误的潜在correlation（<em>Agarwal et al., Towards Causal VQA: Revealing &amp; Reducing Spurious Correlations by Invariant &amp; Covariant Semantic Editing. CVPR 2020</em>）。比如在下面的例子中，雨伞和灯笼的颜色是无关的，但是模型错误的捕获了这种联系：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904170002737.png"   style="zoom:33%;" /></p>
<p>在另外的例子中，斑马和斑马的数量是相关的，但是模型没有能够捕获相关性：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904170109873.png"   style="zoom:33%;" /></p>
<p>那如何能够让模型更加robust？研究人员提出的一种方案是同时处理这种不相关的object和相关的object：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904170243235.png"   style="zoom:33%;" /></p>
<h2 id="sub-challenge-4-knowledge">Sub-Challenge 4: Knowledge</h2>
<p>接下来是如何利用knowledge辅助多模态融合？</p>
<blockquote>
<p>The derivation of knowledge in the study of inference, structure, and reasoning.</p>
</blockquote>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904171350617.png"   style="zoom:33%;" /></p>
<p>接下来是几个knowledge的实例。首先是multimodal knowledge graph辅助VQA（<em>Marino et al., OK-VQA: A visual question answering benchmark requiring external knowledge. CVPR 2019</em>）：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904171725486.png"   style="zoom:33%;" /></p>
<p>为了能够利用knowledge辅助QA，研究人员提出的方法（<em>Gui et al., KAT: A Knowledge Augmented Transformer for Vision-and-Language. NAACL 2022</em>）：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904171811521.png"   style="zoom:33%;" /></p>
<p>另一个利用multimodal knowledge graph的例子（<em>Zhu et al., Building a Large-scale Multimodal Knowledge Base System for Answering Visual Queries. arXiv 2015</em>）：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904171908431.png"   style="zoom:33%;" /></p>
<p>实际上，还存在着大量可以研究的点：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904172117577.png"   style="zoom:33%;" /></p>
]]></content>
      <categories>
        <category>tutorial</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>6-transference</title>
    <url>/tutorial/multimodal-cmu-2022/6-transference/</url>
    <content><![CDATA[<h1 id="mmml-tutorial-challenge-5-transference">MMML Tutorial Challenge 5: Transference</h1>
<p>Transference是指对于一个资源可能受限的主modality，使用另外的modality进行辅助。定义：</p>
<blockquote>
<p>Transfer knowledge between modalities, usually to help the primary modality which may be noisy or with limited resources</p>
</blockquote>
<p>存在两个可能的关键挑战：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904185728838.png"   style="zoom:33%;" /></p>
<span id="more"></span>
<h2 id="sub-challenge-1-transfer-via-foundation-mondels">Sub-Challenge 1: Transfer via Foundation Mondels</h2>
<p>challenge定义，通过利用pretrained model来迁移knowledge：</p>
<blockquote>
<p>Adapting large-scale pretrained models on downstream tasks involving the primary modality.</p>
</blockquote>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904190645782.png"   style="zoom:33%;" /></p>
<p>下面是一个利用language model辅助visual task的实例（<em>Tsimpoukelli et al., Multimodal Few-Shot Learning with Frozen Language Models. NeurIPS 2021</em>）：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904191330947.png"   style="zoom:33%;" /></p>
<p>在这个过程中，提前训练好的language model的参数是不变的。</p>
<p>还有一个方法是representation tuning，例如下面的例子，通过self-attention衡量audio information和vision information对language representation的重要程度，然后shift language representation（<em>Ziegler et al., Encoder-Agnostic Adaptation for Conditional Language Generation. arXiv 2019</em>, <em>Rahman et al., Integrating Multimodal Information in Large Pretrained Transformers. ACL 2020</em>）：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904191936187.png"   style="zoom:33%;" /></p>
<p>还有研究者使用multitask learning进行模态信息的迁移（<em>Liang et al., HighMMT: Towards Modality and Task Generalization for High-Modality Representation Learning. arXiv 2022</em>）：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904192254951.png"   style="zoom:33%;" /></p>
<p>还有类似的Gato（<em>A Generalist Agent</em>）：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904193047811.png"   style="zoom:33%;" /></p>
<h2 id="sub-challenge-2-co-learning">Sub-Challenge 2: Co-learning</h2>
<p>通过共享representation space来transfer information，定义：</p>
<blockquote>
<p>Transferring information from secondary to primary modality by sharing representation spaces between both modalities.</p>
</blockquote>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904193409904.png"   style="zoom:33%;" /></p>
<p>对于如何引入modality B，有两种方式：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904193707229.png" style="zoom:33%;" /></p>
<p>可以在input layer融合modality B，也可以在prediction layer引入modality B。</p>
<h3 id="co-learning-via-fusion">Co-learning via fusion</h3>
<p>一个通过fusion进行co-learning的实例如下图（<em>Socher et al., Zero-Shot Learning Through Cross-Modal Transfer. NeurIPS 2013</em>）。它通过把image embedding靠近相应的word embedding，比如horse image embedding应该接近horse word embedding。在实现的时候，采用了challenge 1 representation中的coordination方式，让两个在不同空间的表示互相协作靠近。这样做好友一个好处就是它可以用于zero-shot，比如对于从来没有见过的class cat。因为我们已经学习到了cat的word embedding，通过model处理后，cat的image embedding应该会靠近cat word embedding。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904194611816.png"   style="zoom:33%;" /></p>
<p>另一个实例是学习joint model（<em>Foundations of Multimodal Co-learning.</em>）：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904195020522.png"   style="zoom:33%;" /></p>
<h3 id="co-learning-via-translation">Co-learning via translation</h3>
<p>接下来是通过在预测层融合其它modality的information。下面是一个在language和text之间进行信息迁移的实例（<em>Pham et al., Found in Translation: Learning Robust Joint Representations via Cyclic Translations Between Modalities. AAAI 2019</em>）：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904195755479.png"   style="zoom:33%;" /></p>
<p>但是这样的做法并不能确保两个模态的信息都被完全使用了，因为这仅仅是language到visual的translation：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904195908638.png"   style="zoom:33%;" /></p>
<p>作者的做法是让image再翻译回language：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904200006443.png"   style="zoom:33%;" /></p>
<p>之后，同样有研究者通过language来生成对应的image（<em>Vokenization: Improving Language Understanding with Contextualized, Visual-Grounded Supervision. EMNLP 2020</em>）：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904200225098.png"   style="zoom:33%;" /></p>
<p>还存在更多可以探究的challenge：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904200535323.png"   style="zoom:33%;" /></p>
]]></content>
      <categories>
        <category>tutorial</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>5-generation</title>
    <url>/tutorial/multimodal-cmu-2022/5-generation/</url>
    <content><![CDATA[<h1 id="mmml-tutorial-challenge-4-generation">MMML Tutorial Challenge 4: Generation</h1>
<p>generation的定义是生成raw modality，也就是说应该和input modalities是不同的modality：</p>
<blockquote>
<p>Learning a generative process to produce raw modalities that reflects cross-modal interactions, structure, and coherence.</p>
</blockquote>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904212852739.png"   style="zoom:33%;" /></p>
<span id="more"></span>
<p>generation的两个维度：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904213140854.png"   style="zoom:33%;" /></p>
<h2 id="sub-challenge-1-translation">Sub-challenge 1: Translation</h2>
<p>translation定义：</p>
<blockquote>
<p>Translating from one modality to another and keeping information content while being consistent with cross-modal interactions.</p>
</blockquote>
<p>比如DALLE（<em>Ramesh et al., Zero-Shot Text-to-Image Generation. ICML 2021</em>）：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904214318958.png"   style="zoom:33%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904215448827.png"   style="zoom:33%;" /></p>
<p>从content和generation的角度来看，因为我们做的translation，因此我们不需要存在信息损失，所以利用coordination来保持两个模态的信息能够互相协作。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904215635706.png"   style="zoom:33%;" /></p>
<p>比如DALL E 2（<em>Ramesh et al., Hierarchical Text-Conditional Image Generation with CLIP Latents. arXiv 2022</em>）和DALL-E核心原理是一致的：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904214502388.png"   style="zoom:33%;" /></p>
<h2 id="sub-challenge-2-summarization">Sub-challenge 2: Summarization</h2>
<p>summarization的定义是缩减信息量并且找出重要的信息：</p>
<blockquote>
<p>Summarizing multimodal data to reduce information content while highlighting the most salient parts of the input.</p>
</blockquote>
<p>比如下面的例子，通过video和language生成summary（<em>Palaskar et al., Multimodal Abstractive Summarization for How2 Videos. ACL 2019</em>）：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904214917678.png"   style="zoom:33%;" /></p>
<p>summarization的content就需要是进行模态的fusion，并且生成的时候需要进行信息的缩减：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904215846397.png"   style="zoom:33%;" /></p>
<h2 id="sub-challenge-3-creation">Sub-challenge 3: Creation</h2>
<p>creation需要创造新的modalities，是一个非常具有挑战性的方向：</p>
<blockquote>
<p>Simultaneously generating multiple modalities to increase information content while maintaining coherence within and across modalities.</p>
</blockquote>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904220312768.png"   style="zoom:33%;" /></p>
<p>实际上现在没有特别符合creation方向的方法，一个非常初步的方法是（<em>Tsai et al., Learning Factorized Multimodal Representations. ICLR 2019</em>）：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904220731108.png"   style="zoom:33%;" /></p>
<p>还存在很多的可以研究的点：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220904221023855.png"  style="zoom:33%;" /></p>
]]></content>
      <categories>
        <category>tutorial</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
  <entry>
    <title>7-quantification</title>
    <url>/tutorial/multimodal-cmu-2022/7-quantification/</url>
    <content><![CDATA[<h1 id="mmml-tutorial-challenge-6-quantification">MMML Tutorial Challenge 6: Quantification</h1>
<p>定义：</p>
<blockquote>
<p>Empirical and theoretical study to better understand heterogeneity, cross-modal interactions, and the multimodal learning process.</p>
</blockquote>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220905090715105.png"   style="zoom:33%;" /></p>
<span id="more"></span>
<h2 id="sub-challenge-1-heterogeneity">Sub-Challenge 1: Heterogeneity</h2>
<p>定义：</p>
<blockquote>
<p>Quantifying the dimensions of heterogeneity in multimodal datasets and how they subsequently influence modeling and learning.</p>
</blockquote>
<p>对于modality异质性的探究有以下几个维度：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220905091224546.png"   style="zoom:33%;" /></p>
<p>有研究者对modality biases进行了探究，例如在下面的VQA task中，因为训练集中80%的banana都是黄色的，因此在使用一个绿色的banana image进行测试的，VQA model也错误的回答成了黄色：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220905091853994.png"   style="zoom:33%;" /></p>
<p>为了解决这个问题，研究人员提出了两种方法。第一种是直接从数据集的角度进行平衡；第二种是从训练过程进行平衡，让VQA model不仅仅依赖于单一的modality，而是也能够充分利用visual modality的信息：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220905092226054.png"   style="zoom:33%;" /></p>
<p>在单模态中也存在social biases。比如下面的例子，模型会简单的根据桌子上有一个电脑而错误的认为在桌子前的是男性；也会因为图片中一个人手里拿的是网球拍，就认为这个人是男性（<em>Hendricks et al., Women also Snowboard: Overcoming Bias in Captioning Models. ECCV 2018</em>）：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220905093014039.png"   style="zoom:33%;" /></p>
<p>另外的研究发现，跨模态反而可能进一步增加social biases：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220905093143869.png"   style="zoom:33%;" /></p>
<p>引入visual information之后反而进一步增加了对性别的刻板印象（stereotype），比如总是认为男性带公文包；女性带钱包。</p>
<p>有研究针对heterogeneity中存在的噪音、多模态模型对于缺失模态的鲁棒性、多模态模型性能和鲁棒性的关系进行了探究：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220905094316311.png"   style="zoom:33%;" /></p>
<p>为了提升模型的鲁邦性，有几种方法被提出：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220905094407401.png"   style="zoom:33%;" /></p>
<p>比如在训练时就人为遮盖掉不同的modality input；使用modality translation来推测缺失的modality等。</p>
<h2 id="sub-challenge-2-cross-modal-interactions">Sub-Challenge 2: Cross-modal Interactions</h2>
<p>cross-modal interaction尝试解释不同模态element之间的联系：</p>
<blockquote>
<p>Quantifying the presence and type of cross-modal connections and interactions in multimodal datasets and trained models.</p>
</blockquote>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220905095703291.png"  style="zoom:33%;" /></p>
<p>下面的工作通过representation fission确定了overall cross-modal interaction的存在：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220905095943110.png"  style="zoom:33%;" /></p>
<p>接下来，研究人员对individual cross-modal interaction进行了探究（<em>Liang et al., MultiViz: An Analysis Benchmark for Visualizing and Understanding Multimodal Models. arXiv 2022</em>）：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220905100409439.png"   style="zoom:33%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220905100504291.png"   style="zoom:33%;" /></p>
<p>进一步，M2Lens对cross-modal interaction进行了分类（<em>Wang et al., M2Lens: Visualizing and Explaining Multimodal Models for Sentiment Analysis. IEEE Trans Visualization and Computer Graphics 2021</em>）：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220905100802906.png"   style="zoom:33%;" /></p>
<p>作者还提供了一个可视化的网站：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220905100907943.png"   style="zoom:33%;" /></p>
<p>最近的，研究者实现了multimodal Transformer的可视化（<em>Aflalo et al., VL-InterpreT: An Interactive Visualization Tool for Interpreting Vision-Language Transformers. CVPR 2022</em>）：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220905101015744.png"  style="zoom:33%;" /></p>
<p>另外有研究者尝试对interoperation model进行评估，因为虽然这些model本身是用来解释multimodal model的，但是这些方法解释的是否正确，能不能让人真的理解，还需要进一步评估。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220905103921503.png"   style="zoom:33%;" /></p>
<p>evaluating interoperation model是一个非常challenging的方向，一个最新的方法是引入人工来评估（<em>Liang et al., MultiViz: A Framework for Visualizing and Understanding Multimodal Models. arXiv 2022</em>）：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220905103954645.png"   style="zoom:33%;" /></p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220905104017522.png"   style="zoom:33%;" /></p>
<p>这一方向还有很多的挑战：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220905104111125.png"   style="zoom:33%;" /></p>
<h2 id="sub-challenge-3-multimodal-learning-process">Sub-Challenge 3: Multimodal Learning Process</h2>
<p>接下来是对multimodal learning process的探究：</p>
<blockquote>
<p>Characterizing the learning and optimization challenges involved when learning from heterogeneous data.</p>
</blockquote>
<p>例如在下面的一个例子，引入新的modality总能够带来更好的性能吗？（<em>Wang et al., What Makes Training Multi-modal Classification Networks Hard? CVPR 2020</em>）</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220905104804142.png"   style="zoom:33%;" /></p>
<p>答案是没有，在上面的实例中，更多的modalities并没有带来更好的性能，相反它意味着更大的计算复杂度，实际上是一个更糟糕的结果。</p>
<p>一种可能的解释是，不同模态的过拟合-泛化的合适点不是一致的（<em>Wang et al., What Makes Training Multi-modal Classification Networks Hard? CVPR 2020</em>）：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220905105121346.png"   style="zoom:33%;" /></p>
<p>解决这一问题的作者提出的方法是，首先通过记录training checkpoints来得到不同modality的overfitting-to-generalization ratio（OGR）：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220905105343046.png"   style="zoom:33%;" /></p>
<p>然后尝试在不同模态的OGR之间进行平衡：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220905105516210.png"   style="zoom:33%;" /></p>
<p>除了上述三个challenge外，还存在许多的challenges：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20220905105751944.png"   style="zoom:33%;" /></p>
]]></content>
      <categories>
        <category>tutorial</category>
        <category>multimodal</category>
      </categories>
      <tags>
        <tag>multimodal</tag>
      </tags>
  </entry>
</search>
