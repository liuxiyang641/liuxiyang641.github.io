<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.4.0">

<link rel="preconnect" href="https://fonts.googleapis.com" crossorigin>
<link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin>
  <link rel="apple-touch-icon" sizes="180x180" href="/images/lxy-apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/lxy-favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/lxy-favicon-16x16.png">
  <link rel="mask-icon" href="/images/lxy-favicon.ico" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css" integrity="sha256-DfWjNxDkM94fVBWx1H5BMMp0Zq7luBlV8QRcSES7s+0=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"liuxiyang641.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.12.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"width":300},"copycode":{"enable":true,"style":"mac"},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":5,"unescape":true,"preload":false}}</script><script src="https://cdn.jsdelivr.net/npm/hexo-theme-next@8.12.2/source/js/config.min.js"></script>

    <meta name="description" content="Multimodal Learning with Transformers: A Survey 2022-06 arxiv 牛津大学学者的一篇多模态Transformer综述，系统的描述了目前多模态Transformer可能注意的不同改进点。  Transformer is a promising neural network learner, and has achieved great suc">
<meta property="og:type" content="blog">
<meta property="og:title" content="MM-Transformer-Survey">
<meta property="og:url" content="https://liuxiyang641.github.io/mmml/MM-Transformer-Survey/index.html">
<meta property="og:site_name" content="Liu Xiyang">
<meta property="og:description" content="Multimodal Learning with Transformers: A Survey 2022-06 arxiv 牛津大学学者的一篇多模态Transformer综述，系统的描述了目前多模态Transformer可能注意的不同改进点。  Transformer is a promising neural network learner, and has achieved great suc">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230223150507977.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230223150851043.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230301104906563.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230223172136061.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230223172712248.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230223173045776.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230223173438330.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230223174400978.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230227144954372.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230301162531606.png">
<meta property="og:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230301201634535.png">
<meta property="article:published_time" content="2023-02-23T03:09:00.000Z">
<meta property="article:modified_time" content="2023-03-01T15:07:36.352Z">
<meta property="article:author" content="Liu Xiyang">
<meta property="article:tag" content="Transformer">
<meta property="article:tag" content="Multimodal">
<meta property="article:tag" content="Survey">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230223150507977.png">


<link rel="canonical" href="https://liuxiyang641.github.io/mmml/MM-Transformer-Survey/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://liuxiyang641.github.io/mmml/MM-Transformer-Survey/","path":"mmml/MM-Transformer-Survey/","title":"MM-Transformer-Survey"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>MM-Transformer-Survey | Liu Xiyang</title>
  




<link rel="stylesheet" type="text/css" href="/css/injector/main.css" /><link rel="preload" as="style" href="/css/injector/light.css" /><link rel="preload" as="style" href="/css/injector/dark.css" />
  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">Liu Xiyang</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">26</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">33</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">114</span></a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#multimodal-learning-with-transformers-a-survey"><span class="nav-number">1.</span> <span class="nav-text">Multimodal Learning with Transformers: A Survey</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#introduction"><span class="nav-number">1.1.</span> <span class="nav-text">1. Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#background"><span class="nav-number">1.2.</span> <span class="nav-text">2. Background</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#multimodal-transformers"><span class="nav-number">1.3.</span> <span class="nav-text">3. Multimodal Transformers</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#multimodal-input"><span class="nav-number">1.3.1.</span> <span class="nav-text">3.1 Multimodal Input</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#self-attention-variants-in-multimodal-context"><span class="nav-number">1.3.2.</span> <span class="nav-text">3.2 Self-Attention Variants in Multimodal Context</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#transformers-for-multimodal-pretraining"><span class="nav-number">1.4.</span> <span class="nav-text">4 Transformers for Multimodal Pretraining</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#task-agnostic-multimodal-pretraining"><span class="nav-number">1.4.1.</span> <span class="nav-text">4.1 Task-Agnostic Multimodal Pretraining</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#task-speci%EF%AC%81c-multimodal-pretraining"><span class="nav-number">1.4.2.</span> <span class="nav-text">4.2 Task-Speciﬁc Multimodal Pretraining</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#transformers-for-speci%EF%AC%81c-multimodal-tasks"><span class="nav-number">1.4.3.</span> <span class="nav-text">4.3 Transformers for Speciﬁc Multimodal Tasks</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#challenges-and-designs"><span class="nav-number">1.5.</span> <span class="nav-text">5 Challenges and Designs</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#fusion"><span class="nav-number">1.5.1.</span> <span class="nav-text">5.1 Fusion</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#alignment"><span class="nav-number">1.5.2.</span> <span class="nav-text">5.2 Alignment</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#transferability"><span class="nav-number">1.5.3.</span> <span class="nav-text">5.3 Transferability</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#efficiency"><span class="nav-number">1.5.4.</span> <span class="nav-text">5.4 Efficiency</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#universalness"><span class="nav-number">1.5.5.</span> <span class="nav-text">5.5 Universalness</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#interpretability"><span class="nav-number">1.5.6.</span> <span class="nav-text">5.4 Interpretability</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#discussion-and-outlook"><span class="nav-number">1.6.</span> <span class="nav-text">6 Discussion and outlook</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Liu Xiyang"
      src="/images/lxy-avatar.jpg">
  <p class="site-author-name" itemprop="name">Liu Xiyang</p>
  <div class="site-description" itemprop="description">Try your best to be an ordinary man.</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">114</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">33</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">26</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/liuxiyang641" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;liuxiyang641" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:liuxiyang@buaa.edu.cn" title="E-Mail → mailto:liuxiyang@buaa.edu.cn" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license site-overview-item animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdn.jsdelivr.net/npm/@creativecommons/vocabulary@2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>



        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://liuxiyang641.github.io/mmml/MM-Transformer-Survey/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/lxy-avatar.jpg">
      <meta itemprop="name" content="Liu Xiyang">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Liu Xiyang">
      <meta itemprop="description" content="Try your best to be an ordinary man.">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="MM-Transformer-Survey | Liu Xiyang">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          MM-Transformer-Survey<a href="https://github.com/liuxiyang641/liuxiyang641.github.io/edit/hexo/source/_posts/mmml/MM-Transformer-Survey.md" class="post-edit-link" title="编辑" rel="noopener" target="_blank"><i class="fa fa-pen-nib"></i></a>
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2023-02-23 11:09:00" itemprop="dateCreated datePublished" datetime="2023-02-23T11:09:00+08:00">2023-02-23</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2023-03-01 23:07:36" itemprop="dateModified" datetime="2023-03-01T23:07:36+08:00">2023-03-01</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper/" itemprop="url" rel="index"><span itemprop="name">Paper</span></a>
        </span>
          ，
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper/Multimodal/" itemprop="url" rel="index"><span itemprop="name">Multimodal</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">阅读次数：</span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>8.4k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>8 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="multimodal-learning-with-transformers-a-survey">Multimodal Learning with Transformers: A Survey</h1>
<p>2022-06 arxiv</p>
<p>牛津大学学者的一篇多模态Transformer综述，系统的描述了目前多模态Transformer可能注意的不同改进点。</p>
<blockquote>
<p>Transformer is a promising neural network learner, and has achieved great success in various machine learning tasks. Thanks to the recent prevalence of multimodal applications and big data, Transformer-based multimodal learning has become a hot topic in AI research. This paper presents a comprehensive survey of Transformer techniques oriented at multimodal data. The main contents of this survey include: (1) a background of multimodal learning, Transformer ecosystem, and the multimodal big data era, (2) a theoretical review of Vanilla Transformer, Vision Transformer, and multimodal Transformers, from a geometrically topological perspective, (3) a review of multimodal Transformer applications, via two important paradigms, i.e., for multimodal pretraining and for specific multimodal tasks, (4) a summary of the common challenges and designs shared by the multimodal Transformer models and applications, and (5) a discussion of open problems and potential research directions for the community.</p>
</blockquote>
<span id="more"></span>
<h2 id="introduction">1. Introduction</h2>
<p>我们期待的理想的人工智能具有的能力至少是可以做到人类能够做到的一切，这里就包括了人类感知世界的方式：看、听、摸等。人类使用特定感知器sensor和外界建立特定的交流通道，这种特定交流通道中传递/表达的信息形式我们称作是模态modality，比如语言或视觉：</p>
<blockquote>
<p>In general, a modality is often associated with a specific sensor that creates a unique communication channel, such as vision and language.</p>
</blockquote>
<p>这篇survey主要是考虑使用Transformer解决多模态任务，Transformer适用于多模态的几点原因：</p>
<ul>
<li>更少的模态特定的假设，比如RNN的序列化输入；CNN的局部迁移不变性，使得Transformer天然的适用于处理更多模态数据</li>
<li>对于许多多模态数据来说，可以被轻易的转换成适合于Transformer的序列输入形式</li>
<li>Transformer的内部结构，比如self-attention，很适合被改造为跨模态交互/多模态融合的形式</li>
</ul>
<p>有一些其它的survey是从更加广泛的模型来讨论多模态学习：</p>
<ul>
<li>Multimodal machine learning: A survey and taxonomy. 2018</li>
<li>Multimodal intelligence: Representation learning, information fusion, and applications. 2020</li>
<li>Multimodal co-learning: Challenges, applications with datasets, recent advances and future directions. 2022</li>
</ul>
<h2 id="background">2. Background</h2>
<p>多模态学习（multimodal machine learning，MML）并不是一个新词，从20世纪80年代开始就有人研究视觉听觉语音识别（<em>Integration of acoustic and visual speech signals using neural networks. 1989</em>）。在深度学习时代，随着Transformer模型的出现，算力的急速增长，多模态数据集规模的不断增加共同促进多模态学习进步。</p>
<p><em>更多背景请参考论文内容</em></p>
<h2 id="multimodal-transformers">3. Multimodal Transformers</h2>
<h3 id="multimodal-input">3.1 Multimodal Input</h3>
<p>对于任意模态数据，要输入到Transformer通常是做两步：</p>
<ol type="1">
<li><p>tokenize the input</p></li>
<li><p>select an embedding space to represent the tokens</p></li>
</ol>
<p>对于单模态数据，我们有不同的方法实现tokenization和选择合适的token embedding。比如对于image，我们可以选择ROI作为tokens，然后CNN导出的feature作为token embedding；可以选择将image划分成不同的patch，每个patch经过linear projection之后作为token embedding；也可以选择将image上的不同object作为tokens，使用GNN学习场景图的特征作为token embedding（<em>Multimodal sentiment detection based on multi-channel graph neural networks. 2021</em>）。</p>
<p>下面的表格是总结的一些多模态tokens处理的方法：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230223150507977.png" /></p>
<p>通常还会加入一些special tokens，用来服务一些特定的目的：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230223150851043.png" /></p>
<p>在实践中，人们常常会在embedding层选择融合不同的信息，属于early-fusion的一种。最常见的方式就是在每个位置的token上直接加上不同的信息。比如在原始的Transformer中，token embedding会加上position embedding；VL-BERT选择“linguistic token embedding <span class="math inline">\(\oplus\)</span> full image visual feature embedding”；InterBERT选择在ROI的embedding加入位置信息，“ROI embedding <span class="math inline">\(\oplus\)</span> location embedding”。</p>
<h3 id="self-attention-variants-in-multimodal-context">3.2 Self-Attention Variants in Multimodal Context</h3>
<p>接下来讨论用于多模态的self-attention变体。</p>
<p>下面是作者总结的变体：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230301104906563.png" /></p>
<p><strong>Early summation</strong></p>
<p>在embedding layer对于两个模态的token embedding直接进行element-wise summing：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230223172136061.png" /></p>
<p>其中的<span class="math inline">\(Z_{(A)}\)</span>和<span class="math inline">\(Z_{(B)}\)</span>表示是来自两个模态的token embedding matrix；<span class="math inline">\(\alpha\)</span>和<span class="math inline">\(\beta\)</span>是两个人工定义的权重；<span class="math inline">\(TF\)</span>表示Transformer layer/block。</p>
<p>这样做的好处是不会增加计算量。</p>
<p>坏处是<span class="math inline">\(\alpha\)</span>和<span class="math inline">\(\beta\)</span>需要人工选择，并且直接相加两个模态的embedding，显得过于粗暴了。</p>
<p><strong>Early Concatenation</strong></p>
<p>不是相加，而是直接拼接两个模态的token序列，组成新的序列输入到Transformer：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230223172712248.png" /></p>
<p>这种做法使得一个模态的token embedding可以直接以其它模态的token embedding作为context进行学习。这种做法也叫做“all-attention”。</p>
<p>坏处是更大的输入序列长度，当然会增加计算复杂度。</p>
<p><strong>Hierarchical Attention (multi-stream to one-stream)</strong></p>
<p>每个模态各自有Transformer，然后拼接到一起输入到一个统一的Transformer：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230223173045776.png" /></p>
<p>这种做法是late fusion的一种；也可以看做是一种特殊的early concatenation（在输入到真正的多模态Transformer之前，先使用单模态Transformer对token embedding进行了编码）。</p>
<p><strong>Hierarchical Attention (one-stream to multi-stream)</strong></p>
<p>和前面的相反，首先使用一个统一的Transformer处理多模态数据，然后每个模态再有自己独立的Transformer：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230223173438330.png" /></p>
<p>这种做法的一个例子是InterBERT。</p>
<p><strong>Cross-Attention</strong></p>
<p>很常见也非常自然的想法，每个模态都有Transformer，但是内部Transformer的query是来自于其它模块：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230223174400978.png" /></p>
<p>这种做法叫做cross attention或者co-attention，是VilBERT方法首次提出（<em>Vilbert: Pretraining taskagnostic visiolinguistic representations for vision-and-language tasks. 2019</em>）。这种做法能够将其它模型的信息引入到当前模态，也没有增加Transformer的输入token序列长度，但是它丢失了全局上下文，也就是不能够像前面的all-attention一样，同时考虑所有模态的token embedding。</p>
<p><strong>Cross-Attention to Concatenation</strong></p>
<p>另外一个变种就是在co-attention之后，使用拼接或者另外的Transformer来继续处理。这样同样可以捕获多个模态的global context。</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230227144954372.png" /></p>
<h2 id="transformers-for-multimodal-pretraining">4 Transformers for Multimodal Pretraining</h2>
<h3 id="task-agnostic-multimodal-pretraining">4.1 Task-Agnostic Multimodal Pretraining</h3>
<p>对于任务无关的预训练Transformer模型，存在以下的几个现状/趋势：</p>
<ul>
<li>Vision-language pretraining (VLP)是有最多研究的方向，包括了image+language，video+language。</li>
<li>VLP模型有两种组合方式：two-stage方式，（如LXMERT，VilBERT，VL-BERT等）使用了object detector（如Faster R-CNN）。end-end方式（如Pixel-BERT，SOHO，KD-VLP等）没有使用额外的object detector。</li>
<li>大多数都是以自监督self-supervised的方式进行训练，但是这种训练方法非常依赖于大量提前对齐的多模态数据作为跨模态监督。比如最常用的image-text pairs，instructional videos（比如教做饭的视频，其中的图像和文本更可能是对齐的）。这种数据实际上也不是很好获得，更多现实情况下可能是weakly-aligned或者unaligned的多模态数据。当然目前也出现了一些弱对齐/无对齐的多模态数据进行预训练的工作（<em>Product1m: Towards weakly supervised instance-level product retrieval via cross-modal pretraining ICCV 21</em>，<em>Simvlm: Simple visual language model pretraining with weak supervision 21</em>，<em>Zero-shot text-to-image generation 21</em>）</li>
</ul>
<p>另外一个很重要的点是如何设计pretext task。pretext task起源于CV领域，可以翻译为前置任务/代理任务/预训练任务；它一般是比较泛化的，能够潜在的对一系列下游任务有帮助的辅助任务。通常是某种自监督学习任务，比如masked language modelling (MLM)、masked object classiﬁcation (MOC)、image rotation等等。</p>
<p>单纯的从任务角度讲，这些pretext task可以分为是单模态预测任务和多模态预测任务。但要注意的是，单模态预测任务实际上很可能涉及到利用多模态信息，这和具体模型训练时的信息编码策略有关。</p>
<p>从motivation的角度讲，pretext task可以分为masking、describing、matching和ordering，如：</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230301162531606.png" /></p>
<p>尽管目前多模态预训练Transformer方法已经取得了很大的进展，比如VLP模型可以在一系列下游的multimodal discriminative tasks达到很好的效果，但是对于生成任务generative tasks不能直接应用。如文献（<em>Xgpt: Cross-modal generative pretraining for image captioning 21</em>）中指出，VideoBERT和CBT都需要额外训练一个解码器才能够完成video captioning任务。</p>
<p>另外，如何设计合适的pretext task也是个重点。多任务学习和对抗学习都被研究用来提升预训练效果（<em>12-in1: Multi-task vision and language representation learning CVPR 20</em>，<em>Product1m: Towards weakly supervised instance-level product retrieval via cross-modal pretraining. ICCV 21</em>），然而多个pretext任务如何平衡？如何设计pretext，越复杂的就越好吗？</p>
<h3 id="task-speciﬁc-multimodal-pretraining">4.2 Task-Speciﬁc Multimodal Pretraining</h3>
<p>也有很多的研究工作是针对特定领域/任务的预训练，这是因为上述的通用预训练模型有些情况下（如预训练语料领域不重叠/结构不能够充分捕获领域特征/预训练任务设计不合适等）很难直接应用到特定领域。此类特定领域/问题/任务包括：</p>
<ul>
<li>vision and language navigation：需要做sequential decision</li>
<li>generative task：一般的VLP模型无法无缝的适用于生成任务</li>
<li>programming：需要考虑代码结构</li>
<li>health</li>
<li>fashion domain</li>
</ul>
<h3 id="transformers-for-speciﬁc-multimodal-tasks">4.3 Transformers for Speciﬁc Multimodal Tasks</h3>
<p>Multimodal Transformer结构当然也可以直接用于特定的多模态任务，具体不展开。</p>
<h2 id="challenges-and-designs">5 Challenges and Designs</h2>
<h3 id="fusion">5.1 Fusion</h3>
<p>按照阶段，fusion可以分为early fusion（input level）、middle fusion(intermediate representation)、late fusion（prediction）。</p>
<p>一个值得注意的方法是bottleneck fusion（<em>Attention Bottlenecks for Multimodal Fusion</em>）</p>
<p><img src="https://lxy-blog-pics.oss-cn-beijing.aliyuncs.com/asssets/image-20230301201634535.png" /></p>
<h3 id="alignment">5.2 Alignment</h3>
<p>真实的数据常常是多个模态数据同时发生，因此天然存在对齐的模态数据。多模态数据的对齐是最常用的想法就是把多模态数据映射到通用空间下，然后通过对比学习等方法进行优化。除了比较粗粒度的image-text match的问题，还有更加细粒度的对齐任务，比如需要图像上region-level的对齐。</p>
<p>很多下游任务都需要对齐能力，比如visual grounding、text-to-speech等。当使用多任务学习进行训练时，可以看做是一种隐式的task-level的对齐。</p>
<h3 id="transferability">5.3 Transferability</h3>
<p>可迁移性是multimodal Transformer的另一个挑战，包括以下几个方面：</p>
<ul>
<li>训练数据与实际/测试数据之间的差距，比如如何把在well-aligned cross-modal pairs训练好的模型迁移到weakly-aligned cross-modal pairs。CLIP是一个很好的工作，它通过prompt template来弥补训练和测试之间的差异。</li>
<li>过拟合是另一个妨碍迁移性的问题。由于Transformer的建模能力比较强，很容易在训练数据上过拟合。</li>
<li>不同任务之间的差异也是需要克服的困难。比如判别任务和生成任务之间的差异，BERT-like的模型不能直接应用在生成任务上。再比如有时候多模态模型需要处理某些模态数据缺失的问题，这种情况下知识蒸馏是一个可能的解决方案。可以用多个单模态Transformer作为teacher，一个多模态Transformer作为student（<em>Towards a uniﬁed foundation model: Jointly pre-training transformers on unpaired images and text 21</em>）。</li>
<li>跨语言的差异。</li>
</ul>
<h3 id="efficiency">5.4 Efficiency</h3>
<p>multimodal Transformer的效率问题主要体现在两个互相影响的方面：</p>
<ol type="1">
<li>需要大量训练样本</li>
<li>随着输入序列长度的增加，训练时间和显存按照平方级增长</li>
</ol>
<p>解决的核心方法是减少训练样本或者减少模型参数，目前有以下几种思路来提高效率：</p>
<ul>
<li>Knowledge distillation. 通过知识蒸馏，从大的Transformer模型获得小的Transformer模型。</li>
<li>Simplifying and compressing model. 移除一些模型的模块，比如在VLP模型中移除object detector；权重共享，multimodal Transformer中的部分模型参数可以共享。</li>
<li>Asymmetrical network structures. 不同的模态给定不同大小的模型部分。</li>
<li>Improving utilization of training samples. 充分挖掘训练样本的潜在信息。</li>
<li>Compressing and pruning model. 选择multimodal Transformer的最优子结构。</li>
<li>Optimizing the complexity of self-attention. 直接优化Transformer的self-attention，比如稀疏注意力。</li>
<li>Optimizing the complexity of self-attention based multimodal interaction/fusion. 优化多模态交互带来的计算成本，比如bottleneck fusion方法。</li>
<li>Optimizing other strategies. 其它策略，比如有研究者（<em>Multiview transformers for video recognition 22</em>）提出可以逐步的融合多模态tokens，而不是直接融合所有多模态token。</li>
</ul>
<h3 id="universalness">5.5 Universalness</h3>
<p>通用性是当前很多模型主要考虑的问题之一，出现了以下几种体现通用性的思路：</p>
<ul>
<li>Unifying the pipelines for both uni-modal and multimodal inputs/tasks. 单模态场景和多模态场景通用，比如上面提到的使用知识蒸馏来增加迁移性。</li>
<li>Unifying the pipelines for both multimodal understanding and generation. 判别任务和生成任务通用。</li>
<li>Unifying and converting the tasks themselves. 模型不变，通过改动任务设置让模型在多个任务上通用，比如CLIP。</li>
</ul>
<h3 id="interpretability">5.4 Interpretability</h3>
<p>可解释性。研究者尝试设计一些探测任务来评估预训练过程中模型到底学习到了什么（<em>Behind the scene: Revealing the secrets of pre-trained vision-and-language models ECCV 20</em>，<em>Probing image language transformers for verb understanding 21</em>）。</p>
<h2 id="discussion-and-outlook">6 Discussion and outlook</h2>
<p>作者提出了几个开放问题：</p>
<ul>
<li>设计更加通用的多模态架构，不仅仅是在多模态任务上，也要在各个单模态任务上取得最好的效果。常常发现尽管使用了更多的多模态数据，多模态模型在单模态任务上的表现不如单模态模型。为了解决这个问题，可能探究和理解multimodal Transformer背后的原理和机制是比不断尝试新的网络架构更有价值的问题。</li>
<li>发现跨模态数据之间的隐式对齐。</li>
<li>multimodal Transformer的高效学习问题还没有被充分探究，尽管efficient Transformer的各种变体已经出现了很多研究工作。</li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">
          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>Liu Xiyang
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://liuxiyang641.github.io/mmml/MM-Transformer-Survey/" title="MM-Transformer-Survey">https://liuxiyang641.github.io/mmml/MM-Transformer-Survey/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/Transformer/" rel="tag"><i class="fa fa-tag"></i> Transformer</a>
              <a href="/tags/Multimodal/" rel="tag"><i class="fa fa-tag"></i> Multimodal</a>
              <a href="/tags/Survey/" rel="tag"><i class="fa fa-tag"></i> Survey</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/nlp/Foundation-models/" rel="prev" title="Foundation-models">
                  <i class="fa fa-chevron-left"></i> Foundation-models
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/nlp/LM-as-KB-Survey/" rel="next" title="LM-as-KB-Survey">
                  LM-as-KB-Survey <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






    <div class="comments gitalk-container"></div>
</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-flag"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Liu Xiyang</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">344k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">5:13</span>
  </span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="//cdn.jsdelivr.net/npm/@next-theme/pjax@0.5.0/pjax.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/hexo-theme-next@8.12.2/source/js/comments.min.js"></script><script src="https://cdn.jsdelivr.net/npm/hexo-theme-next@8.12.2/source/js/utils.min.js"></script><script src="https://cdn.jsdelivr.net/npm/hexo-theme-next@8.12.2/source/js/motion.min.js"></script><script src="https://cdn.jsdelivr.net/npm/hexo-theme-next@8.12.2/source/js/next-boot.min.js"></script><script src="https://cdn.jsdelivr.net/npm/hexo-theme-next@8.12.2/source/js/pjax.min.js"></script>

  
<script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/hexo-theme-next@8.12.2/source/js/third-party/search/local-search.min.js"></script>

  <script class="next-config" data-name="pdf" type="application/json">{"object_url":{"url":"https://cdn.jsdelivr.net/npm/pdfobject@2.2.8/pdfobject.min.js","integrity":"sha256-tu9j5pBilBQrWSDePOOajCUdz6hWsid/lBNzK4KgEPM="},"url":"/lib/pdf/web/viewer.html"}</script>
  <script src="https://cdn.jsdelivr.net/npm/hexo-theme-next@8.12.2/source/js/third-party/tags/pdf.min.js"></script>


  <script src="https://cdn.jsdelivr.net/npm/hexo-theme-next@8.12.2/source/js/third-party/fancybox.min.js"></script>


  
  <script data-pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"ams","js":{"url":"//cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"}}</script>
<script src="https://cdn.jsdelivr.net/npm/hexo-theme-next@8.12.2/source/js/third-party/math/mathjax.min.js"></script>


<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.css" integrity="sha256-AJnUHL7dBv6PGaeyPQJcgQPDjt/Hn/PvYZde1iqfp8U=" crossorigin="anonymous">

<script class="next-config" data-name="gitalk" type="application/json">{"enable":true,"github_id":"liuxiyang641","repo":"liuxiyang_blog_comment","client_id":"b800b344e096846a4608","client_secret":"45ac194feea7e642c29f8e13180184cc98afb3e6","admin_user":"liuxiyang641","distraction_free_mode":true,"proxy":"https://cors-anywhere.azm.workers.dev/https://github.com/login/oauth/access_token","language":"zh-CN","js":{"url":"https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.js","integrity":"sha256-Pmj85ojLaPOWwRtlMJwmezB/Qg8BzvJp5eTzvXaYAfA="},"path_md5":"e7d931efd50d54ea496fb6b748d1baee"}</script>
<script src="https://cdn.jsdelivr.net/npm/hexo-theme-next@8.12.2/source/js/third-party/comments/gitalk.min.js"></script>
<div class="moon-menu">
  <div class="moon-menu-items">
    
    <div id="moon-menu-item-back2bottom" class="moon-menu-item">
      <i class='fas fa-chevron-down'></i>    </div>
    
    <div id="moon-menu-item-back2top" class="moon-menu-item">
      <i class='fas fa-chevron-up'></i>    </div>
    
  </div>
  <div class="moon-menu-button">
    <svg class="moon-menu-bg">
      <circle class="moon-menu-cricle" cx="50%" cy="50%" r="44%"></circle>
      <circle class="moon-menu-border" cx="50%" cy="50%" r="48%"></circle>
    </svg>
    <div class="moon-menu-content">
      <div class="moon-menu-icon"><i class='fas fa-ellipsis-v'></i></div>
      <div class="moon-menu-text"></div>
    </div>
  </div>
</div><script src="/js/injector.js"></script>
</body>
</html>
